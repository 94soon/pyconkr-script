이번 세션은 청와대 국민청원 데이터로 파이썬 자연어 처리 입문하기를 할 거예요. 

제공해드린 이메일로 노트북 정보를 받으셨을 텐데 해당 노트북에 폴더를 통째로 내 드라이브로 사본을 생성해주세요. 

와이파이 정보는 여기 슬라이드에도 제가 올려놨어요. 

슬라이드를 참고해 주셔도 되고요. 

이 슬라이드는 여기 국민청원 데이터로 자연어처리 입문하기, 여기 폴더에 와서 보시면 슬라이드가 같이 있습니다. 

이번 세션에는 4시 30분부터 7시 30분까지 청와대 국민청원 데이터로 파이썬 자연어처리 입문하기를 진행할 예정입니다. 

일단 노트북을 복사를 해서 첫째 노트북을 띄워주세요. 

첫 번째 국민청원 분석 및 시각화라고 1번 노트북부터 드라이브에 보시면 5번 노트북까지 있는데 1번부터 5번까지 순서대로 진행을 할 거예요. 

그런데 여기에서 5번 노트북은 제가 참고용으로 올려놨고요. 

1번부터 4번까지 진행을 하고 5번은 간단하게 설명을 드리고 끝내는 걸로 할 거예요. 

3시간 동안 1번부터 4번까지 진행을 하고 5번은 간략한 설명을 드리고 끝내는 걸로 할 거고요. 

노트북의 간략한 설명을 드리면 첫 번째 노트북에서 분석 및 시각화를 해볼 거고요. 

그리고 두 번째 노트북에서 명사를 추출해서 자연어처리를 사용해서 워드 클라우드를 그려볼 거고 세 번째로 워드투 백으로 단어 유사도를 살펴볼 거고요. 

그리고 네 번째로 국민청원 투표수를 평균보다 높은 투표수를 받는지 낮은 투표수를 받는지 텍스트 데이터를 분석해서 투표수를 예측해보는 투표수의 평균보다 높은지 낮은지를 예측해보는 튜토리얼를 진행할 거예요. 

그리고 다섯 번째로 국민청원 카테고리 분류가 있는데 생각보다 카테고리 분류를 하려면 많은 데이터를 써서 시간이 많이 들여서 노트북를 해야 하는데 여기에 있는 노트북들이 다 지난 1년 2개월치의 국민청원 데이터가 있어요. 

작년 8월 19일부터 국민청원이 시작된 걸로 알고 있는데 작년 8월 데이터부터 10월 12일까지 데이터가 제가 업로드했고 그 데이터를 불러와서 볼 건데 데이터가 생각보다 많아서 그 데이터를 전체를 다 자연어처리를 하게 되면 오늘 이 시간 안에 노트북을 돌리는 걸 끝낼 수 없어서 제가 일부 데이터만 가지고 실습을 진행할 거예요. 

그리고 이 노트북이 GPU도 지원하지만 실습을 진행하는 데 성능이 그렇게 좋지는 않아서 일단 1번부터 4번까지 진행하고 5번은 간단한 설명을 하고 마치도록 할 거예요. 

지금 도우미분들에게 만약에 노트북이 복사가 안 돼서 이 노트북 실행이 안 되는 분들은 도우미분들에게 노트북 실행을 문의해 주세요. 

제 소개를 드리면 저는 청와대 국민청원데이터로 파이썬 자연어처리 입문하기를 진행할 박조은이라고 합니다. 

감사합니다. 

오늘 튜토리얼은 다른 곳에서 진행을 하기도 했었고 그다음 파이콘 2018 올해 파이콘에서 했던 튜토리얼이기도 해요. 

그래서 이번에 다시 진행을 하게 되었고요. 

청와대 국민청원 데이터로 파이썬 자연어처리 입문하기로 자연어처리를 해볼 텐데요. 

생각보다 자연어처리를 하려고 하면 데이터 셋이 없어요. 

영어로 된 데이터 셋은 다른 데에서 구할 수 있는 셋이 많은데 한국어 데이터 셋은 다르게 해서 구해야 하는데 보통 라이센스가 많이 문제가 되고, 가장 많이 쓰시는 게 네이터 영화 리뷰 데이터, 그것도 파이콘에서 박은정 님이 발표하실 때 사용했던, 그걸로 사용하고 있는데요. 

이번 튜토리얼에서는 청와대 국민청원 데이터를 사용할 거예요. 

이걸 사용하는 이유 중 하나는 공공물 저장물, 자유이용 허락. 

그래서 라이센스가 여기 보시면 출처를 표시하고 비상업적으로 이용할 수 있는 거라 사용하게 되었어요. 

사실 이 세미나는 유료 세미나이긴 해요. 

비용을 내고 등록을 하셨지만 파이콘준비위원회에서 하는 세미나고 여기에 여러분이 준비하신 비용은 노쇼 방지 비용으로 하고 있어요. 

만약 세미나에서 비용이 남으면 그 비용은 다른 세미나를 준비하거나 다음 파이콘에 준비하는 데 사용할 거예요. 

그래서 이 튜토리얼도 비영리 튜토리얼이라는 걸 말씀드리고요. 

청와대 국민청원 데이터로 텍스트 데이터 다루기를 실습할 거예요. 

텍스트 데이터, 저희가 의외로 위키데이터라든지, 분석할 데이터가 있긴 한데 텍스트 데이터분석은 챗봇, 자연어 처리를 해보려고 하는데 오늘은 자연어처리 입문하기예요. 

가장 기본이 되는 부분들을 같이 실습해보려고 해요. 

그중 하나가 Bag of words라는 거예요. 

기계에서 자연어를 처리하게 하려면 텍스트 데이터를 이해시켜야 하는데 그걸 이해시키기 위해서는 문자로 된 데이터를 숫자로 바꿔주는 작업이 필요해요. 

그래서 Bag of words라는 걸 사용해서 보통 자연어처리 시작을 하는데 이 Bag of words는 가장 단순하지만 효과적이라는 거. 

하지만 이 Bag of words라고 해서 하나의 가방에 텍스트, 워드를 담아주는 거예요. 

피처를 추출하게 될 거예요. 

Bag of words로 tri-gram 사용하는데요. 

위에가 전체의 일부를 찍어본 거예요. 

저희가 일단 하게 될 실습인데요. 

거기 보면 훨씬이라는 단어는 전체 국민청원데이터 중 288개가 등장하고요. 

저기 힘들게 라는 단어가 있는데 세 번째 인덱스에서 한 번 등장해요. 

저런 식으로 텍스트의 이 단어가 몇 번이 등장하는지 이렇게 벡터화해서 여기 보면 아티클 뷰가 있고 아티클 뷰 HTML, 이런 식으로 3개가 붙어 있어요. 

그래서 단어를 3개씩 묶어준 거예요. 

tri-gram를 써서. 

그다음 TF-IDF라는 걸 사용할 거예요. 

이 TF-IDF는 뭐냐 하면 특정한 단어가 문서 내 얼마나 자주 등장하는지를 나타낸 것과 그다음 이 값이 자주 등장하는 단어가 중요하다고 생각할 수도 있죠. 

하지만 단어 자체가 너무 문서에 자주 등장하면 그 단어는 너무 흔해서 의미가 없을 수 있어요. 

예를 들어서 그런 단어를 불용어라고 하는데요. 

예를 들어서는, 너는, 했다, 한다. 

이런 단어는 굉장히 자주 등장하는 단어지만 사실 그 문서에서 중요한 의미를 갖진 않아요. 

그래서 그러한 단어는 흔하게 등장하는 단어는 중요도를, 가중치를 낮추고 특정 단어에만 전체 문서에는 자주 등장하지 않지만 특정 문서에는 자주 등장하는 단어가 있을 수 있어요. 

그 단어는 가중치를 높게 주는 거예요. 

예를 들어서 청와대 국민청원 데이터에서 초등 돌봄 교실이라는 국민청원에 이러한 단어가 등장하는 청원이 있을 수 있어요. 

초등 돌봄 교실라는 단어는 특정 청원에서 굉장히 자주 등장해요. 

하지만 전체 청원에서는 자주 등장하지 않을 거예요. 

예를 들면 블록체인이라는 단어도 특정 청원에서 굉장히 자주 등장할 거예요. 

공매도, 특정 청원에서 많이 등장하겠지만 전체 청원에서 자주 등장하지 않을 거예요. 

그런데 안녕하세요? 

청원합니다. 

이런 건 많이 등장하는데 큰 의미를 갖지 않기 때문에 너무 빈번하게 등장하는 단어는 가중치를 낮게 설정할 거예요. 

그래서 아까 Bag of words로 벡터를 생성했을 때 그 단어의 값들이 정석값으로 들어갔을 거예요. 

이렇게 가중치를 적용하게 되면 적용한 값이 들어가 있어요. 

그래서 TF-IDF로 가중치를 적용해서 저희가 오늘 투표수가 평균보다 높은지 적은지를 예측해볼 거예요. 

그래서 일단 여기 노트북이 5개가 있어요, 국민청원 데이터 텍스트 데이터를 다루기 앞서서 이 국민청원 데이터를 분석할 거예요. 

첫 번째 공공데이터로 파이썬 입문하기에 하신 분 있지만 여기 자연어처리에만 참석한 분 있기 때문에 설명을 하고 가도록 할게요. 

판다스는 엑셀처럼 파이썬에서 쓸 수 있는 엑셀이라 생각하시면 돼요. 

그대신 파이썬을 쓸 수 있기 때문에 엑셀보다 훨씬 더 다양한 기능으로 사용할 수 있을 거고요. 

그리고 더 많은 데이터를 처리할 수 있을 거예요. 

청와대 국민청원 데이터를 실습 데이터를 받아서 저희가 작업할 텐데 제가 여기 이 데이터를 지금 받으실 필요는 없어요. 

왜냐하면 제가 이 파일은 올려놨기 때문에, 이 SCV 파일을 10월12일까지 파일을 모아놨어요. 

오늘은 10월 13일이에요. 

그래서 그 데이터를 가지고 일단 이 데이터를 시각화할 거예요. 

국민청원 데이터가 지금 1년 넘게 모였어요. 

그래서 국민청원 데이터를 보면 저도 몰랐던 사회적 이슈를 알게 되더라고요. 

그래서 최근 어떤 이슈가 1년간 어떤 이슈가 핫했었는지 판다스와 플라나로 시각화를 해서 저희가 살펴볼 거예요. 

일단 저희는 튜토리얼을 진행하기 때문에 데이터가 너무 많이 보이면 지저분하기 때문에, 저희 시각화툴로 플라나인이라는 걸 사용할 텐데. 

이건 좀 더 시각화를 할 수 있도록 맵핑 툴이에요. 

보통 파이썬에서는 C본을 많이 사용하는데 이번 실습에서는 이걸 사용하고 기존 R을 사용하셨던 분은 R에 있는 문법을 그대로 쓸 수 있기 때문에 아주 똑같지 않지만 비슷하게 사용하실 수 있어요. 

그래서 이걸로 데이터를 시각화할 거예요. 

판다스로 데이터를 분석할 거기기 때문에... 

그리고 파일 임폴트를 해줘서. 

그리고 오늘 여기에는 아마 구글 콜레보레이트 노트북을 사용하시는 분이 처음 있을 거예요. 

이걸 사용하는 이유는 저희가 주피터 노트북을 사용하게 되면 환경이 달라서 환경 설정하다 3시간이 다 지나요. 

그래서 모두 다 같은 환경에서 실습하기 위해서 구글 콜레보레이트 노트북을 사용하게 되었습니다. 

그리고 이거의 장점은 저희가 다 같은 환경을 쓸 수 있다는 장점도 있지만 주피터 노트북에 비해서 한번 라이브러리를 설치하면 그걸 다음부터 설치할 필요는 없는데 콜레보레이터는 새로 열 때마다 설치를 해줘야 하는 단점이 있어요. 

그래서 한글폰트도 역시 마찬가지로 설치를 따로 해줘야 해요. 

저는 나눔 고딕 폰트를 사용해서 설치했고요. 

맵플라리 폰트를 바꿔주는 이유는 그램 오브 그래픽스를 사용하는 플랫나인이 맵핑해놓은 툴이에요. 

판다스로 일단 데이터를 살펴볼 건데요. 

데이터를 불러올 때 시간이 걸리기 때문에 여기 데이터를 미리 실행하도록 할게요. 

원래 제가 여기 데이터 세싱 파일을 올린 이유가 주석을 풀고 실행하게 되면 파일 업로드할 때 있는 인터페이스가 떠요. 

CSV 파일을 여기 업로드하고 진행하려고 했는데 업로드하는 데 와이파이를 여러 명이 쓰다 보니, 그다음 이 파일의 사이즈가 커서 시간이 1시간이 걸릴 것 같더라고요. 

그래서 제가 S3에 올린 데이터를 바로 올린 걸로 실습 진행할 거예요. 

지금 여기 CSV 파일은 제가 수집한 건 아니에요. 

링크가 있는데 청와대 국민청원 데이터라고 해서 직접 크롤링을 해서 매일 매일 크롤링을 해서 여기 모인 데이터를 받아온 거예요. 

이 데이터는 강규형님께서 수집한 데이터를 제가 사용하고 있습니다. 

크롤링 과정이 궁금하신 분은 여기 저장소에 가셔서 파이썬 파일들을 참고하시면 좋고요. 

여기 스타도 남겨주시면 좋을 것 같습니다. 

그래서 제가 수집한 데이터는 아니에요. 

이 데이터를 가지고 와서 제 계정에 올렸어요. 

제가 실행을 했는데 깜빡하고 다시 실행하네요. 

국민청원 데이터를 다시 읽어오도록 할게요. 

여기까지 혹시 진행이, 실행까지 하신 분? 

다 되셨나요? 

안 되신 분? 

노트북 안 열리는 분? 

그러면 계속 이어서 진행할게요. 

지금 이미 읽어온 데이터가 있어서. 

이게 실행이 되어야 다음 셀이 진행되는데 콜레보레이터리를 실행하는 동안 소개하면 지금 비지라고 뜨는데. 

연결이 반드시 되어 있어야 하고요. 

그다음 여기 런타임에 보면 체인지 런타임이라고 있어요. 

파이썬 2나 3으로 선택해서 실행할 수 있어요. 

오류가 뜨는 분은 파이썬3으로 돼 있는지 확인하시고요. 

이 코드는 파이썬3 기반으로 돼 있습니다. 

여기 GPU, TPU 지원하게 돼 있는데요. 

저는 NONE로 설정하고 진행할게요. 

데이터를 다 불러왔는데 데이터가 28만 건이 있어요, 지난 1년간 국민청원 데이터가 28만 건이 수집돼 있고요. 

일부 데이터만 보도록 할게요. 

판다스에서 일부 데이터만 미리 볼 때 저희가 터미널에서 파일을 미리보기할 때 헤드나 테일로 읽어보잖아요. 

마찬가지로 데이터 프레임에 데이터를 담아줬고요. 

헤드로 불러오면 기본적으로 5개의 데이터를 불러와요. 

좀 더 많은 데이터, 적은 데이터보고 싶으면 숫자를 지정하면. 

이렇게 지정한 숫자만큼 불러옵니다. 

그래서 헤드로 불러왔기 때문에 2017년 8월 19일부터 국민청원 등록이 시작했어요. 

청원은 스텔러데이지호에 대한 제안. 

가장 최근에 제가 10월 10일 데이터까지 불러왔는데요. 

마지막 번호가 266, 여기 보면 아티클 아이디가 비어 있는 경우가 있는데 가끔 삭제된 청원들도 있더라고요. 

266번이 마지막인데요. 

여기 보면 북핵문제보다 부동산 소득불균형 문제에 대해서 확실히 입장표명하십시오., 오타가 있네요. 

자연어처리 도구를 사용하게 되면 여기 튜토리얼에서는 포함이 돼 있지 않은데 맞춤법을 처리해줄 수 있고요. 

띄어쓰기 같은 것도 교정을 해줄 수가 있어요. 

여기 실습은 일단 저희가 시간 내 진행하기 위해서 그런 것들은 포함이 돼 있지 않아요. 

그리고 판다스에서 컬러스라고 해서 데이터 프레임의 컬럼만 따로 불러올 수 있어요. 

국민청원 데이터 위에서 헤드와 테일로 어떤 게 있는지 미리 보긴 했지만 이렇게 컬럼스로 찍어볼 수 있어요. 

그러면 여기 어떤 컬럼이 있는지 보면 스타트 엔드 앞에 아티클 아이디, 스타트 엔드, 엔서드, 보트, 카테고리, 타이틀, 콘텐츠가 있어요. 

국민청원 사이트에 가서 한번 보도록 할게요. 

네트워크가 조금 느린데요. 

국민청원 데이터에 가서 보면 번호가 있고 분류가 있고 제목이 있고 청원인, 청원 기간. 

기간은 스타트와 엔드, 분류와 카테고리, 그다음 제목이 타이틀로, 여기 내용을 한번 들어가서 보도록 할게요. 

내용을 들어가서 보면 그 내용이 콘텐츠에 들어가 있어요. 

와이파이가 좀 느린데요. 

여기 청원 개요가 내용이 들어가 있고 여기 투표수가 있어요. 

참여인원이 있는데. 

국민청원에서 위에 보면 답변대기중 청원이 있어요. 

답변을 받기 위해서는 몇 건 이상 투표를 받아야 되는지 혹시 아시나요? 

손 들고 말씀해 주시면. 

저기 먼저 손을 들어주셨어요. 

20만 건 이상 투표를 하면 답변을 받을 수가 있어요. 

답변 대기건으로 돼요. 

아까 손들고 말씀해 주신 분은 이따 오시면 제가 책을 1권 드리도록 하겠습니다. 

20만 건 이상이 답변 대상 건이 돼요. 

제가 클래시피케이션을 만들 때 답변 대상건을 클래시피케이션을 하는 걸 만들어도 재미있겠다는 생각을 했어요. 

전체 청원이 30만 건 가까이 되는데 답변 대상 청원이 몇 건 정도 될까요? 

이것도. 

혹시 손들고 말씀해 주실 분? 

-50건이요. 

-50건이 안 돼요. 

이따 책 1권 드릴게요. 

50건이 안 돼요. 

그래서 전체 30만 건 중 50건밖에 안 되니까 그 50건을 예측하려고 하니까 예측률이 낮게 나와요. 

예측이 너무 재미가 없잖아요. 

예측은 재미있게 만들기 위해서 평균 투표수보다 높은지 낮은지, 이걸로 튜토리얼을 구성했어요. 

그래서 국민청원 사이트에서 이 데이터들이 어떻게 생겼는지 봤고요. 

판다스에서 인포를 찍어보게 되면 이 데이터들이 컬럼의 데이터 타입이 어떤 타입인지 볼 수 있어요. 

스타트와 엔드는 데이터 타입으로 돼 있어요 여기에서 파스 데이트에 스타트와 엔드는 데이트로 파스를 해달라고 지정했기 때문에 데이트타입으로 불러와진 거예요. 

그리고 엔서드와 보스는 인트 타입, 카테고리, 타이틀, 콘텐츠는 오프젝트 타입으로 가져왔어요. 

숫자로 된 건 기본값이 숫자로 된 데이터에서 카운트값이나 링값, 표준 편차, 최소값, 최대값, 사분의 수에 대해서 지스크라이브 한 걸 보여줘요. 

그래서 이전 세션에서 공공데이터 분석하기를 하면서 왜 데이터를 시각화해야 하는지에 대해서 이야기를 했어요. 

그걸 잠깐 재미로 보여드리도록 할게요. 

이게 앤스콤의 4인방이라는 그래프예요. 

여기 보면 이 4개의 그래프를 봤을 때 위에 설명을 보면 링값, 콜데이션 값이 다 같다. 

선형 회귀값도 같아요. 

얘를 디스크라이브해서 봤을 때 다 같은 값으로 보일 거예요. 

그렇다고 해서 얘를 같은 데이터로 볼 수 있느냐, 시각화를 해서 보면 다 다른 데이터인 거죠. 

데이터사우르스라고 보면 이 공룡으로 이렇게 그려져 있잖아요. 

그런데 이 나머지 12개의 그래프들이 다 민, 스텐다드, 코디네이션 값이 같아요. 

같은데 다른 표현을 하고 있죠. 

그래서 데이터 시각화가 필요하다는 설득을 하기 위한 자료고요. 

그래서 저희가 이렇게 디스크라이브로 이런 값을 볼 수 있지만 이 데이터를 시각화해서 다시 볼 거예요. 

그리고 볼 때는 기본값이 수치값이기 때문에 이렇게 수치값만 지금 나오지만 판다스에서 이렇게 물음표를 하게 되면, 지금 노트북이 느려서 실행이... 

이렇게 도움말을 볼 수가 있어요. 

그래서 여기 도움말에 보면 여기 옵션으로 어떤 걸 줄 수 있는지 나오거든요. 

그래서 여기 이런 시리즈 데이터가 있고 이 시리즈 데이터를 디스크라이브를 했을 때 숫자잖아요. 

숫자를 디스크라이브했을 때 이런 식으로 요약값을 볼 수 있어요. 

그리고 얘는 문자 데이터잖아요. 

이 문자 데이터를 디스크라이브했을 때는 좀 다른 값을 볼 수 있어요. 

그래서 보면 여기 AA, B, C가 있는데 이 시리즈 데이터는 4개예요. 

카운트 값은 4고 유니크한 값은 3개고 톱은 A예요. 

프리퀀시가 가장 많이 등장하는 게 2번이고 그 값은 A라는 거예요. 

그래서 이렇게 만약에 내가 오프젝트 데이터에 대해서 요약을 보고 싶으면. 

얘는 오브젝트 데이터만 있을 때 디스크라이브했으니까... 

오브젝트 데이터를 보고 싶을 때 인크로드 올 하면 숫자형 데이터, 오브젝트 데이터를 같이 볼 수 있고 이렇게 수치형 데이터만 보고 싶을 때 인클루드 B의 넘버... 

해당 타입에 해당되는 요약 정보를 볼 수 있습니다. 

그래서 디스크라이브 하든지 다른 판다스의 다른 것들도 물음표를 통해서 도움말을 그때그때 볼 수 있어요. 

그리고 이제 결측치가 있는지 볼게요. 

이전 세션에서 공공데이터로 분석하기를 했는데 결측치가 굉장히 많이 있었어요. 

그런데 국민청원 데이터의 경우에는 결측치가 없어요. 

결측치가 여기 콘텐츠에, 없는 결측치고요. 

결측치가 거의 없어요. 

그래서 결측치 처리하는 건 따로 하지 않아도 될 거예요. 

그리고 사용하지 않는 컬럼, 답변이 없는 컬럼을 그냥 지우도록 할게요. 

그리고 그냥 답변 대상건을 다시 뽑아볼 거예요. 

이거를 지우고 나면 7개의 컬럼에서 6개의 컬럼이 남게 될 거예요. 

그래서 여기 앤서드를 지우고 앤서라는 새로운 컬럼을 만들었어요. 

판다스에서 새로운 컬럼을 만들 때 이렇게 컬럼을 지정해주고 해당 데이터를 넣어주게 되면 데이터가 들어가면서 새로운 컬럼이 생겨요. 

여기 앤서라고 해서 20만 건이 넘는 건 1로 넣어줬어요. 

그래서 여기에 헤드를 찍어줬어요. 

아까 앤서드라는 게 사라지고 여기 트로폴스로 값이 들어간 걸 확인할 수 있어요. 

앤서에 답변 대상건인지 아닌지, 들어간 걸 확인할 수 있고요. 

청원 기간이 얼마나 됐는지 좀 궁금하더라고요. 

청원 기간, 컬럼을 생성해줬어요. 

스타트와 엔드, 엔드에서 스타트를 빼서 어느 기간 동안 청원이 이루어졌는지 듀레이션이라는 컬럼을 만들어서 넣어줬어요. 

듀레이션별로 어센딩을 해서 봤더니 7일이 가장 적은 기간이더라고요. 

그리고 얘를 벨류 카운트로 한번 청원이 얼마나 기간 동안 가장 많은지 찍어봤어요. 

판다스에서 벨류카운트를 쓰게 되면 저희가 그룹 바이 한 다음에 카운트한 것처럼 결과를 볼 수가 있어요. 

그래서 보면 30일인 게 가장 많아요. 

그다음 90일, 7일 , 15일, 60일을 볼 수 있어요. 

그런데 최근에는 7일짜리 청원은 거의 없더라고요. 

이건 처음 시작됐던 초기에 시행착오를 겪으면서 청원 기간이 조정된 것 같아요. 

청원 기간이 90일이고 답변대상건을 보면 이런 청원이 나와요. 

청원 기간이 60일이고 답변대상건을 또 보도록 할게요. 

작년에 있었던 청소년보호법과 관련된 청원, 청원 기간이 30일이고 답변대상건은 많네요. 

다른 건 1건씩인데. 

답변대상건을 보면 2017년 9월 30일 11월 4일, 11월 17, 11월 24일, 12월 3일. 

청원 기간이 30일 건에 대해서 이런 청원들이 있었어요. 

이거는 다 답변 대상인 청원건이고요. 

답변 청원 기간이 7일이고 답변 대상건을 보도록 할게요. 

청원 기간이 7일인데 답변 대상의 건은 없어요. 

그래서 보면 제가 여기에서 셰이프를 찍도록 했어요 새로 만들어진 데이터가 셰이프인데 0건으로 뜨거든요. 

그래서 여기에서 청원 기간이 7일 데이터의 시작 날짜를 그룹화하면 8월 19에서 9월4일, 답변 기간이 7일로 추정되고 국민청원건들을 그냥 데이터 프레임으로 찍어보면 이렇게 돼 있는 걸 볼 수 있어요. 

2017년 8월 19일부터 2017년 9월 14일까지만 7일짜리의 국민청원이 있는 걸 볼 수 있어요. 

그리고 어느 분야에 청원이 가장 많이 들어 있는지 보도록 할게요. 

역시 카테고리를 벨류 카운트를 해주고 컬럼을 카테고리와 카운트로 맞추기 위해서 컬럼을 지정해줬어요. 

지정하지 않으면 다른 이름으로 뜨거든요. 

컬럼을 지정해서 카테고리와 카운트로 출력이 되도록 했어요. 

보면 가장 많이 청원이 들어온 분야가 어느 분야죠? 

정치개혁 분야의 청원이 가장 많죠. 

그다음 기타, 인권, 성평등 순으로 돼 있어요. 

청원이 얼마 동안 집계되었는지. 

청원 집계는 여기에서 저희가 지금 청원 집계일을 찍어보면 388일 동안 청원이 집계가 됐어요. 

이거는 제가 오늘까지 데이터가 반영된 게 아니라 지금 여기에 있는 데이터만 봤을 때 388일치의 국민청원 데이터가 있는 걸 볼 수 있고요. 

그리고 가장 청원이 많이 들어온 날이 언제인지 봤더니 2017년 11월 10일, 그다음 17년 9월 5일, 1월 11일, 2월 6일, 2017년 11월 9일이에요. 

이 날들을 각각 다 보면 뭔가 사회적인 이슈가 있었던 날이에요. 

그날 특별히 국민청원이 다른 이유가 있어서 뭔가 사회적인 이슈가 있어서 저렇게 청원이 많이 올라와 있는데 그런 날을 보면 같은 내용의 청원이 굉장히 많아요. 

도배한 글들이 많은 걸 볼 수 있어요. 

피봇 테이블로 국민청원이 투표를 가장 많이 받은 분야를 볼 텐데 청원이 많이 들어온 분야는 어디였죠. 

정치개혁 분야였어요. 

그런데 투표를 가장 많이 받은 분야는 인권, 성평등. 

그래서 투표를 가장 많이 받았어요. 

여기 피봇 테이블이라는 걸로 카테고리에, 몇 개의 투표를 얼마나 받았는지 계산을 해봤는데 제가 판다스를 소개하면서 엑셀과 비슷한 툴이라고 말씀드렸어요. 

그래서 엑셀에서도 이거하고 똑같이 피봇 테이블을 그려볼 수 있을 거예요. 

그런데 30만 건 가까이 되기 때문에 아마 CSV 파일을 엑셀에서 읽으려고 하면 일단 구글 스프레드시트에서 열리지 않을 거고요. 

그리고 엑셀에서 읽는다 하더라도 굉장히 느리거나 안 열릴 수 있어요. 

엑셀에서 이걸 읽으려고 시도하지 않았어요. 

그리고 투표를 가장 많이 받은 날을 보도록 할게요. 

청원을 많이 받은 날과 투표를 많이 받은 날에도 다를 거예요. 

투표를 가장 많이 받은 날을 보면 제가 이거 셀 실행하는 걸 미리 말씀을 못 드렸는데 아마 잘 실행을 하고 계실 것 같아요. 

질문을 안 주셔서 주피터 노트북과 마찬가지로 시프트 엔터를 하거나 왼쪽 플레이 버튼을 누르게 되면 셀이 실행됩니다. 

투표를 가장 많이 받은 날을 봤더니 2018년 6월 13일 가장 투표를 많이 받았어요. 

이날도 뭔가 사회적인 이슈가 있었겠죠. 

그래서 투표를 35만 건 이상, 20만 건 이상 받으면 답변 대상건이 되는데 투표를 35만 건 이상 받은 날짜를 보면 그날은 대부분 뭔가 사회적 이슈가 있었던 날이에요. 

특히 가장 이렇게 투표를 많이 받은 날은 더 핫한 사회적 이슈가 됐던 날이라 그날의 뉴스 기사를 찾아보면 해당 청원과 내용을 알 수 있습니다. 

그래서 2월 같은 경우 두 번째로 많은 2월 19일 이슈는 어떤 청원일지 혹시 맞힐 수 있는 분? 

제가 책 1권을 드리도록 하겠습니다. 

2월 19일 미투요? 

아니에요. 

빙상연맹이에요. 

맞아요. 

빙상연맹과 관련된 사건이 있을 때 선수들에 대한 국민청원이 있었어요. 

2월 19일에. 

그리고 청원을 많이 받은 날하고 투표를 많이 받은 날을 보기 위해서 2개의 데이터 프레임을 합치기 위해서 인덱스를 생성했고 합쳐서 보도록 할게요. 

보면 6월 24일 같은 경우는 답변대상건이 2건이고요. 

투표도 좀 많이 받았네요. 

그래서 이렇게 청원을 많이 받은 날, 투표를 많이 받은 날을 봤어요. 

여기 보면 청원이 많은 날, 2017년 9월 6일 청원이 되게 많았고 그리고 투표를 가장 많이 받은 날은 6월 13일, 2018년 6월 13일이었어요. 

그리고 답변 대상 청원이 20만 건 이상되면 답변대상 청원인데 아까 제가 50건 미만이라고 했는데 지금 보니 52건이네요. 

또 헤드와 답변 대상 청원에 대해서 헤드를 찍어보면 이런 청원들이 답변대상 청원에 있는 걸 볼 수가 있어요. 

답변대상 청원 중 가장 투표를 많이 받은 건은 여기 소트 벨류로 해서 보트라는 컬럼을 소칭해서 역순으로 소칭하도록 했어요. 

저희 엑셀에서도 위에 컬럼을 누르면 소칭할 수 있잖아요. 

저 보트라는 컬을 어센딩 펄스해서 큰것부터 하게 되면 보트가 가장 많은 것부터 답변대상 청원 중 나오겠죠. 

6월 13일에 시작된 청원인데 제주도 불법난민 문제에 대한 청원이었어요. 

가장 투표를 많이 받은 청원이네요. 

그리고 이제 시각화를 해보도록 할게요. 

이렇게 시각화를 해보게 되면 제가 일부러 이렇게 그래프의 글씨가 깨지게 그렸어요. 

여기 그래프 글씨가 깨진 이유는 한글폰트를 지원을 안 하기 때문이에요. 

저희가 이 노트북을 시작할 때 한글폰트를 설치했거든요. 

그래서 나눔바른고딕을 설치했어요. 

폰트를 지정하게 되면 폰트가 보이게 돼요. 

이렇게 폰트가 보이게 되는데 여기 보면 이렇게 폰트가 깨지는 모양을 두부 모양이라고 해서 실제로 두부라고 부르더라고요. 

재미있었어요. 

그래서 글씨가 겹쳐보이지 않도록 여기 로테이션을 60도로 회전을 하도록 했고요. 

그렇게 해서 봤더니 아까 제가 디스크라이브를 해 보면서 실제로 얘를 시각화해보면 우리가 봤던 것과 다른 모습일 수도 있다고 말씀을 드렸었는데 여기 보면 저희가 벨류 카운트 했을 때와 같은 값이 보이는 걸 볼 수 있어요. 

보면 정치개혁 분야의 청원이 가장 많아요. 

많은 걸 볼 수가 있고요. 

카테고리별 투표수를 보게 되면 지지플롯의 그램 오브 그래픽스 문법을 간략하게 설명드리면 여기 AS라고 해서 에스테틱 부분에 X축, Y축, 컬러를 지정해서 저희 C본에서 슈를 지정해서 데이터를 구분해서 보는 옵션이 있는데 여기서는 이따 기능 사용할 텐데 필이나 컬러를 사용해서 데이터를 구분할 수 있고요. 

저기 지오매트릭 부분에 제가 어떤 폰트의 시각화를 할 것인지, 표현을 할 수가 있어요. 

그래서 카테고리별로 투표수를 보면 아까 정치개혁 분야의 청원이 가장 많았었는데 여기 투표수를 봤을 때는 인권, 성평등이 가장 많은 걸 볼 수가 있어요. 

그리고 코드 플립을 사용해서 X축, Y축을 바꿀 수 있어요. 

저희가 판다스에서 데이터 표현할 때 트랜스포드를 사용해서 하거든요. 

그래프를 그릴 때는 이걸 사용해서 X축과 Y축을 바꿔서 표현을 했어요. 

그리고 포인트를 불러서, 답변 대상 청원건은 당연히 20만 건 이상이 되어야 되기 때문에 20만 건 이상의 청원들이 몰려 있고 아까 청원을, 그 답변을 굉장히 많이 받은 청원이 있었는데 저 위에 올라가 있는 걸 볼 수가 있어요. 

그리고 답변 대상 여부로 박스툴로 봤는데 위에 답변대상 청원이 편차가 좀 큰 걸 볼 수 있어요. 

그 밑에는 답변 대상이 아닌 건들 경우는 다 몰려 있는데 대부분 밑의 데이터가 많이 몰려 있는 편이에요. 

그리고 여기 아까 제가 설명을 하면서 필이나 컬러로 여기 데이터를 구분해서 볼 수 있다고 말씀드렸는데 여기 답변 여부로 데이터를 구분해서 표현을 하도록 했어요. 

그리고 투표를 가장 많이 받은 카테고리인 인권 성평등에서 투표수가 많은 순으로 상위 10개만 보도록 할게요. 

투표수가 인권 성평등에서 상위 10개를 보면 이러한 청원들이 있는 걸 볼 수가 있어요. 

여기에서 제가 인권성평등만 따로 모아서 봤고요. 

시계열 데이터를 보도록 할게요. 

얘도 시간, 시간이 지난 날짜가 있어요. 

시작 날짜, 끝나는 날짜가 있어서 얘를 시계열 데이터로 분석할 수 있어요. 

제가 시계열 데이터 분석까지 하지 않고 그냥 월별 청원수만 찍어봤어요. 

데이터 프레임에서 컬럼을 스타트 몬쓰, 스타트, 데이, 스타트는 아워는 없기 때문에 영어로 들어가 있어요. 

의미는 없는데 저렇게 쓸 수 있다는 걸 보여드리고 싶어서 만들었고요. 

스타트 먼스를 보면 이렇게 값이 들어가 있는데 1월 12월이 가장 없고, 가장 적은 건 10월이에요. 

이렇게 데이터 값이 나오는 건 일단 지금 1년이 되었잖아요. 

그런데 10월은 아직 1년이 되지 않았어요. 

그리고 11월하고 12월은 작년에 뭔가 이슈가 있어서 11월에 특히 청원이 많이 올라왔어요. 

그래서 11월은, 다른 달 같은 경우 청원이 많은 걸 볼 수 있어요. 

그리고 또 여기 보면 시작한 날짜에 대한 날짜별 청원수를 찍어보면 1일부터 31일까지 찍어봤어요. 

보면 11일쯤 굉장히 데이터가 많이 들어온 걸 볼 수 있어요. 

이걸 보고 11일은 달마다 청원이 많이 들어오는 걸까, 이렇게 분석을 해볼 수 있을까요? 

저 날은 특정 날짜에, 아마 11월 11일 거예요. 

2017년 11월 11일인데 청원이 굉장히 많이 들어왔어요. 

그래서 그날 데이터가 굉장히 많아서 많이 보이는 거예요. 

그래서 날짜 데이터에, 시간이 없기 때문에 시간은 벨류 카운트로 찍어도 다 0으로 돼 있어서 의미는 없어요. 

그다음 카테고리별로 투표수와 답변 여부를 카테고리별로 찍어봤어요. 

이거는 이렇게도 찍어볼 수 있다는 걸 보여드리고 싶어서 찍어봤고요. 

일자별로 청원 시작일별 집계수, 지금 2018년 8월 19일부터 시작이 됐잖아요. 

10월 10일까지 있는데 보면 11월 1일이 많이 들어가 있죠. 

그리고 이날도 9월 좀 전에 8월에 굉장히 많이 들어온 날이 있어요. 

그리고 시작일별 투표수를 보면 청원이 많이 들어온 날은, 투표수를 보면 그 날짜 주변에 이렇게 몰려 있는 걸 볼 수 있는데 11월 11일은 청원 많이 들어왔는데 투표수는 생각보다 튀지는 않아요. 

이런 것들을 볼 수가 있어요. 

청원이 많이 등록된 날. 

3000건 이상 청원이 등록된 날, 어떤 청원이 있는지 보면 2017년 9월 5일, 11월, 11일, 1월 11일이 있어요. 

그래서 9월 5일에 어떤 청원이 들어왔나 보니까 소년법 폐지에 대한 이슈가 있었어요. 

작년 9월. 

그러한 이슈가 있었고 그리고 9월 5일 들어온 청원만 따로 봤어요. 

그랬더니 그날 들어온 청원은 인권 성평등, 육아 교육에 대한 청원이 압도적으로 많은 걸 볼 수 있어요. 

카테고리별 청원수도 9월 5일 보면 투표수도 인권 성평등이 압도적으로 투표수가 많은 걸 볼 수 있어요. 

정치개혁 분야에 청원이 많이 들어온 날, 작년 11월 11일 굉장히 청원이 많이 들어왔어요. 

그래서 보면 이날 이런 도배글이 엄청 많았어요. 

정치적인 이슈라 제가 따로 이야기하지는 않겠습니다. 

그래서 각 데이터의 수를 집계를 해봤어요. 

재미로 스타드 먼스, 임폴트. 

이런 데이터들을 여기에서 집계를 해봤어요. 

그리고 제목과 내용에 들어가는 단어 수를 세봤어요. 

저희는 자연어처리를 할 거기 때문에 제목하고 내용에 들어가는 단어숫자가 뭔가 상관관계가 있는지 보고 싶어서 시각화를 해봤더니 제목이 짧으면 타이틀이 짧으면 내용도 짧은지. 

시각화해본 거예요. 

그런데 여기 제목이 짧은데 내용이 굉장히 많은 거 있죠. 

그리고 제목이 굉장히 긴데 내용이 짧은 게 있고, 그렇게 큰 상관관계를 보이지 않는데 대부분 이쪽에 데이터가 많이 몰린 게 보이고. 

투표수와 단어수, 단어수가 많으면 투표를 많이 받을까? 

이런 생각이 들었어요. 

그런데 투표수와 단어수를 보니까 투표수가 높다고 해서 단어수가 꼭 많은 건 아니었어요. 

이렇게 분석을 했고요. 

특정 단어가 들어가는 청원을 볼 거예요. 

그래서 가상화폐가 들어가는 청원을 본다든지 아니면 돌봄아이 초등교육, 이런 게 들어가는 청원을 본다든지. 

해서 두 번째 노트북에서는 이런 데이터를 가지고 워드 클라우드를 그려보는 실습을 하고 세 번째 노트북에서는 워드 투 백으로 단어 벡터화해서 유사도를 보는 걸 할 거예요. 

지금 첫 번째 노트북은 여기까지 진행을 하고요. 

지금 31분인데 45분까지 휴식시간을 갖고 다음 두 번째 노트북부터 진행을 해 보도록 하겠습니다. 

그리고 아까 책 받으실 분, 앞으로 와 주시면 책 드릴게요. 

이제 다시 두 번째 노트북을 시작하도록 하겠습니다. 

역시 마찬가지로 시프트 엔터를 눌러서 실행을 해주셔야 하는데 노트북의 상태가 커넥트가 되었는지 확인을 해 주시고 워드클라우드 그려보는 실습을 하도록 할게요. 

파이너썬에서 자연어처리 대표적으로, 파이콘에서 두 번 정도 설명을 드렸는데 여러 형태 분석기를 파이썬에서 쓸 수 있도록 모아놓은 거예요. 

C+나 자바로 작성된 자연어처리를 파이썬에서 쓸 수 있도록 만들어진 도구인데, 구글 콜레보레이터로 실행하려면 자바를 따로 설치하든지 해서. 

파이썬에서 자바를 사용할 수 있는 툴을 따로 설치해야 하는데 콜레보레이터에서 설치하고 하다가 너무 힘들어서 포기했어요. 

이 자연어처리 도구도 작년 파이콘에서 소개가 된 자연어처리 도구예요. 

soynlp라고, 이 soynlp를 가지고 명사를 추출하고 토큰화하는 작업을 할 거예요. 

soynlp를 설치했고요. 

그다음 이 soynlp를 설치가 잘 됐는지 보도록 할게요. 

soynlp 보면 , 이미 설치가 됐다고 뜨고. 

다시 설치하도록 할게요. 

soynlp를 설치했고요. 

보면 soynlp의 경우는 여기 라이엔스가 언노운이라고 돼 있어요. 

개발자분께 메일을 드려서 사용해도 되냐고 물어봤더니 사용해도 된다고 하시더라고요. 

이 soynlp를 가지고 튜토리얼를 진행할 거예요., 판다스나, 넌파이는 이미 콜레보레이터에 설치돼 있기 때문에 임폴트만 하면 돼요. 

정규 편식을 쓸 수 있는 것도 임폴트를 하도록 하고요. 

데이터를 아까와 마찬가지로 S3 계정에 있는 데이터를 가지고 오도록 할게요. 

쉐이퍼를 찍어보게 되면 아까와 마찬가지로 같은 데이터기 때문에 30만 건 가까이, 28만 건이 있는 걸 확인할 수 있죠. 

뒤에서 테이블 찍어보게 되면 최근 데이터들, 부동산 관련, 아니면 소득 불균형 문제의 청원이 있는 게 보이네요. 

그리고 여기에서는 자신의 관심사에 맞는 단어로 데이터를 가지고 오도록 할게요. 

저는 돌봄, 육아, 초등, 보육이라든지 이런 키워드에 관심이 많은 편이에요. 

그래서 저는 이런 키워드로 가지고 와서 봤지만 여러분께서는 여러분이 관심 있는 분야, 만약 나는 블록 체인에 관심이 많으면 이런 것들을 키워드를 넣어서 가지고 올 수 있겠죠. 

본인이 관심 있는 키워드를 여기 넣어주시면 좀 더 실습이 재미있을 것 같아요. 

제가 특정 키워드로 워드클라우드를 그리려고 하는 건 전체 키워드로 그리게 되면 워드클라우드가 제대로 원하는 정보를 표현이 안 돼요. 

30만 건이기 때문에 데이터를 그리는 데 시간이 굉장히 오래 걸려요. 

그래서 게다가 저희가 같은 와이파이 망을 사용하고 있기 때문에 네트워크가 느려질 수 있어서 데이터를 줄여서 가지고 오려고 제가 일부러 키워드를 추출을 해서 그리는 걸로 했어요. 

보면 제가 관심 있는 키워드를 보면 11495건이에요. 

이 키워드는 1만 건이 넘게 있고요. 

그래서 헤드를 찍어보게 되면 저는 초등돌봄이라든지 이런 게 나왔으면 하는데 그런 육아, 이런 것과 관련된 청원들이 있어요. 

그리고 토크나이저는 스페이스바로 분리를 해서 단어들을 토큰화할 수 있어요. 

여기서 soynlp를 사용하는 김에 soynlp에 있는 토크나이저를 이용해서 토큰화를 할 거예요. 

위에서 보면 청원들이 여러 개가 있는데 여기에서 인덱스 번호가 15번인 것만 텍스트를 따로 갖고 와서 보도록 할게요. 

위에 인덱스가 15번인 거 한국 채식인구 100만명인데 학교 급식 및 군대에서 현미채식 선택권을 보장해달라는 청원. 

이 청원을 가지고 한번 데이터를 보도록 할게요. 

이거에 대한 샘플 인덱스를 넣어서 타이틀만 보면, 이런 콘텐츠가 들어가 있는 걸 볼 수 있어요. 

그리고 토큰화해서 보게 되면 아까 제가 토큰화 하는 걸잠시 말씀드렸어요. 

이 세션이 시작하기 전에, 여기 각각의 단어들을 저렇게 토큰화하고 등장하는 단어에만 불을 켜준다고 보면 되는데요. 

토큰화를 해줬더니 한국 채식인구 100만 명, 쉼표로 해줬어요. 

여기에 필요없는 이런 점이라든지 느낌표 같은 건 없어도 되는데 이런 것들도 같이 토큰화가 된 걸 볼 수 있고요. 

콘텐츠도 토큰화를 해줬어요. 

콘텐츠도 토큰화를 했는데 전부 다 보면 많기 때문에 위에서 이슈된 것만 보도록 했어요. 

이 토큰화된 걸 타이틀하고 콘텐츠만 개수를 한번 찍어봤어요. 

그랬더니 전부 다 콘텐츠의 토큰을 보면 553개의 토큰이 있고 그다음 제목에는 12개의 토큰이 있는 걸 볼 수 있어요. 

텍스트 데이터를 전처리를 해줘야 하는데 특별히 여기에서는 다른 전처리는 해주지 않고 개인 문자만 제거를 해줬어요. 

다른 웹페이지에 있는 걸 갖고 오면 다른 게 많은데 국민청원 같은 경우는 HTML은 없고 개인문자만 있어요. 

정규표식을 사용해서, 개인 문자를 그냥 공백으로 바꿔줬어요. 

개인문자를 제거를 해 주고 어플라이에 여기 미리 만들어둔 프리폴세싱이라는 함수를 사용하도록 했어요. 

실행을 해줄 때 앞에 타임이라는 코드를 적어줬어요. 

이걸 적어주면 실행될 때 시간이 얼마나 걸리는지 알 수 있어요. 

이 셀 전체를 실행하는 데 시간을 알고 싶으면 이 타임을 위에 퍼센트를 2개를 써주고 밑으로 내리면 셀이 실행될 때 시간이 얼마나 걸리는지 출력을 해줄 수 있어요. 

시간이 좀 전처리를 하는 데 시간이 오래 걸리기 때문에 역시 밑에 것도 타임을 찍어서 전처리 토크나이즈를 해주고 그다음 토큰으로 된 거에서 전부, 아까 제가 1만 개가 넘었는데 1만 개 중 3개만 불러오도록 했어요. 

위에서 3개만 슬라이스. 

파이썬에 있는 슬라이스 기능을 써서 3개만 갖고 오도록 했어요. 

샘플 인덱스에서 제가 샘플로 처리한 토큰 중에서 여기 10개의 토큰만 보도록 했어요. 

그러면 앞에서 문재인 대통령, 각 정부 인사분들께 마음속 깊이 존경과 감사를 표한다고 토큰화가 된 걸 볼 수 있어요. 

그래프에 레티나 디스플레이를 적용하고 폰트를 설치 따로 하도록 할게요. 

그리고 워드클라우드를 설치하고 이제 워드클라우드를 그려주도록 할 거예요. 

그래서 이 워드클라우드를 그릴 때 여기 스타워즈를 따로 불러와서 적어주게 되면 스타워드를 제거해줘요. 

한국어에는 해당이 안 돼요. 

한국어에서 불용어를 제거하려면 별도 처리가 필요해요. 

스타워드는 불용어를 의미해요. 

불용어에 대해서 설명해주실 수 있는 분? 

불용어는 굉장히 자주 등장하는 단어, 나는, 너는, 합니다, 한다 있잖아요. 

굉장히 많이 등장하는 단어, 일상생활에서 많이 사용하는 단어고 꼭 쓰는 단어인데 실제로 이런 자연어처리를 할 때는 그 의미의 핵심 의미를 갖지 않아서 그런 단어들은 제거를 하는 게 좋아요. 

NHK에 보면 대부분 많이 쓰는 언어의 경우에 불용어가 미리 정의가 돼 있는 게 있는데 한국어의 경우에는 널리 쓰이는 불용어 셋은 없는 것 같아요. 

찾아보게 되면 불용어 셋이라고 해서 만들어놓은 것들이 몇 가지가 있는데 보통 예전에 한 커뮤니티에서 비슷한 글이 올라왔어요. 

한국어 불용어 셋이 있는지, 그런데 한국어는 그때그때 만들어서 많이 처리하는 것 같아요. 

그리고 한국어는 중의어라든지 동음이의어, 그런 게 많아서 그런지 불용어를 쉽게 처리할 수 있는 셋은 딱이 추천할 건 아직까지 발견하지 못했어요. 

지금까지 토큰화한 걸 이렇게 그려봤어요. 

여기에서 워드클라우드를 그려봤더니 역시 불용어가 좀 많이 보여요. 

여기서 보면 그리고, 정말, 이런, 있는, 합니다, 이런 것들은 되게 많이 등장해요. 

그래서 워드클라우드에 굉장히 크게 그려졌어요. 

실제로 큰 의미를 갖진 않잖아요. 

그래서 여기에서 soynlp를 사용해서 명사만 추출하도록 했어요. 

저런 조사라든지 문장 끝에 붙는 동사를 제거하는 좀 더 의미있는 워드클라우드를 그리지 않을까 해서요. 

그래서 soynlp를 사용해서 여기 나우 익스트렉터를 사용해서 명사만 따로 모아뒀어요. 

그랬더니 아까에 비해서 어떤가요? 

이 워드클라우드에 비해서. 

그리고, 정말, 있는, 합니다. 

크게 표시한 단어가 표시가 안 되는 걸 볼 수 있죠. 

아까의 워드클라우드보다는 좀 더 의미가 있는 워드클라우드가 된 것 같아요. 

그래서 여기 보면 가장 크게 나오는 단어가 어린이집, 그다음, 그런데 여기 어린, 교사, 국민, 이런 게 있고요. 

그다음 여성, 보육, 이런 단어가 보이는 것 같아요. 

제가 관심 있어하는 초등, 육아, 돌봄, 이런 키워드와 관련된 워드클라우드를 그렸는데요. 

여러분은 여러분이 관심이 있는 키워드를 여기 위에 보면 정규편식으로 찾는 부분이 있는데 여기에 관심 있는 키워드를 넣어서 한번 그려보시는 걸 추천할게요. 

그래서 워드클라우드 그려보기는 사실 튜토리얼이긴 한데 여러분이 시프트 엔터만 치면서 실행해볼 수 있는 그런 튜토리얼로 진행을 했어요. 

이 튜토리얼은 여러분이 직접 원하는 키워드로 워드클라우드를 그려보시면 좋을 것 같아요. 

그리고 이제 다음으로 세 번째 워드 투 백 단어 유사도 보기를 할게요. 

지금 여기 커넥트라고 해서 노트북을 실행하기 위해서 노트북 실행을 해 주시고요. 

도우미 분들에게 노트북 실행을 하는 방법을 도움을 요청해 주시면 도우미분들께서 도와주실 거예요. 

제가 처음에 Bag of words를 설명드렸는데 Bag of words단어 벡터를 구성하다 보면 벡터 사이즈가 크고 스파이스해요. 

그래서 뉴럴의 성능이 나오지 않아요. 

오늘 주로 자연어처리를 하게 될 텐데 이 WORD2VEC는 딥러닝이에요. 

여기에서 단어 유사도나, 유사도의 점수를 계산, 이런 실습을 하고 다음 실습을 하도록 할게요. 

이 WORD2VEC의 경우 스파이스한 단어 벡터를 갖는 거에 대한 단점을 보완을 해줘요. 

그래서 단어의 의미를 내포한 댄스 벡터로 만드는 게 특징이고 이 WORD2VEC은 주변의 단어가 비슷하면 해당 단어의 의미가 유사하다는 아이디어로 만들어져 있어요. 

그래서 실습의 끝에 저희가 벡터화한 단어를 시각화해서 그려볼 텐데 비슷한 단어가 비슷한 위치에 돼 있는 걸 확인할 수 있을 거예요. 

WORD2VEC를 설명할 때 자주 등장하는 이미지예요. 

이 이미지를 보게 되면 차이나는 베이징, 러시아는 모스크바, 제펜은 도쿄. 

각 국가와 수도가 연결된 걸 볼 수 있어요. 

그리고 국가는 다 이쪽, 왼쪽에 위치하고 있고요. 

도시는 오른쪽에 위치한 걸 볼 수 있어요. 

그래서 비슷한 의미를 갖는 단어끼리 비슷한 위치에 있는 걸 볼 수 있어요. 

그리고 위에는 보면 맨 위에는 베이징의 경우 아시아, 제펜은 일본이긴 한데 주로 아시아 국가가 있고 밑의 국가들은 유럽 쪽 국가들이 있는 걸 볼 수 있어요. 

그래서 비슷한 단어들은 비슷한, 주변에 있는 단어들이 비슷한 의미를 갖는 단어들인 걸 볼 수 있어요. 

WORD2VEC 인베딩을 실시간으로 볼 수 있도록 이렇게 시각화해놓은 사이트들도 있어요. 

이 사이트 보면 학습률, 러닝메이트에 따라서 WORD2VEC가 그려지는 걸 볼 수 있는데요. 

실행을 해보도록 할게요. 

지금 학습을 하면서 얘가 계속 비슷한 위치를 찾아가려고 해요. 

인풋 벡터, 아웃풋 벡터가 있고 워터, 주스가 비슷한 게 있고. 

여기 보면 여기 글씨가 겹쳐서 잘 안 보이는데 밀크, 드링크, 위트 비슷한 위치에 있는 걸 볼 수 있어요. 

링크 있으니까 재미삼아 해 보셔도 될 것 같아요. 

WORD2VEC은 크게 두 가지로 나뉘어요. 

컨티니어스 맥오브... 

여기 보면 제가 아재개그처럼 예제를 만들었어요. 

제가 만든 예제인데 컨티니어스 Bag of words하고 스킵 그램, 이 컨티니어스 Bag of words는 여기에 어떤 단어가 들어갈지 맞히는 거예요. 

밑에 답이 있긴 한데 저기 보면 제가 아재개그처럼 만든 건데 다 배라는 단어가 들어가요. 

저 배라는 단어가 사실 다 다른 의미예요. 

4개가 들어가는데 그 배가 동음이의어인데 다른 의미를 갖죠. 

스킵 그램은 역으로 예측하는 거예요. 

배라는 단어, 주변에 올 수 있는 단어를 예측하는 거예요. 

그 단어 주변 배, 먹는 배라면 그 주변 어떤 단어가 올지, 타는 배라면 주변에 어떤 단어가 올지 주변 단어를 예측하는 거예요. 

제가 참고자료도 같이 링크를 해놨고요. 

젠심이라는 게 파이썬은 젠심에 WORD2VEC가 구현돼 있어요. 

soynlp와 젠심을 설치하도록 할게요. 

그리고 역시 판다스, 넌파이, 레. 

임폴트를 하고 폰트도 설치해줘야 해요. 

로컬 주피터 노트북을 사용한다면 이렇게 폰트나 자주 사용하는 라이브러리는 한 번만 설치하게 되면 다음부터는 임폴트만 하면 사용할 수 있었는데 이 구글 콜레보레이터 노트북은 새로운 거 열 때마다 필요한 라이브러리를 매번 설치해야 한다는 단점이 있죠. 

그래서 데이터도 제가 S3에 업로드한 데이터를 불러오도록 할게요. 

그리고 이번에는 지난번에 워드클라우드를 그릴 때 제가 육아, 초등돌봄 이런 걸 키워드로 가지고 실습을 했었는데 이번에는 은행, P2P, 주식, 증권, 공매도. 

이런 키워드를 가지고 오려고 해요. 

P2P 금융도 하고 있는데 손해를 엄청 많이 보고 있어요. 

매달 장기연체가 늘어나고 있어요. 

제 장기연체된 돈을 돌려받기 위해서 P2P에 관심을 가져야겠죠. 

P2P, 은행, 금융, 주식, 공매도로 설정을 해서 이거와 관련된 국민청원 목록만 가지고 왔어요. 

왜냐하면 전체 국민청원 데이터를 가지고 이것도 학습을 시키면 훨씬 결과가 좋게 나와요. 

그런데 전체 데이터를 가지고 실습을 하다 보면 이 노트북의 성능이 지원이 안 돼요. 

그래서 나눠서 이걸 만들어주거나 따로 처리를 해야 하는데 이번 실습 시간 안에 이걸 돌려보기 위해서 제가 일부러 키워드를 추출해서 해당 키워드에 해당되는 단어만 WORD2VEC을 설정하려고 해요. 

저는 금융과 관련된 키워드를 뽑았기 때문에 파이넨스라는 데이터 프레임을 담았어요. 

여기서 헤드만 찍어봤더니 카테고리가 경제민주화, 3개가 보이고 나머지가 보이는데 소액주주 보호해 주세요. 

이런 것들이 많이 보이네요. 

그리고 뒤에서 또 기본적으로 5개, 여기 값을 적어주지 않으면 디폴트 값이 5개예요. 

5개를 찍어보면 역시 여기는 부동산과 관련된 이슈들도 보이네요. 

그리고 샘플로 보고 싶은 인덱스 번호를 넣어보도록 할게요. 

여기 보면 11번. 

소액주주 보호를 위해 요청드립니다. 

이걸 샘플 텍스트로 해보도록 할게요. 

인덱스 번호를 11번으로 넣었고 11번 텍스트의 타이틀만 갖고 오고 콘텐츠를 갖고 오게 되면 아까 워드클라우드를 그릴 때 그 데이터 전처리를 해줬잖아요. 

이번에는 어떻게 전처리를 해주냐면 아까 개인문자로 제거했는데 이번에는 한글, 영문만 남기고 모두 제거토록 할게요. 

한글, 영어, 숫자만 남기려면 0에서 9까지 남겨주시면 숫자까지 남게 될 거예요. 

그리고 특수문자나 이모티콘은 때로는 의미를 갖기도 하기 때문에 특정 문자만 제거할 때는 이렇게 제거를 해줘도 돼요. 

그래서 위에 제가 주석으로 표시해놨는데 예제로 저렇게 쓸 수도 있다는 걸 보여드리기 위해서 지우지 않고 주석 처리를 해놨어요. 

저는 여기에서 한글하고 영문만 남기고 모두 제거를 하도록 할게요. 

그래서 프리펄세싱을 해서 보면 아까 장OO 대표 기이한 행동에 대해서 이야기했는데. 

제거된 걸 볼 수 있어요. 

샘플 텍스트에 적용하고 제대로 처리가 된 걸 봤기 때문에 전체 텍스트에 어플라이에 함수를 적어줘서 실행하도록 했어요. 

전처리 과정이 지금 여기에서는 데이터가 그렇게 많지 않아서 금방 끝났어요. 

그런데 많으면 많을수록 굉장히 오래걸리기 때문에 이럴 때는 멀티스레드를 사용해서 여러 개의 스레드로 이 작업하는 걸 권장해요. 

저도 전처리 작업을 할 때는 스레드를 여러 개 생성해서 시간을 줄이는 편이에요. 

그리고 soynlp를 사용해서 토큰화를 해줄 텐데요. 

텍스트 데이터 전처리 이해하기라고 해서 트위터 한국어 형태소 분석기에 가보면 정규화, 토큰화, 어근화, 어구 추출과 관련된 이런 예시가 잘 설명돼 있더라고요. 

그래서 제가 출처를 밝히고 거기에 있는 내용을 일부 가지고 왔어요. 

정규화 같은 경우는 저렇게 ㅋㅋ 같은 단어가 많이 들어갈 때 다 ㅋㅋ가 들어가잖아요. 

저런 단어를 처리할 때 정규화를 하고 토큰화는 토큰화를 해줄 때 품사를 붙여서 토큰화를 해줄 수 있어요. 

어근화, 스테밍 같은 경우는 줄여서 표시를 하는 거예요. 

영어의 경우는 스테밍을 해주게 되면 그 단어가 달라지기도 해요. 

원형을 추출을 해주는 그런 작업을 하는데 트위터 형태소 분석기에서 제공하는 저런 스테밍 같은 경우는 입니다를 이다라고 바꿔주는 작업을 하고 어구 추출, 한국어 처리 예시, 처리하는 예시. 

이런 식으로 어구를 추출한다든지 이렇게 해서 저희가 형태소 분석기라는 거, 자연어처리 도구를 사용하게 되면 이런 걸 사용하기 위해서. 

soynlp에도 보면 키워드를 추출한다든지 품사를 추출한다든지. 

이런 기능들을 제공을 하고 있어요. 

많은 기능들 중에서 저는 일단 토크나이저만 사용하도록 할게요. 

토크나이저를 사용하게 되면 이렇게 기본적으로 스페이스바 단위를 바탕으로 토큰화를 해주게 돼요. 

소액주주 보호를 위해 조사요청 드립니다. 

이렇게 토큰화가 됐고요. 

그다음 콘텐츠도 샘플 텍스트에 대해서 해보도록 했어요. 

이렇게 토큰화된 데이터의 길이를 한번 봤어요. 

타이틀과 콘텐츠를 봤더니 타이틀은 6개, 콘텐츠는 174개. 

저희가 똑같은 걸 실습을 워드클라우드 그리는 데서도 했어요. 

그래서 지금 토큰의 길이를 세어봤고요. 

그리고 여기 전체 데이터에 토큰화를, 시간이 오래 걸릴 것 같아서 시간을 찍도록 했고요. 

지금 제가 17000개 정도였던 거 같은데 일부 데이터만 추출했기 때문에 시간이 금방 걸리는데 아마 여기에 있는 30만 건 가까이 되는 데이터를 전부 전처리한다면 시간이 오래 걸릴 거예요. 

그리고 그렇게 전처리 한 데이터는 따로 저장을 해놓고 사용하는 게 좋을 거예요. 

이제 WORD2VEC을 돌릴 텐데 WORD2VEC을 돌릴 때 학습을 시킬 때 모델에서 로그를 찍을 수 있도록 툴을 불러와줬고요. 

WORD2VEC으로 토큰들을 학습시킬 거예요. 

모델로 만들어줄 거예요. 

지금 WORD2VEC으로 모델을 학습시키고 있어요. 

46%, 72% 진행이 되는 걸 볼 수 있고요. 

두 번째 에포크를 돌고 있네요. 

이렇게 생성된 모델을 원민워드라는 이름으로 저장을 해주도록 할게요. 

지금 제가 그 키워드만 뽑아서 이거는 금방 끝나는데 30만 건 가까이를 돌리면 여기 콜레버레이터리로는 어려울 거예요. 

저장을 했고요. 

단어 사전 수를 보면 44만 건 정도의 단어가 생성되었어요. 

그래서 이 단어 중 30개, 상위 10개만이라고 돼 있는데 30개만 보면 많이 등장하는 단어를 봤더니 불용어를 제거하지 않고 했거든요. 

가장 많이 등장하는 단어가 불용어죠. 

사실 의미가 있지는 않아요. 

불용어를 제거하고 했으면 좀 더 의미가 있는 단어가 나왔을 거 같은데 그래서 아쉽지만 지금 이대로 진행을 할 텐데 여기서 카운터를 통해서 자주 등장하는 단어를 볼게요. 

역시 불용어가 자주 등장한 걸 볼 수 있고 가장 적게 등장하는 단어는 원양자원은이라는 단어예요. 

주식이라는 단어의 벡터를 그려보면 이렇게 벡터가 생겼어요. 

나중에 이 벡터들을 XY값으로 전환해서 차트의 그래프로 시각화할 거예요. 

제가 주식 공매도 P2P 이런 키워드로 학습을 시켰기 때문에 주식이라는 단어에 대해서 한번 유사한 단어를 추출해보도록 했어요. 

WORD2VEC에 모스트 시밀러, 주식이라는 단어를 넣어줬어요. 

그랬더니 이런 단어들, 투자, 투기, 외국인, 코스닥, 부동산, 이런 것들이 단어가 나오는 걸 볼 수 있어요. 

여기 코스닥이 나오니까 한번 코스닥도 찍어볼까요? 

코스닥을 찍어보면 주식시장, 코스피, 증시, 투기, 폐쇄 이런 게 나오는데 코스닥과 연관된 단어 같나요? 

이게 조금 더 유사한 단어를 추출하게 하려면 데이터 셋을 많이 넣어주면 돼요. 

지금 실행하는 데 금방 걸렸잖아요. 

눈으로 볼 수 있는 속도로 WORD2VEC을 생성해줬는데 실습을 하실 때 좀 더 많은 데이터를 넣고 작업을 하시면 좀 더 유사한 단어가 나올 거예요. 

그래서 부동산하고 유사한 단어를 추출했더니 투기라는 단어, 강남발이라는 단어. 

여기 코스닥은 가까운 의미가 있는 것 같지는 않은데. 

그다음 유사도가 없는 단어, 부동산, 증권, 현금, 코스닥, 코스피. 

이렇게 해서 유사도가 없는 단어를 봤더니 여기서는 부동산이 유사도가 없는 걸로 나와요. 

여기서 가장 유사도가 없는 게 부동산으로 생각되시나요? 

그리고 가장 유사한 단어, 주식. 

주식에도 코스닥, 부동산, 위에는 위에도 주식을 했었는데. 

그러면 여기는 다른 단어를 해볼게요. 

뭘 하면 좋을까요? 

현금으로 해볼까요? 

현금으로 했더니 카드, 인테리어, 이런 게 나오네요. 

인테리어는 왜 현금에 나올까요? 

유사한 단어에 인테리어 나왔고 저축은행, 이런 게 나오네요. 

공매도를 뽑아보면 거래소, 공매도 폐지. 

이런 게 나오고 주식하고 증권은 파지티브로 넣고 현금은 네가티브로 넣을게요. 

그러면 이런 단어가 나와요. 

그리고 단어 유사도를 보도록 할게요. 

주식하고 부동산은 얼마나 연관이 있는지, 유사도가 0.85로 나와요. 

주식, 투자는 0.89. 

그다음 주식하고 증권은 0.788, 주식하고 현금 0.67. 

여기서 보면 주식하고 부동산은 좀 유사도가 높아보이네요. 

그리고 주식하고 투자도 유사도가 높아보이고요. 

삼성하고 증권을 해볼게요. 

이것도 유사가 꽤 높네요. 

주식, 공매도. 

얘는 그렇게 높지는 않네요. 

그래서 이 단어 벡터를 생성해줬잖아요. 

아까 주식에 대한 벡터가 보면 이렇게 생겼어요, 이 벡터를 XY축으로 표현을 해주고 싶어요. 

굉장히 얘가 차원이 커요. 

이 차원이 큰 데이터를 차원 축소를 해줘야 해요. 

XY축에 표현하려면. 

일단 모델을 불러오고 TSNE도 임폴트를 해줬어요. 

WORD2VEC로 만든 걸 TSNE에 넣어주고 전체 데이터를 다 넣어주면 글자가 너무 많아서 일부 데이터, 150개의 데이터에 한해서만 시각화를 해주도록 할게요. 

TSNE에 핏 랜스폼을 하게 되면 차원을 축소시켜줘요. 

아까 되게 컸었는데 이 차원을 줄여줘서 저희가 XY좌표에 표현할 수 있는 형태로 차원 축소를 해줬고요. 

그래서 이 데이터의 일부 20개만 뽑아봤어요. 

첫 데이터가 존경하옵는인데 이 데이터에 XY 좌표를 생성해줬어요. 

이걸 그래프를 그리기 위해서 한글 폰트를 설치해줘야 해요. 

한글 폰트 설치하고 이렇게 시각화를 하게 되면 아까 존경하옵는이 맨 앞에 있었는데 여기 주식투자, 거래소가 그 근처에 있는 걸로 보이고요. 

그리고 있습니다, 때문입니다, 것입니다. 

이것도 어떻게 보면 그렇게 필요한 단어는 아닌데 불용어에 가까운 단어이긴 한데 비슷한 위치에 있는 걸 볼 수 있어요. 

그리고 여기 공정하고 국민들의. 

이런 것들이 비슷한 데 몰려 있는 게 보이고 코스피, 상장폐지, 금융 이런 단어, 주식투자는 이런 단어가 위에 몰려 있는 걸 볼 수 있어요. 

그래서 이번 실습에서는 저희가 WORD2VEC으로 단어를 벡터화 해주고 벡터화된 단어를 이 XY좌표로 TSNE로 축소해서 하는 걸 해봤어요. 

이거를 저희는 많은 데이터를 가지고 실습을 해본 게 아니기 때문에 좀 더 많은 데이터로 학습을 시키면 좀 더 의미 있는 결과를 얻을 수 있을 거예요. 

이거를 실습해볼 수 있는 재미있는 사이트가 있어서 제가 링크를 해놨던 거 같은데 지금 링크를 찾기 어렵네요. 

일단 쉬는 시간을 갖고요. 

지금 6시 26분이에요. 

그래서 한 10분 정도 쉬는 시간을, 6시 35분까지, 35분 적고 40분까지 쉬는 시간을 갖도록 할게요. 

6시 40분까지 쉬는 시간을 갖고 이제 기계 학습으로 직접 해보는 실습을 진행하도록 하겠습니다. 

이제 40분이 되어서 네 번째 노트북인 국민청원 투표수 이진분류를 시작해보도록 하겠습니다. 

국민청원 데이터로 기계학습으로 어떤 걸 해볼 수 있을까 고민을 해봤어요. 

이걸로 기계학습 튜토리얼 예제를 만들려고 했더니 처음에는 투표수를 예측하는 걸 했어요. 

그런데 투표수를 시각화해서 보면 굉장히 아웃라이어 데이터가 가끔씩 등장을 하고 투표수가 100개를 넘지 않는 청원들이 굉장히 많이 있어요. 

그래서 양쪽 끝의 데이터를 같이 가지고 분류를 해보려고 하니까 생각보다 성능이 잘 안 나오더라고요. 

그래서 실습을 튜토리얼을 재미있게 해보려면 결과가 좋게 나와야 재미가 있잖아요. 

전처리 이야기를 했을 때 결과가 좀 더 좋아지면 재미가 있어서 그래서 예측이 잘 되는 예제를 만들려고 해서 아울라이어 데이터를 제외를 하고 이진분류를 하는 튜토리얼을 만들어봤어요. 

그래서 오늘 여기서 투표수 이진분류라고 돼 있는데 투표수를 예측하는 게 아니라 투표가 평균 이상의 투표수를 얻을지 평균 이하의 투표수를 얻을지, 해서 바이너리 클래시피케이션을 하려고 해요. 

제가 준비해놓은 노트북은 2개가 있는데 이 바이너리 클래시피케이션, 멀티클래스가 있어요. 

투표수가 평균보다 높은지 낮은지 예측하는 게 있고 카테고리를 예측하는 게 있어요. 

실습을 하기 위해서 제가 이 노트북의 환경에서 빨리 돌리기 위해서 개수를 적게 하고 돌렸더니 성능이 그렇게 좋게 나오지 않아요. 

그래서 이진분류가 실습을 해보기에는 좀 더 재미있을 것 같아서 이진분류 위주로 진행해보도록 할게요. 

일단 응답여부를 0과 1로 예측을 해야 하기 때문에 투표수의 데이터를 평균을 구하고 평균보다 높은지 낮은지 0과 1로 구분해줄 거예요. 

케글에서도 보면 영화 리뷰 평점을 예측하는 경진대회들이 있어요. 

그 경진대회도 보면 튜토리얼식으로 있는 경진대회를 보면 거기 평점이 다 있는데도 그 평점을 이런 식으로 바이너리 클래시피케이션을 만들어놨어요. 

그래서 내가 바이너리로 추천인지 아닌지, 네가티브인지 파지티브인지 예측하는 튜토리얼이 있는데 그 튜토리얼에서 뭔가 힌트를 얻어서 만들게 된 튜토리얼입니다. 

그래서 기본적으로 내장이 돼 있는 판다스, 넌파이, 알리를 임폴트 하도록 할게요. 

데이터를 로드해오도록 할게요. 

지금 계속 같은 데이터를 사용해서 실습을 하고 있고요. 

그리고 디스크라이브를 해보게 되면 여기 페티션에서 아티클 ID도 수치형 데이터라 여기 디스크라이브를 했을 때 값이 나오게 되는데 여기 앤서드하고 보트 데이터가 있억. 

앤서드 같은 경우에는 얘가 답변 여부인데 아까 답변 여부에서 봤을 때 답변 대상건이 52건이잖아요. 

그래서 전체 30만 건 가까이 되는 데이터 중 52건밖에 안 되기 때문에 얘가 답변 대상건인지 아닌지 예측하는 것도 성능이 잘 안 나오더라고요. 

그래서 오늘은 이 투표수에 대해서 여기 투표수의 링값을 보면 평균값이 141이에요. 

여기서도 평균 투표수가 100건 이하인 건 제외를 시켰어요. 

그리고 1만 건 이상인 것도 제외를 시켰어요. 

그래서 너무 값이 작거나 너무 큰 거를 제외하고 전체 30만 건의 데이터 중에서 1만 2000건으로 데이터를 줄였어요. 

그래서 30만 건을 또 한꺼번에 텍스트 데이터 전처리를 하게 되면 오늘 실습 시간 안에 전처리가 끝나지 않아요. 

그래서 일단 1만 2000건으로 데이터를 줄여서 실습을 진행하도록 할게요. 

그래서 이 실습을 다시 로컬 노트북에서 해보실 때는 전체 데이터를 가지고 해보셔도 되고요. 

아니면 이 수치를 좀 조정을 해서 다시 돌려보셔도 됩니다. 

그래서 아울라이어데이터를 제거한 아이들을 DF라고 해서 따로 담아줬고 여기서 다시 평균을 구해보도록 할게요. 

아까는 141건이 투표수였어요. 

다시 투표수를 구하니까 699건이 평균값이 된 거예요. 

그래서 이진분류 대상을 정할 건데 답변 대상건이 여기서 몇 건인지 봤는데 왜냐하면 여기 20만 건 이상이 되어야 답변대상건이 되는데 1만 건 이상인 건 제외하도록 했기 때문에 여기서는 답변대상의 건은 없는 거예요. 

그래서 얘를 여기 판다스에 보면 그래프 시각화하는 툴이 내장돼 있어요. 

보트를 저희가 만든 이 새로 만든 데이터 프레임이 있는 걸 그려봤어요. 

1000건 이하의 대부분 데이터가 몰린 걸 볼 수 있어요. 

그래서 1000건 이하의 데이터가 몰려 있는 걸 볼 수 있고요. 

투표수를 평균보다 높게 혹은 적게 받았는지 해보기 위해서. 

그리고 평균 투표수를 구해올 거예요. 

아까도 구했듯이 여기서 민으로 평균만 구해오도록 했어요. 

699.214. 

이렇게 되는데 여기서 평균보다 넘으면 1로 다시 세팅을 했어요. 

기본값은 0으로 세팅을 해주고 평균을 넘었을 경우에만 1로 세팅을 해줬어요. 

다시 이 컬럼의 값을. 

보트 파지티브, 네가티브 데이터 타입을 찍어보면. 

트루인지 펄스인지 이렇게 0과 1로 변경을 해줄 거예요. 

그 데이터 타입을 인트로 하게 되면 트루는 1로, 펄스는 0으로 변경이 돼요. 

그래서 헤드를 찍어보면 이렇게 트루인 거는 1, 네거티브는 0으로 포스 언더바 데이터가 이렇게 설정이 되는 걸 확인해볼 수 있어요. 

샘플로 보고 싶은 인덱스의 번호를 넣어보도록 할게요. 

여기 샘플 인덱스가 38번으로 돼 있는데 여기 38번이 없네요. 

그래서 번호를 19번으로 해줄게요. 

그리고 샘플 인덱스의 타이틀을 갖고 와서 보면 국가유공자 예우와 관련된 국민청원이에요. 

콘텐츠를 갖고 오게 되면 국가유공자와 관련된 그런 내용들이 있는 걸 볼 수 있어요. 

그래서 이제 데이터 프레임에 있는 데이터를 전처리를 해줄 건데 전처리를 해주기에 앞서서 앞에서 저희가 샘플 타이틀과 샘플 콘텐츠를 뽑은 것에 한해서 전처리 함수가 잘 돌아가는지 테스트를 해볼 거예요. 

여기서 불용어도 같이 제거를 하도록 할게요. 

이 불용어는 WORD2VEC으로 제가 모델을 생성했을 때 가장 많이 나왔던 단어들이에요. 

그래서 이 단어들은 스타워드로 넣고 얘네가 있으면 제거를 해주도록 할게요. 

얘도 이 스타워드도 다시 만들어줘야 하는 게 여기서 합니다, 뭐 이런 단어가 있으면 제거가 안 돼요. 

그래서 얘도 수정할 필요가 있는데 일단 이렇게 불용어를 제거할 수 있다는 걸 말씀을 드리고 싶어서 이렇게 해봤고요. 

샘플 텍스트 데이터에 적용하도록 할게요. 

샘플 콘텐츠에 적용하고 그 결과를 한번 보도록 할게요. 

이렇게 스페이스바로 리플레이스를 하게 했더니 숫자 같은 게 저렇게 빠진 걸 볼 수 있어요. 

이 위에서 보면 문자가 아닌 건 다 제거하도록 했어요. 

그래서 이렇게 위에 샘플 텍스트가 이렇게 있는 단어들이 이렇게 바뀐 걸 확인할 수 있고요. 

전체 데이터의 텍스트에 대해서 전처리를 하도록 할게요. 

그리고 전체 텍스트, 콘텐츠에 대해서 스타워드를 제거를 해주도록 했어요. 

보통 기계학습을 할 때 교사학습이냐 비교사학습이냐, 아니면 지도학습이냐 비지도학습이냐. 

번역하는 거에 따라서 다르게 많이 표현되고 있는 것 같아요. 

그런데 저는 지도학습하고 비지도학습이 더 익숙한 것 같아요. 

그래서 분류를 할 때 기계학습을 분류할 때 지도학습인지 비지도학습인지 나누는 편이에요. 

저희가 오늘 하고자 하는 건 지도학습일까요, 비지도학습일까요? 

지도학습이에요. 

왜 지도학습일까요? 

지도학습과 비지도학습의 가장 큰 차이가 뭘까요? 

무슨 값이 있을까요? 

좀 더 설명해주실 수 있는 분? 

레이블. 

레이블데이터가 있느냐 없느냐에 따라서 지도학습이냐 비지도학습이냐로 나뉘어요. 

여기서 지금투표수가 다 들어 있어요. 

그래서 학습세트 테스트 세트를 만드는데, 학습 세트에만 들어 있을 거예요. 

학습세트의 텍스트 데이터에 따라서 0인지 1인지 값이 들어가 있잖아요. 

텍스트 데이터에는 들어가 있지 않아요. 

저희는 그 값이 없다고 가정하고 그 값을 예측할 거예요. 

임의로 그냥 데이터를 세트를 만들었어요. 

이 세트를 7:3의 비율로 나눠서 만들 거예요. 

학습세트가 많으면 많을수록 어떨까요? 

정확도가 높아지겠죠. 

왜냐하면 학습하는 게 더 많으니까. 

저희는 여기에 일부 데이터만 넣고 했는데 더 많은 데이터를 넣고 학습시키면 정확도가 더 높아질 거예요. 

학습세트와 테스트 데이터를 7:3로 나눠도 되고 8:2, 9:1도 돼요. 

그런데 학습세트는 테스트세트보다 항상 많아야 해요. 

적으면 학습이 안 돼요. 

학습세트가 적어서 학습이 안 되는 상태를 뭐라고 할까요? 

학습이 잘 안 된 상태. 

보통 학습할 때 핏이라는 단어를 쓰거든요. 

크게 말씀하시면 좋을 것 같아요. 

언더피팅이라고 표현을 하죠. 

그런데 그 학습세트를 너무 잘 학습을 했어요. 

그래서 이 학습세트는 굉장히 잘 예측을 하는데 새로운 세트가 들어왔을 때 예측을 못해요. 

그런 상태를 뭐라고 할까요? 

오버피팅이라고 하죠. 

그래서 학습이 너무 과하게 됐을 때 오버피팅이라는 표현을 쓰고 학습이 너무 적게 됐을 때 언더피팅이라는 표현을 써요. 

저희는 학습세트와 테스트세트를 7:3의 임의의 비율로 나눠주고 이 세트의 개수를 구했잖아요. 

사실 리셋 인덱스를 안 해도 상관없어요. 

여기에서 개수로 나눠줄 거예요. 

개수로 0.7 해서 나눠주면 전체 데이터가 12622개예요. 

그런데 거기에서 70%의 데이터가 8835개거든요. 

그래서 70%의 데이터를 학습 데이터로 쓸 거예요. 

나머지 8835개 외의 데이터를 테스트로 쓸 거예요. 

스플릿 카운트를 구했어요. 

0.7을 곱해서 저 12622개의 70% 데이터는 8835개예요. 

그래서 여기 DF에서 이렇게 스플릿을 해줬어요. 

저 8835를 해주고 뒤에 트레인으로 해서 했어요. 

이렇게 되면 앞에서 저 70%의 숫자, 8835개의 데이터가 학습세트로 데이터 프레임이 만들어졌어요. 

그래서 이렇게 학습 세트가 제대로 만들어졌는지, 8835개를 만들어서 봤어요. 

그랬더니 위에서 8835개가 이렇게 학습세트로 되었고요. 

그리고 학습세트에서 투표수가 평균보다 많은 건을 봤더니 8000개 중에서 1870개가 평균보다 투표수가 많은 건이에요. 

파지티브인 게. 

이번에는 테스트 데이터를 만들어줄 건데 아까는 이 콜론을 앞에 써줬어요. 

그런데 이 콜론을 이번에는 이 앞에 써줬었죠? 

그런데 여기는 뒤에 써줬어요. 

그러면 8835개 뒤의 데이터는 테스트 세트에 담아준다는 거예요. 

나머지 데이터는 테스트 세트에 담아줬어요. 

아까 8835개는 학습 세트, 나머지 3787개는 테스트 데이터, 7:3으로 나눠줬어요. 

테스트 데이터도 헤드로 찍어볼게요. 

원래는 테스트 데이터에는 레이블 데이터, 보트, 파지티브, 네거티브, 콜론이 없어야 정상이에요. 

이 데이터를 예측하는 게 저희가 할 일이에요. 

그래서 테스트 세트에서 투표수가 평균보다 많은 건을 봤어요. 

전체 데이터가 3787건인데 여기서 평균보다 투표수가 많은 건 738건이에요. 

그러면 저희가 예측을 하고 저 개수를 세어볼 거예요. 

단어 벡터화하기라고 해서 사이킨런이라는 걸 써서 할 건데 잠시 보고 갈게요. 

파이썬에서 기계학습을 한다다고 하면 보통 사이킷런을 사용해요. 

그래서 여기 메뉴에 보면 클래시피케이션, 리그레션, 클러스터링, 디미셔널리티 리덕션, 모델 셀렉션, 등등 있잖아요. 

여기서 좋아하는 메뉴가 있는데 튜토리얼을 좋아해요. 

여기 있는 튜토리얼을 따라하는 것만으로도 이제 이 사이킷 런을 배울 수가 있는데 여기 튜토리얼 중에서, 제가 좋아하는 이미지예요. 

사이킷 런에서 이런 일들을 할 수가 있어요. 

보면 사이킷 런으로 할 수 있는 일들이 여기 그려져 있는데 클래피피케이션, 저희가 오늘 할 거예요. 

바이너리 클래시피케이션. 

이진분류를 할 거예요. 

저기 보면 SVC, 뭐 이런 게 있고, 오른쪽에는 리그레션이 있어요. 

이건 뭘까요? 

클래시피케이션하고 회기하고 차이점이 있다면 뭘까요? 

클래시피케이션은 저희가 분류하는 거죠. 

분류하는 거고 이 리그레션은 저희가 투표수가 몇 건인지 예측하고 싶어요. 

특정 청원인 텍스트를 보고 이 텍스트로 몇 건의 투표수를 받겠다고 예측하고 싶어요. 

그럴 때는 클래시피케이션을 쓸까요? 

리그레션을 쓸까요? 

리그레션을 써요. 

그리고 저희가 국민청원 분류에서 카테고리를 분류하고 싶어요. 

그러면 클래시피케이션을 쓸까요, 리그레션을 쓸까요? 

클래시피케이션을 쓰겠죠. 

그리고 단어들이 유사한 단어들끼리 모으고 싶어요. 

유사한 청원끼리. 

여기서 4개 중 뭘 쓸까요? 

클러스터링을 써요. 

그리고 여기 4개에서 우리가 디메셔널리티를 오늘 실습을 했어요. 

뭘로 했을까요? 

맞아요. 

워드벡터를 2차원으로 축소를 했었죠. 

XY축으로. 

여기 디미셔널리티를 보면... 

그래서 여기 있는 이런 사이킷 런에 있는 툴을 사용해서 저희가 분류도 할 수 있고 클러스터링, 차원 축소 등을 할 수 있어요. 

그래서 보통 클래시피케이션에서 주로 많이 사용되는 게 디지던트리, 확장을 해서. 

그런 특이 모델의 경우 클래시피케이션에도 사용할 수 있고 리그레션에도 사용할 수 있는 편이에요. 

그래서 저희는 사이킷 런을 사용해서 투표수를 이진분류를 할 텐데 이것도 사이킷 런을 사용해서 벡터화를 할 거예요. 

그래서 여기 사이킷 런 사이트에 가서 보면 프리프로세싱이라고 있잖아요. 

전처리 할 수 있는 도구들도 사이킷 런에서 제공하고 있어요. 

그래서 여기 전처리 도구에 있는 것 중에서 피처 익스트렉션이라는 거에 텍스트, 피처를 추출한다는 거죠. 

거기서 텍스트 데이터에서 피처를 추출하는 거예요. 

카운트 벡터라이저라는 걸 통해서 개수를. 

그래서 단어 단위로 벡터를 하는데 저기. 

그 불용어들을 제외하고 벡터를 해주게 돼요. 

토큰이 나타난 최소 개수, 그 전체 문서에서 토큰이 하나밖에 등장하지 않는다. 

그래서 너무 적게 등장하는 단어는 사실 오타일 수도 있잖아요. 

그래서 그런 단어들을 제거해주고 싶어요. 

그래서 최소 2번 이상 등장했으면 좋겠다. 

2번 이상 등장하는 걸 쓸 거고요. 

그다음 n-gram을 1부터 3까지 했어요. 

가방에 담아줄 때 1, 2, 3개씩 묶도록 했어요. 

위에서 제가 샘플 텍스트를 하나 보면 여기서 국가유공자 예우 및 지원, 여기도 지우너라고 오타가 있는데 만약 국가유공자 예우지원 이게 스페이스로 지원대 있다면 n-gram으로 만들 때 국가유공자, 국가유공자 지원, 그다음 예우, 지원 이런 식으로 옆에 있는 단어를 2개, 3개씩 묶어줄 거예요. 

그렇게 해서 벡터화 해주도록 했어요. 

그리고 맥스 피처, 피처 수를 몇 개를 할 건지. 

저는 피처의 수를 2000개까지만 만들어주도록 했어요. 

그런데 이 피처의 수는 지금 트레이 데이터, 학습세트하고 테스트 세트하고 피처 수가 똑같아야 해요. 

이걸 만들어줄 때 학습세트, 테스트세트를 똑같이 만들어줄 건데 몇 개로 만들 건지, 많이 만들면 좋은데 너무 많이 만들면 오버피팅이 될 수 있겠죠. 

저는 2000개로 했는데 2만개로 하면 좀 더 많은 학습을 해서 더 좋은 결과가 나올 수 있을 거예요. 

일단 빨리 실습이 정해진 시간 안에 끝내야 하기 때문에 일단 2000개의 단어를 만들도록 했어요. 

그리고 여기서도 역시 시간을 측정하도록 했는데 아까 타임을 옆에 써줬어요. 

그런데 지금은 퍼센트가 2개로 됐어요. 

저렇게 써주게 되면 그냥 저 셀을 실행하는 시간을 찍어줘요. 

그래서 그 줄만 찍고 싶을 때는 퍼센트를 하나를 쓰고 왼쪽에 써주는데 전체 셀에 걸린 시간을 찍어주고 싶을 때 저렇게 위에 써주고 퍼센트를 2개 적어주면 돼요. 

그래서 지금 트레이 데이터를 벡터화시켜줬고, 텍스트 데이터를 벡터화해줄 거예요. 

테스트데이터를 벡터화 해주고 있어요. 

지금 저희는 데이터의 수를 임의로 줄였기 때문에 금방 끝나는데 사실 데이터를 많이 돌리고 의미있는 결과를 얻기 위해서는 많이 돌려야 해요. 

여기서 저희가 만든 걸 보려고 해요. 

보면 여기 아까 제가 슬라이드에 보여드렸던 데이터하고는 달라요. 

데이터가 달라졌기 때문에. 

이런 것들도 같은 단어로 처리할 수도 있어요. 

데이터 전처리를 해서. 

여기는 힘들다는 의미가 비슷한데 다르게 표현된 걸 볼 수 있고 저 앞에 아티클 뷰, 인덱스 같은 경우는 사실 필요없는 단어이긴 한데 모아지긴 했어요. 

COKR 뉴스, 저건 뉴스 URL일 것 같아요. 

CO, COKR, COKR 뉴스로 단어가 1, 2, 3개씩 묶여서 n-gram으로 묶인 걸 볼 수 있어요. 

여기 그냥 일부 보여지는 데이터에 힘든 분이 많은 것 같아요. 

그리고 제가 TF-IDF로 가중치를 생성해주겠다고 아까 말씀을 드렸는데요. 

다시 한번 이걸 말씀드리면 특정 청원에서는 자주 등장하지만 전체 청원에서는 자주 등장하지 않는 단어가 있을 거예요. 

그런 단어들에 가중치를 주는 거예요. 

그래서 지금 특정 단어의 가중치를 줄 텐데 얘도 사이킷 런에 있는 피처 익스트립션에 TF-IDF을 사용해서 가중치를 적용해주도록 할게요. 

가중치를 트레이 데이터와 테스트 데이터에 적용해줬고요. 

그래서 랜덤 포레스트를 사용해서 학습을 시키도록 할게요. 

랜덤 포레스트를 사용할 건데 이거에 대해서 잠시 설명하고 갈게요. 

랜덤 포레스트는 위키입니다. 

위키에 있는 걸 같이 볼 텐데요. 

일단 랜덤 포레스트를 이해하기 위해서 결정 트리를 알아야 해요. 

이건 특정 조건이 있어요. 

예를 들어서 만약 질문이 있어요. 

얘가 이 청원은 분류가 정치냐, 육아냐. 

이런 식으로 계속 물음표가 있을 거예요. 

그 물음표에 대해서 트루, 펄스냐에 따라서 계속 가지를 뻗어나가요. 

그래서 여기 보면 이렇게 가지를 뻗어나가면서 결정트리로 걔를 분류해주는 거예요. 

그런데 이 결정트리는 좀 단순하고 시각화해서 보기 좋다는 장점이 있어요. 

그런데 이 결정트리의 단점을 보완해서 얘의 성능을 좋게 해주는 게 랜덤 포레스트인데요. 

이 랜덤 포레스트은 트리를 여러 개를 만들어요. 

결정트리를 여러 개를 만들어서 이 트리에 이걸 좀 더 크게 보도록 할게요. 

앙상블 모델을 이용한 랜덤 포레스트 테스트 과정이라고 나오는데요. 

이게 얻어진 결정트리로부터 얻어진 결과를 평균 곱하기 혹은 과반구 투표 방식을 통해서 최종 결과를 도출하는 거예요. 

그래서 결정트리보다 조금 더 좋은 결과를 얻을 수가 있어요. 

랜덤 포레스트를 통해서 바이너리 클래시피케이션을 해줄 거예요. 

노트북으로 돌아가서 랜덤 포레스트는 아까 제가 분류에도 쓸 수 있고 트리 알고리즘은 분류에도 쓰고 회기에도 쓴다고 말씀드렸어요. 

그래서 분류에 쓸 때는 랜덤 포레스트 클래시 파이어를 불러오고 회기에 쓸 때는 랜덤 포레스트, 랜덤 포레스트 분류기를 선언을 해주는데 여기서 N 이스미터는 수치가 높으면 좋아지는데 빨리 하기 위해서 100으로 하고 N잡스는 얘가 가진 CPU 코어를 몇 개 사용할 건지. 

그런데 여기서 내가 노트북을 다른 사람하고 공유할 때 각각 그 노트북마다 가지고 있는 장비마다 코어의 개수가 다를 거예요. 

그래서 마이너스 1로 써주게 되면 그 장비가 가지고 있는 CPU 코어를 최대한 사용해요. 

마이너스 1로 해줬고요. 

만약 저는 노트북에 코어가 4개면 4개를 쓰겠다. 

3개면 3개를 쓰고 하나는 다른 작업에 쓰겠다. 

지정해줄 수도 있고요. 

랜덤 스테이트는 얘를 돌릴 때마다 다른 결과가 나와요. 

그래서 여기의 값을 파라미터 값을 바꿔줬을 때 N 에스메이터 값을 올려주거나 밑에 보면 굉장히 많은 옵션이 있어요. 

이걸 바꿔줬을 때 어떤 옵션을 바꿔줬을 때 성능이 올라가는지 내가 측정하고 싶잖아요. 

그런데 랜덤 스테이트가 지정돼 있지 않으면 돌릴 때마다 다르게 나와요. 

그래서 어떤 값을 올려서 성능이 높아졌는지 알기 어렵기 때문에 저렇게 랜덤 스테이트를 지정해줘요. 

그러면 돌릴 때마다 같은 결과를 얻을 수 있어요. 

학습에 사용할 레이블 데이터를 지정할 건데. 

평균보다 높은지 낮은지를 넣어줬어요. 

랜덤 포레스트의 핏으로 학습을 시키는 거예요. 

그다음 얘를 평가를 할 거예요. 

어큐러시로 평가를 측정할 거예요. 

어큐러시라는 방법을 통해서. 

그런데 이 평가하기는 그래서 얘가 얼마나 맞힐 건지 예측하는 거예요. 

평가를 하는 건데 우리가 모의고사를 봐요. 

시험을 볼 때, 굉장히 모의고사를 많이 보죠. 

그리고 특정 모의고사를 한번 또 봐요. 

그 모의고사에서 성적을 좋게 받았어요. 

그런데 그 모의고사를 잘 받았다고 해서 실전에서 시험을 과연 잘 볼까요? 

잘 볼 수도 있고 못 볼 수도 있겠죠. 

그런데 만약 모의고사를 못 봤어요. 

실전에서도 잘 볼 확률이 떨어지죠. 

그래서 모의고사를 쳐서 이 모의고사 점수를 보는 거예요. 

그래서 점수가 78점이 나왔어요. 

모의고사 점수는 78점이 나왔는데 실제 얼마로 측정할 수 있는지 테스트 데이터를 넣고. 

여기에서 프레딕트를 할 때 레이블 데이터가 없어요. 

그 레이블 데이터를 예측하는 게 숙제이기 때문에. 

예측을 했더니 일단 0000으로 예측 했어요. 

그래서 예측한 개수를 봤더니 3787개, 그다음 예측 결과를 저장하기 위해서 일단 데이터 프레임에 담아줄게요. 

그리고 0과 1이 몇개가 되었는지 볼게요. 

우리가 예측한 거에서 1로 예측한 개 13개, 아까 이거보다 훨씬 많았던 것 같아요. 

그래서 얘를 넣어주고요. 

한번 실제 우리는 답을 알고 있잖아요. 

실제 얘가 파지티브인지 네거티브인지 알고 있잖아요. 

이 값을 실제 데이터에서 예측한 데이터를 빼줬어요. 

이 예측 데이터를 빼주게 되면 여기 맨 마지막에 프레드 디프라는 컬럼을 만들어줬어요. 

그래서 값을 빼주게 되면 여기 0인지 1인지, 0인 아이는 값이 똑같은 거겠죠. 

값을 맞춘 거예요. 

1은 값을 못 맞췄겠죠. 

1이면 값을 못 맞춘 거고, 마이너스 1도 있을 거예요. 

마이너스 1도 못 맞힌 거죠. 

0이 맞힌 거예요. 

그러면 전체 데이터 개수 중 몇 개를 예측했나 봤더니 전체 3787건 중 3042건을 예측했어요. 

얘가 평균보다 높게 투표를 받을 건지 낮게 투표를 받을 건지, 실제 예측 비율을 계산해보면 80% 정도를 맞힌 걸 볼 수 있어요. 

얘는 모의고사보다 실전에서 더 좋은 결과를 얻었어요. 

그래서 모델을 만들고 그 모델을 평가를 해보고 이렇게 결과를 구해보고, 이게 국민청원의 투표수를 이진분류를 해봤어요. 

제가 이 모델을 개선해서 100%까지 해봤어요. 

이 모델을 좀 더 개선을 하고 전처리하는 과정을 좀 더 바꾼다면 아마 이거보다 더 높은 결과를 얻을 수가 있을 거예요. 

저는 지금 일부 데이터만 가지고 있는데 전체 30만 건 데이터잖아요. 

거기서 한 10만 건만 해도 얘보다 훨씬 좋은 결과를 얻을 수가 있을 거예요. 

그래서 이진분류는 여기까지 하고요. 

그다음 카테고리 분류는 잠깐 설명만 드리고 마치도록 할게요. 

카테고리 분류는 이진분류보다 조금 더 어려워요. 

그래서 얘는 일단 설명을 드리면 거의 비슷해요. 

그래서 전처리하는 과정도 비슷하고 얘도 똑같이 데이터 프레임을 앞뒤 데이터를 조금 자르고 중간에 있는 데이터만 가지고 예측을 하도록 했어요. 

보면 카테고리가 생각보다 꽤 많아요. 

이 카테고리, 테스트 데이터를 보고 얘가 어떤 카테고리인지 맞히는 거예요. 

기타에 186건이 있어요. 

기타의 카테고리가 내가 정말 이거를 국민청원을 입력할 때 여기서 어떤 카테고리를 선택해야 할지 몰라서 기타 카테고리로 입력을 했을 수 있잖아요. 

그래서 이 기타 카테고리를 저는 미리 데이터 전처리에서 다른 카테고리로 예측해줬어요. 

기존 데이터를 가지고 얘는 기타가 아니라 저기 있는 특정 카테고리 중 하나일 거라고 예측해서 넣어줬어요. 

그렇게 기계학습으로 전처리를 해주기도 해요. 

그래서 여기서 그렇게 기계학습으로 전처리를 해줬는데 여기도 보면 인권, 성평등이 아까 저희가 시각화를 했을 때도 인권 성평등이 가장 많았주고 얘는 판다스에 내장된 걸로 시각화를 한 거예요. 

인권, 성평등이 많아요. 

그리고 예측값과 실측값 비교를 위해서 카테고리도, 아까 보트의 경우도 투표수도 평균보다 높은지 낮은지 바이너리 값을 생성했는데 여기서도 실제 데이터와 예측에 사용할 데이터, 카테고리 데이터를 하나 더 칼럼을 생성해줬어요. 

이 샘플 데이터를 아까와 마찬가지로 전처리를 해주고 불용어를 제거했어요. 

샘플 데이터에 적용했다가 전체 데이터에 적용했어요. 

아까와 마찬가지로 학습 데이터와 테스트 데이터를 7:3으로 나눴어요. 

그리고 아까와 똑같은 방식으로 벡터화를 해줬어요. 

그래서 여기서는 카테고리를 레이블 데이터로 쓸 거예요. 

그래서 카테고리 데이터를 이거는 카테고리가 얼마나 비율이 있는지 그 비율을 그냥 구해봤어요. 

그래서 어떤 카테고리가 예측이 될지야 그리고 똑같이 단어를 벡터화 해줬어요. 

단어를 벡터화 해주고 그리고 똑같이 TF-IDF로 가중치를 줬어요. 

그리고 랜덤 포레스트로 학습을 시켰고요. 

그리고 학습이 잘 되었는지 평가를 했어요. 

그리고 평가를 하고 예측을 했는데 39점이 나왔죠. 

그리고 예측을 했는데 실제 결과가 어떻게 나왔을까요? 

정확도, 14%. 

굉장히 낮죠. 

그래서 원래 데이터 중에서 기타로 분류되었으나 분류 기간을 다르게 예측한 청원에는 또 이런 청원이 있어요. 

그래서 여기 재미있는 청원도 네요. 

게임 업계 1위 인벤의 근로실태 조사를 촉구합니다. 

이런 청원도 있네요. 

그래서 카테고리를 분류해보는 것까지 제가 노트북으로 올려놨는데 제 계정에 오시면 훨씬 이거보다 많은 전처리를 하고 그다음 기타 카테고리도 분류를 했던 예제가 있어요. 

청와대 국민청원 데이터 분석해서 여기 카테고리 클래시피케이션 기타 카테고리를 예측을 하고 그다음 XG부스트로 한 게 있으니 이것도 같이 참고하시면 좋을 것 같아요. 

오늘 준비한 튜토리얼은 여기까지고요. 

이하 질문세션 (영상 편집에 따라 사용하거나 사용하지 않음)



혹시 질문 있으신 분 있으세요? 

-말씀하실 때 기계학습 전처리를 한다고 하셨는데... 

-이 노트북이 바로 그건데요. 

여기 보면 기타 카테고리가 저는 특정 카테고리인데 얘가 입력하는 사람이 그 카테고리를 선택을 못하겠어서 기타라고 생각했죠. 

그래서 걔를 기타인 것과 아닌 거를 나눴어요. 

기계학습으로 기타 제거하기라고 나와 있는데 기타로 분류된 청원 중 특정 카테고리로 분류해주면 정확도가 좀 더 높아질 거라고 제가 가설을 세웠어요. 

그래서 데이터 프레임을 나눌 때 아까 테스트 세트랑 나눴잖아요. 

그런데 여기서는 그렇게 안 나누고 기타와 기타 아닌 걸로 나눴어요. 

기타 아닌 게 학습세트, 기타인 게 테스트 세트가 되는 거예요. 

기타인 걸 그냥 이 텍스트 데이터를 벡터화해서 분류를 다른 걸로 다 바꿔준 거예요. 

그래서 보면 여기 기타가 없어졌어요. 

그래서 이 데이터를 다시 합쳤어요. 

기타인 것과 기타가 아닌 걸로 나눴잖아요. 

그래서 여기에 판다스 컴켓으로 DF 낫 ETC, 아닌 걸로 합쳤어요. 

여기서 학습 세트, 테스트 세트를 다시 7:3으로 나눴어요. 

그래서 다시 전처리를 하고 다시 돌렸어요. 

다시 학습시켰어요. 

그래서 얘가, 얘는 XG부스트로도 돌리고 랜덤 포레스트로도 돌렸어요. 

그래서 예측한 카테고리를 봤더니 정확도를 봤더니 예측 비율이 랜덤 포레스트로 예측했을 때 507건의 데이터 중 40건 예측을 하고요. 

그다음 XGB로 했을 때 44건을 예측했어요. 

정확도가 카테고리 분류는 좀 많이 힘들었어요. 

그런데 얘도 제가 데이터를 전부 다 쓰지 않았어요. 

그래서 데이터를 좀 더 많이 쓰면 정확도가 높아질 거예요. 

혹시 또 질문 있는 분 있으세요? 

그러면 오늘 튜토리얼은 여기서 마치도록 하겠습니다. 

늦게까지 참여해 주셔서 감사합니다. 

그리고 오늘 도우미로 참여해 주신 분들 잠시 일어나시겠어요? 

박수 부탁드릴게요. 

감사합니다. 

그리고 저희가 간식을 굉장히 많이 준비했는데 간식이 몇 박스가 남았어요. 

그래서 많이 가지고 가셔도 돼요. 

뒤에 있는 간식 많이 가지고 가세요. 

한 봉지씩 들고 가셔도 됩니다. 

한 봉지, 한 박스씩 들고 가셔도 되니까 뒤에 있는 간식 마음껏 들고 가세요. 



