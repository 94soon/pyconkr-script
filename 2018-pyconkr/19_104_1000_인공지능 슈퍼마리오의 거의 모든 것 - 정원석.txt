안녕하세요?

인공지능 슈퍼마리오의 거의 모든 것 발표를 맡은 정원석입니다.

이른 아침부터 제 발표를 보러 와주셔서 감사합니다.

발표를 시작하기 전에 제 소개를 먼저할게요.

저는 정원석이고요.

뉴욕시립대 데이터 사이언스 학과 졸업에 커넥쳐AI 연구원이고요.

그리고 상생하는 연구소 그리고 모두의 연구소에서 강화학습 대회를 하는 CTRL 웹작을 맡고 있고 딥러닝 콜리지에서 강화학습을 연구하고 있고 강화학습 이외에 스마트 생산 안에서 로스를 줄이기 위한 챗봇을 개발하였습니다.

오늘 제가 발표할 내용은 강화학습에 관한 내용인데요.

강화학습을 쉽게 설명하기 위해서 동물, 사람이 학습하는 방법, 강화학습이 동물, 사람이 학습하는 방법과 얼마나 닮았고 그리고 강화 학습을 이용해서 슈퍼마리오가 어떻게 학습을 하는지 보여드리겠습니다.

쉽게 설명해드리기 위해서 기존 심리학 예제, 그리고 제가 직접 실험했었던 예제들을 가지고 강화학습과 슈퍼마리오를 설명할게요.

먼저 동물은 어떻게 학습을 할까요.

모든 동물은 학습능력이 있습니다.

심지어 세포가 300여개밖에 없는 예쁜꼬마선충 또한 학습 능력이 있는데요.

머리철수반사라고 하는 행동이 있는데 어떤 위험한 물체가 있을 때 다른 판단해서 반사적으로 어떤 행동을 하는 것입니다.

그래서 예쁜꼬마선충에 머리를 건드리면 일정 거리를 뒤로 물러납니다.

그런데 이렇게 여러 번 머리를 건드리게 되면 건드릴수록 뒤로 일정기간 후퇴하는 거리가 점점 줄어들게 되는데요.

이는 예쁜꼬마선충이 자기의 머리를 건드리는 것이 더 이상 위험하지 않을 것이라고 학습을 해서 일정 거리를 뒤로 가는 것을 점점 줄어드는 것을 헤비테이션이라고 합니다.

그래서 비슷한 예제로 에드워드가 어떤 행동의 결과가 만족스러우면 다음에도 그 행동을 계속 반복하고 반대로 만족하지 않으면 그 행동을 하지 않는다는 이론으로 로우 이펙트를 발견을 했는데요.

행동을 반복하게 하는 작업은 리인포스먼트라고 하고 행동을 자극하는 것을 퍼니시먼트라고 하는데 고양이를 상자에 가두고 상자 안에는 레버가 있는데 그 중 하나가 밖으로 나가게 할 수 있는데 고양이의 호기심 때문에 레버를 만지다 우연히 밖으로 나가는 레버를 누르고 밖으로 나가는 레버를 눌렀을 때 문이 열렸을 때 에드워드는 그 고양이에게 간식을 주었습니다.

그런데 이 실험을 계속하면 할수록 밖으로 나가는 시간이 점점 줄어드는 현상을 발견하였습니다.

그럼 사람도 동물과 같이 리인포스먼트와 퍼니시먼트 같은 자극에 의해서 학습하는 게 가능할까요.

사람은 태어나서 그냥 누구를 보고 배우는 것보다 업고, 기고, 걷는 것은 자기가 행동을 해보면서 스스로 배우게 되는데요.

그렇게 환경과 상호작용을 하면서 배우게 됩니다.

그래서 생각해 보면 사람도 자기가 좋아하는 것은 더 하려고 하고 그리고 싫어하는 것은 하지 않으려고 하는데 이거에 관해서 좀 제대로 된 리인포스먼트와 퍼니시먼트를 잘 설명할 수 있는 실험이 있을까 해서 탭볼을 사용해서 실험을 해봤는데요.

탭볼은 머리를 밴드에 묶고를 그 끝에는 공이 있는데 공을 펀칭하면 빠른 속도로 고무줄의 탄력으로 얼굴로 다가 오는데요.

맞으면 굉장히 아파요.

그래서 이게 복싱 선수들이 탭볼을 사용을 해서 학습훈련을 합니다.

그래서 제가 총 5일 동안 실험을 했고요.

5일 동안 실험을 했고 하루에 10분씩 5일 동안 했습니다.

그래서 첫 번째 날은 최고로 많이 때린 횟수 3번, 맞은 횟수는 2번입니다.

처음에 굉장히 어렵더라고요.

아! 그래서 두 번째 날은 최고 점수는 23번이고요.

첫 번째 날보다 공이 더 잘 보였어요.

바로 다음 날인데도, 그런데 무서워서 피하게 되더라고요.

전날에 받은 고통 때문에 조그만 얼굴로 다가와서 피하게 되는 현상이 보였습니다.

세 번째 날은 공도 잘 보였지만 저희 집 고양이가 제가 탭 볼을 치는 거를 보러 왔습니다.

그래서 이 친구가 제가 탭볼을 치다가 한 대 맞습니다.

그래서 그다음 날부터 전혀 고양이가 제가 탭볼을 연습할 때 올라오지 않았어요.

4일째는 공도 잘 보이고 힘조절도 되니까 더 세게 쳐보고 싶은 생각이 들더라고요.

그래서 복싱 선수처럼 보이고 싶어서 쳤는데 그 뒤에 제 장농이 있는 거를 깜빡하고 그게 튕겨가지고 뒤에 맞았는데 굉장히 아팠어요. 

너무 세게 쳐서 그래서 3일 정도 탭볼 연습을 하지 못했습니다.

그리고 3일 뒤에 탭볼을 다시 했는데요.

이때 79번을 했어요.

왜 그런지 모르겠는데 너무 쉬워지고 그래서 제가 79번 치는 모습을 보여드릴게요.

잘하죠?

그래서 제 직접적인 실험으로 인해서 사람도 리인포스먼트 또한 퍼니시먼트에 의해서 학습을 하는 것이 가능하지 않을까 라는 생각을 했고요.

여기서 리인포스먼트, 그러니까 어떠한 상황에서 그 행동을 반복하게 하는 자극은 타격감이라거나 아니면 성공했을 때 쾌감, 도파민 상승이 있고 퍼니시먼트는 간단합니다.

고통입니다.

쾌감을 많이 느끼게 하기 위해서 공이 있을 때 치려고 하고 고통을 피하게 하기 위해서 공이 앞에 왔을 때 피한다거나 더 치려고 하겠죠.

그래서 제가 앞에서 설명한 동물, 사람이 학습하는 방법을 토대로 강화학습에 대해서 쉽게 설명드리겠습니다.

강화학습 또한 사람과 마찬가지로 상호작용을 하면서 학습을 합니다.

이런 상호작용을 컴비테이션으로 표현을 했고 수학적으로 바꾼 것을 리인포스먼트 러닝이라고 해요.

그래서 리인포스먼트 러닝은 리워드, 어떤 보상을 체득하는 걸 하는데 에이전트는 여러 가지 액션을 해보면서 리워드를 가장 높게 받는 액션을 선택을 하지만 선택된 액션이 당장 리워드뿐만 아니라 먼 미래의 리워드 또한 상황의 영향을 끼칠 수 있어요.

그래서 익스플로이테이션과 익스플로레이션이라는 개념이 중요합니다.

이것은 뭐냐하면 에이전트가 어떤 액션을 선택할 때 자기가 가진 액션 중 가장 좋아 보이는 거를 선택하는 거를 전자.

반대로 익스플로레이션은 현재 상황에서 가장 좋은 액션은 아니지만 먼 미래를 위해서 탐험을 해보는 겁니다.

그래서 실험을 했는데 저희 집 고양이 두 마리 소개드리겠습니다.

이게 예제가 좋다고 생각해서 한 거고 하나는 셀이이고 러시안블루이고 얘는 호기심이 굉장히 많습니다.

얘는 새로운 게 왔을 때 시도를 해보고 그리고 먼치킨 풀이는 익스플로이테이션만 합니다.

먹는 거, 어떤 상황이 있을 때 먹는 거를 선택합니다.

전혀 다른 거를 시도하지 않아요.

그래서 맨처음 봤을 때 풀이는 한번 탭볼이 왔을 때 냄새를 맡고 음식이 아니기 때문에 그냥 가요.

전혀 익스플로레이션을 하지 않습니다.

뭔지 궁금하지 않지만 세리는 호기심이 굉장히 많아서처음 본 순간 저거를 잡고
쳐다보기도 하고 그리고 잡습니다.

그래서 3일 동안 훈련을 시켰는데요.

풀이는 페일했어요.

왜냐하면 제가 원하는 거는 얘네들도 저처럼 탭볼을 칠 수 있을까가 궁금 했었는데요.

그래서 세리한테 우연치 않게 공을 탭할 때마다 제가 맛있는 간식을 줬어요.

하지만 풀이는 전혀 그럴 기회조차 없었고요.

그래서 3일 정도 했을 때 저희 집 고양이 세리는 보면 공을 탭합니다.

되게 잘하죠?

그래서 이 뒤에는 몇 번 치다가 간식을 안 주니까 애교를 부려요.

간식 달라고

그래서 이렇게 익스플로이테이션과 익스플로레이션을 통해서 동물이 학습하고 그 방법에 따라서 강화학습 또한 그렇게 학습을 합니다.

그래서 강화학습은 마코프 디시션 프로세스를 통해서 학습을 하는데요.

어떠한 상황을 통해서 자기는 액션을 선택하고요.

액션은 액션의 환경에 받아서 그 액션에 맞는 보상, 리워드 값과 다음의 상황을 에이전트에게 다시 전달을 해줍니다.

예를 들어서 공이 있는 상황에서 저희 세리는 그 공을 받죠.

그래서 탭볼을 탭합니다.

그러면 저라는 환경이 세리한테 포지티브 리워드를 주는데 간식입니다.

그런 반복을 계속하게 되면 고양이는 내가 공이 왔을 때 탭을 한 가치를 굉장히 높게 판단합니다.

그래서 이런 상황이 왔을 때 탭을 하려는 상황이 발생하겠죠.

그래서 강화학습에서는 결정을 하기 위해서 두 가지밸류를 놓고 의사 결정을 하는데 하나는 스테이트 벨류입니다.

어떠한 상황 자체를 벨류로 측정을 하는 거고요.

그리고 두 번째는 어떠한 스테이트, 상황에서 제가 할 수 있는 액션들의 가치를 측정을 하는 건데요.

이 밸류 베이스트 리인포스먼트 러닝은 두 가지 밸류를 가지고 의사결정을 하기 위해서 기준으로 씁니다.

그래서 강화학습에서는 이러한 옵티멀 폴러시를 찾는 게 목적인데요.

옵티멀 폴러시는 어떤 목적을 달성하기 위해서 가장 좋은 정책을 찾는 것이 목적입니다.

그러면 이 강화학습을 사용해서 슈퍼마리오를 어떻게 학습했는지 보여드리도록 하겠습니다.

똑같이 마코프 디시젼 프로세스를 사용했어요.

여기서 에이전트는 슈퍼마리오, 스테이트는 픽셀 자체입니다.

픽셀이라고 하면 이미지 자체고요.

게임했을 때 보여지는 그 화면이고요.

이 에이전트는 스테이트를 보고서 자신이 할 수 있는 액션 중에서 하나를 선택을 하게 됩니다.

여기서 액션은 조이스틱 있죠.

위, 왼쪽, 오른쪽, 아래, 점프, 불꽃쏘는 버튼, 그리고 이것들의 조합.

이 액션들 중에서 하나를 선택을 합니다.

그러면 환경이 그 액션을 받고 그에 맞는 리워드와 스테이트를 제공을 해줘요.

그래서 이런 프로세스를 통해서 에이전트인 슈퍼마리오는 그 상태에서 액션에 맞는 각각 액션에 따른 측정을 합니다.

그런데 여기서 환경은 무엇일까요.

슈퍼마리오의 환경은 뭘까요.

개인이 슈퍼마리오의 환경을 만들었고요.

이거는 팁 인스톨스, 진 슈퍼마리오 브루스로 설치할 수 있습니다.

주소는 아래 나와 있고요.

인스톨하고 불러와서 그래서 환경 변수에 선언을 합니다.

슈퍼마리오 버전 제로로 해서 이렇게 되면 가장 기본적인 슈퍼마리오가 인폴트가 되는데요.

여기서 변수를 사용해서 리셋을 하게 되면 초기화 상태로 돌아갑니다.

여기서 초기화 상태는 맨처음 게임이 돌아가는, 스타트하는 지점입니다.

랜더를 하게 되면 제가 화면을 시각화할 수 있어요.

리셋하고 랜더를 하게 되면 이런 식으로 뜨게 됩니다.

그래서 지금 마리오가 어떠한 액션, 가만히 있는 이유가 뭐냐하면 액션을 이 환경에 넣어주지 않았기 때문입니다.

그래서 이 환경은 여러 가지가 있는데요.

슈퍼마리오는 월드가 8개가 있고요.

각 월드 하나당 레벨 4개씩 있습니다.

그래서 이렇게 있고요.

그래서 아까 환경을 불러올 때 슈퍼마리오 월드, 원하시는 월드를 저기에다 숫자를 넣고 그다음에 그 월드 안에서 레벨을 불러오려면 세로 줄에 있는 거를 불러오면 됩니다.

숫자를 넣어주시면 되고요.

지금 여기 보시면 알겠지만 레벨 1, 2, 3, 4가 월드마다 굉장히 비슷하게 생겼는데 레벨 1과 레벨 2, 3, 4가 색깔이 좀 달라요.

색이 좀 다릅니다.

그래서 저희가 만약 1탄을 사람이 플레이를 할 수 있으면 레벨 2도 어느 정도 할 수 있을 텐데 강화학습에서는 인풋을 스테이…

인풋을 이미지 자체로 보기 때문에 레벨1, 2에서의 상황이 전혀 다른 것이라고 판단을 해요.

그렇기 위해서 전처리를 해 주는데요.

전처리가 지금 버전이 4개가 있습니다.

첫 번째는 오리지널, 두 번째는 배경을 검은색으로 바꾼 버전이고 세 번째는 뭉그러뜨려놨어요.

네 번째는 점수 또한 뭉그러뜨려서 거의 보이지 않게 형태만 보이게 바꿔 놓은 건데요.

이것에 의미가 있는 게 조금 제네럴라이제이션시켜주는 역할을 하고 있습니다.

그래서 버전 4로 이렇게 학습을 하더라도 스테이트, 이미지 자체로 보기 때문에 형태만 어느 정도 알아도 얘네는 학습을 할 수 있고요.

오히려 오리지널 버전1같이 밑에 보면 벽돌도 자세히 다르고 구름도 그림자 같은 것도 있고 이런 게 오히려 학습을 방해할 수 있다는 겁니다.

그래서 슈퍼마리오의 골은 각 레벨의 끝에 있는 깃발을 잡는 것이 목표예요.

그래서 이 강화학습을 학습시키기 위해서 아까 저희가 봤었던 리인포스먼트와 퍼니시먼트의 작용이 굉장히 중요한데요.

설정하는 것이 굉장히 중요한데 이것을 저는 이렇게 설정을 했어요.

리워드는 깃발에 가까워지면 가까워질수록 양수값을 줬고요.

그리고 목표에 도착하면 굉장히 높은 리워드를 줬습니다.

그리고 반대로 패널티를 줘서 이 슈퍼마리오가 그 상황이 왔을 때 그 행동을, 죽는 행동을 피하도록 학습을 시키기 위해서 퍼니시먼트를 줬는데요.

이것은 어떤 목표를 달성하지 못하면 음수값을 줬고 그리고 시간이 지날 때마다 시간이 400초가 있습니다.

시간이 지날 때마다 음수값을 주고 그리고 또한 깃발에서 멀어지는 행동을 하면 음수값을 줬습니다.

그래서 스테이트하고 액션을 어떻게 불러오는지에 대해서 먼저 알려드릴게요.

그래서 아까 ENW로 변수를 선언하고 옵저베이스 스페이스 하고 240, 256, 3 세팅을 하는데요.

높이, 가로, 채널값입니다.

RGB값이고요.

그리고 ENV액션스페이스 N이라고 하면 설정할 수 있는 갯수가 256개인데 그 조이스틱에서 선택할 수 있는 각각의 액션들은 6개밖에 없지만 이것을 조합해서 실행을 할 수 있는 게 256개 정도 된다는 거죠.

그런데 그 액션을 모두 계산하기가 시간도 오래 걸리고 불필요해요.

왜냐하면 제가 원하는 거는 깃발을 슈퍼마리오가 깃발을 잡게 하는 것이 목표인데 사실 불꽃을 안 쏴도 되거든요.

밑에도 필요 없을 거고 위를 보는 것도 필요없을 겁니다.

제가 슈퍼마리오한테 하게 하고 싶은 거는 깃발을 더 빨리 가는 액션을 더 효율적으로 하게 하고 싶어요.

그래서 레퍼를 사용을 해서 무브먼트를 바꿀 거예요.

그래서 심플무브먼트라고 너 가만히 있는다, 그리고 오른쪽으로 움직이는 행동, 그리고 오른쪽으로 가면서 점프하는 행동, 오른쪽으로 가면서 불꽃을 쏘는 행동 그리고 점프 그리고 혹시 모르니까 왼쪽으로 가는 액션들 이렇게 디스크립트를 바꿔줘서 ENV 변수에 넣어줬어요.

바이너리먼트인바이먼트 함수를 사용해서요.

익스플로레이션 익스플로이테이션 적절하게 사용하는 것이 중요하다고 했는데요.

그 행동을 하게 하기 위해서 입실론그리디라는 함수를 선언을 했고요.

그래서 넘파이 함수를 사용해서 어떤 입실론 값을 정해놓고 1 이하의 값을 정해놓고 입실론값보다 더 낮은 숫자가 나오면 제가 할 수 있는 액션 중에서 랜덤으로 하나를 설정을 하는 거예요.

그게 탐험을 한다고 판단을 했고 반대로 그렇지 않으면 제가 슈퍼마리오가 할 수 있는 액션들 중에서 가장 밸류, 가치가 큰 가치를 알그레스로 선언을 해서 액션에 선언을 하라고 했어요.

그래서 ENV 스텝 함수에 액션을 넣으면 환경이 액션을 받고 그에 맞는 리워드, 그다음 스테이트를 에이전트에 전달해준다고 했잖아요.

ENV스텝이라는 게 그 역할을 해 주는 거고요.

ENV스탭은 다음 액션을 받아서 다음 스테이트, 리워드, 돈 그리고 인포라는 값을 뱉습니다.

여기서 돈은 이 액션을 했을 때 마리오가 죽었는지 살았는지, 인포는 디버깅하기 위해서 필요한 정보들인데 디스턴스나 혹은 스코어가 몇인지 이런 것들입니다.

사실 이것을 학습에 넣는 거는 반칙이라고 볼 수 있죠.

그래서 이 익스플로레이션을 어떻게 설정하느냐?

하이퍼파라미터인데요.

익스플로레이션을 하면서 어떠한 기억 메모리에 넣어놓습니다.

그래서 이 기억으로 슈퍼마리오는 점점 더 좋은 액션을 선택할 수 있도록 학습을 해요.

이런 입실론 값을 가지고 익스플로레이션을 하고 있습니다.

처음에는 입실론값을 1로 줄 겁니다.

이것은 아기가 태어나서 아무거나 보면 입에 넣는데 그냥 할 수 있는 모든 액션들이 익스플로레이션인 거예요.

이 익스플로레이션을 1이라고 하는 거는 모든 상황에서 계산하지 않고 익스플로레이션을 한다는 거고요.

이거를 0.1까지 줄일 거예요.

0.1은 90%의 확률로 내가 가지고 있는 액션 중에서 가장 높은 액션을 선택을 하겠죠.

이것을 20만 번에 의해서 타임스텝에 의해서 점점점점 줄여나갈 건데요.

이것의 의미는 처음에는 탐험을 많이 하다가 학습을 하면 할수록 점점 줄어드는 역할을 하는 거죠.

그래서 그러한 액션을 선택을 하게 되면 방금 전에 말씀드린 거와 같이 넥스트 스테이트, 리워드, 돈, 인포를 주는데요.

여기서 디큐를 사용해서 어떤 메모리, 기억 장치를 만들어놓을 거예요.

그래서 얘가 하는 스테이트, 어떠한 스테이트에서 바로 여기 이미지에서 에이전트는 이런 액션을 선택을 했고 액션을 선택을 했더니 액션을 받고 환경이 어떤 리워드를 줬고 넥스트 스테이지를 줬다는 거를 업핸드를 합니다.

10만이 지나게 되면 저장공간이 넘어가게 되면 맨처음에 했었던 스테이트액션리워드를 없애고 새로운 것을 채워넣게 됩니다.

그래서 얘는 점점 더 좋은 행동들을 기억하고 있겠죠.

그래서 이러한 학습을 시키기 위해서 딥러닝을 사용을 하는데요.

텐서플로우를 사용을 했는데요.

이것을 사용해서 어떤 정답값과 그리고 그렇게 하지 않은 비헤이비어 액션 값의 로스를 줄여나갑니다.

그래서 아담 옵티마이저를 사용을 했고요.

이렇게 아담 옵티마이저를 사용을 해서 로스를 줄이는데 여기서 이제 정답이 뭐냐하면 기존의 슈퍼바이저러닝과 다르게 강화학습은 자신이 선택한 액션들 중, 그러니까 경험들 중에서 가장 좋아 보이는 거, 그런 것들을 정답으로 놓고 자기 경험에 의해서 이렇게 했더니 이 상황에서는 이 액션이 가장 좋겠구나라는 거를 왼쪽에 있는 정답으로 판단하고 오른쪽에 있는 건 어떠한 스테이트에서 입실론 그리디를 통해서 액션을 선택을 한 거예요.

이것의 차이, 로스를 줄여가면서 파라미터를 계속 갱신을 합니다.

이렇게 스테이트를 받고 딥러닝 모델을 지났는데요.

컨벌루션레이어를 지나고 풀리커넥션을 한 다음 오른쪽에 보면 각각의 액션들이 있는데요.
각각의 액션들의 가치를 다 뽑아요.

다 뽑은 뒤에 입실론 그리디를 통해서 만약에 탐험을 하게 돼야 하면 랜덤하게 선택을 하고요.

그렇지 않으면 이 가치들 중에서 가장 좋은 액션을 선택을 하겠죠.

그래서 이 더블 디큐엔이라는 알고리즘은 이렇게 학습을 합니다.

그래서 여기서 이미지 한장이 들어가지 않고요.

이미지 네 장이 들어갑니다.

그 이유는 한 장만 들어가게 되면 그 전의 상황을 알 수 없기 때문에 이렇게 네 장을 주고요.

그것을 인풋으로 받고 그 인풋으로 받아서 큐네트워크, 딥러닝 모델에 들어가게 되면 얘가 그림에서 봤듯이 모든 액션들의 가치를 백리턴하게 됩니다.

그러면 이 리턴된 액션 밸류들을 가지고 그 중에서 입실론 그리디를 가지고 익스플로이테이션 할 것인지 익스플로레이션 할 것인지 그 중에 하나 액선을 정합니다.

그래서 환경은 액션을 받아서 다음 스테이트, 리워드를 에이전트에게 전달을 합니다.

그런데 이 리플레이 메모리의 정보들을 차곡 쌓은 다음에 왼쪽에 있는 로스 줄이는 식을 사용을 해서 리플레이 메모리에서 몇 개씩을 뽑아서 학습을 해요.

그다음에 로스 줄이는, 로스가 줄어들고 파라미터가 변하기 때문에 파라미터는 Q네트워크라고 보이는 딥러닝에 적용을 하게 되고 얘는 이 반복작업을 몇천, 몇만 번 하면서 점점 액션을 배우게 됩니다.

그래서 1천 에피소드, 3천, 5천 정도 시켰는데요.

1천 번 할 때 하루 정도 소요를 했습니다.

이것은 지금 바로 죽죠.

학습이 덜 된 상태이고 여기서는 익스플로레이션 값이 한 0.8, 0.7 정도 되었던 거 같아요.

그것의 의미는 70%는 랜덤으로 액션을 선택하고 있다는 거고 그리고 30%는 저 가치를 계산을 해서 그 액션들 중에서 가장 높은 액션을 선택하겠죠.

하지만 가장 높은 액션을 선택한다고 하더라도 이게 진짜 좋은 액션은 아닐 거라고 생각을 해요.

왜냐하면 학습이 다 되어 있지 않기 때문에


그리고 3천 번


이때는 아까보다 조금 더 잘 가죠.

여기서는 죽는데 이제 패널티는 중간중간에 있는 그런 괴물들? 

괴물들이고 그리고 밑에 구멍으로 빠져나가면 퍼니시먼트니까 그런 이런 스테이트에서 이런 행동을 했었을 때 패널티를 주겠죠.

그러면 얘는 더 이상 그 상황에서 그 액션을 선택하지 않으려고 할 거예요.

5천 번이고, 4일 정도 학습을 했습니다.

여기서의 시간은 제 컴퓨터의 기준이고요.

지금 그래도 잘하고 있어요.

왼쪽으로 피하기도 하고, 성공을 했습니다.

그래서 (웃음) 이게 제가 플레이한 건 아니고요.

그리고 아까 전에 했었던 강화학습을 통해서 얘는 5천 번이라고 하는 거는 한 번 죽었을 때 에피소드 1이고요.

그리고 이러한 과정을 5천 번 정도 했을 때 얘는 깃발까지 가는 모든 스테이트들에 대해서 학습을 하게 되고 이 스테이트, 그러니까 이 이미지에서는 이러한 액션을 했더니 과거 기억을 되살려 보니까 나는 리워드를 받았어라고 알게 됩니다.

그래서 이 계속 타임 스텝, 각 프레임마다 그에 맞는 액션을 선택을 하게 되고요.

결국에는 이렇게 깃발로 가게 되는 목적을 달성하게 되는 거죠.

그래서 오늘 동물이 어떻게 배우는지, 사람이 어떻게 배우는지, 또 동물과 사람이 배우는 방법으로 강화학습이 얼마나 닮았고 그리고 강화학습을 이용해서 슈퍼마리오를 학습시켜봤는데요.

그거 말고도 여러 가지 환경들이 있어요.

오픈AI에서 제공하는 아타리, 기존에 알고 있는 벽돌 깨기, 브레이크 아웃, 카트풀 이런 것들이 오픈AI에 있고요.

알파고를 만든 딥마인드랩에서 실험을 하기 위해서 만들어 놓은 환경.

그리고 많이 아시는 스타2도 환경으로 제공을 하고 있고요.

그리고 제가 한 슈퍼마리오, 그리고 소닉, 그리고 마인크래프트 또한 강화학습 환경으로 제공이 되고 있습니다.

그리고 제공 되는 것뿐만 아니라 저희가 직접 환경을 만드는 것도 가능해요.

그래서 이 게임뿐만 아니라 실제 산업에서도 똑같이 시뮬레이터를 만들어서 그걸로 강화학습으로 문제 해결하는 것이 가능하고요.

여기서부터는 제가 했었던 것들인데 방금 전에 슈퍼마리오를 학습시키기 위해서 시켰던 알고리즘은 DDQN이라고 딥마인드가 아탈이 벽돌깨기를 했었던 학습 방법입니다.

그런데 이 방법 말고도 저희는 액션 밸류를 이렇게 업데이트를 했는데 그거 말고 폴리시, 정책 자체 업데이트 방법도 있고 아까 보여드렸던 여러 가지를 환경에 따라서 그에 맞는 알고리즘을 써야 합니다.

지금 보이는 소닉 같은 경우에는 여러 가지 DQN에 덧붙여서 여러 가지 것들을 한 건데 이거는 슈퍼마리오보다 훨씬 어렵기 때문에 조금 더 복잡한 학습 알고리즘을 사용을 했고요.

오른쪽은 DDPG라는 알고리즘을 사용을 했는데 환경이 뼈다귀가 걷는건데 여기서 근육들이 작용을 합니다.

여기서 아웃풋은 조이스틱처럼 사방이 아니라 뼈다귀가 걷기 위해서 각각 근육들의 가치들이 있어요.

그래서 컨티뉴스한 액션들이라고 하고 이거에 맞는 알고리즘을 잘 사용하는 것이 굉장히 중요합니다.

그리고 또한 사람이 배우는 방법 같이 사람은 한번에 어려운 것을 배우지 못하니까 단계별로 배우는 커리큘럼 러닝 또한 존재를 해요.

그래서 저는 저 높은 월을 높게 하는 에이전트를 만들고 싶은 건데 한 번에 배우기 어려우니까 왼쪽 아래와 같이 유니티라는 거를 사용해서 환경을 만들었고요.

그래서 점점 레벨이 가면 갈수록 저 월에 대한 높이가 올라가죠.

그래서 결국은 학습 속도가 더 빠르다라는 거고요.

그리고 이미테이션 러닝 또한 존재를 하는데요.

선생님이 있고요. 

학생이 있어서 학생은 선생님이 하는 행동을 보고 그대로 배웁니다.


왼쪽이 선생님, 오른쪽이 학생인데 왼쪽은 저예요.

제가 잘 못해서 학생이 학습을 잘 하지 못하더라고요.

그래서 선생님이 굉장히...

중요하죠.

그래서 보면 제가 왼쪽이 저인데 제가 잘 못하죠.

강화학습은 좋아하는데 게임은 잘 못합니다.

그래서 신기한 게 얘가 진짜 제가 하는 거 그대로 따라해요.

이거를 왜 하느냐 하면 강화학습을 학습시키는 데 굉장히 시간이 오래 걸립니다.

아까 슈퍼마리오, 간단한 환경조차도...

계속 하네요.

환경조차도 한 4일 정도 걸렸는데 점점점점 더 어려운 환경, 태스크, 혹은 복잡한 목표를 달성하기 위해서 강화학습으로 에이전트를 학습시키는 거는 굉장히 오랜 시간이 걸립니다.
그래서 초기 학습시간을 줄이기 위해서 이미테이션 러닝을 통해서 어느 정도 학습 시간줄이는 그런 시도들이 되고 있고요.

그래서 이 파이썬과 딥러닝, 파이썬을 사용한 딥러닝 라이브러리, 그리고 제가 보여드렸던 환경, 강화학습 알고리즘만 있으면 학습을 하는 것이 가능합니다.

그래서 오늘 제가 보여드렸던 거는 파이썬 언어를 사용해서 집에 가셔서 제 깃헙을 보시고 인공지능 슈퍼마리오를 만들어보시는 건 어떨까요.

오늘 제 발표는 여기서 마치도록 하겠습니다.

감사합니다.
