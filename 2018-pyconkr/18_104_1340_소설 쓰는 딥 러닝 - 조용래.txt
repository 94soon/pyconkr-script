https://www.youtube.com/watch?v=03mEWRC2hyA

안녕하세요? 

저는 소설 쓰는 딥 러닝이라는 발표를 준비하고 있는 조용래입니다.

발표를 시작하기 전에 저를 짧게 소개해드리면 제 이름은 조용래고요.

온라인에서는 드림곤플라이라는 닉네임을 사용하고 있습니다.

제가 처음 파이썬을 시작한 건 대학교 쯤에 쟝고로 만들기 시작한 때부터고요.

그리고 파이썬 라이브러리를 공부해왔고 현재는 딥러닝과 자연어처리방식을 하고 있습니다.

오늘 발표드릴 프로젝트는 저 혼자 준비한 것은 아니고요.

제 친구들과 함께 준비했습니다.
이 자리에도 와 있는데요.

인공지능 작사와 작곡 서비스를 개발하고 있는 포자랩스라는 스타트업의 김범중, 노혜미, 정구봉, 허원길 그리고 김우정 이렇게 저 6명이서 준비한 프로젝트입니다.

그러면 본격적으로 소설 쓰기에 대한 이야기를 해볼까요?

소설을 쓰기 전에 일단 소설답기 위해서는 어때야 될까요?

가장 기본적으로 적어도 글을 읽을 수 있으려면 문법적으로 어색한 부분이 없어야 될 것 같습니다.

그리고 문장과 문장 사이의 문맥이 자연스럽게 이루어져야겠죠.

또한 새로운 이야기만 좋을 것 같습니다.

재미와 감동과 교훈과 인생의 진리와 그리고 우주의 이치까지 있으면 좋겠지만 너무 많은 걸 바라지는 맙시다.

인공지능은 아직 어리고 모자란 부분이 많으니까요.

한국어는 언어적으로 교착어라고 합니다.

교착어라는 건 한 단어 안에 형태소들의 모양이 살아 있는 언어를 말하는데요.

예를 들면 가시었겠다라는 각 단어의 경우 가 시 었겠 다 이렇게 분석을 할 수가 있죠.

한국어는 어간과 접사, 어미, 조사가 한단어 안에 붙어 있는 형태로 되어 있기 때문에 이를 적절히 나누어주는 것이 분석을 쉽게 할 수 있는 방법입니다.

두 번째 장애물은 일반적으로 기억력이 나쁘다는 것입니다.

사람이 이야기할 때는 1시간 전에 했던 이야기도 이어서 할 때가 있죠.

그리고 글을 쓸 때도 결론을 내면서 처음도입했던 이야기를 이용하면서 글을 마치기도 합니다.

이 예시는 혹시 아시는 분들은 아시겠지만 칼의 노래라는 소설의 첫 문장인데요.

한번 읽어보겠습니다.

버려진 섬마다 꽃이 피었다.

꽃피는 숲에 저녁 노을이 비치어 구름처럼 부풀어 오른 섬들은 바다에 결박된 사슬을 풀고 어두워지는 수평선 너머로 흘러가는 듯 싶었다.

이 흘러가는을 이해하기 위해서 섬들을 봐야 됩니다.

이걸 이해하기 위해서는 버려진을 봐야겠죠.

그런데 아직까지 딥러닝에게 이런 오래된 단어를 기억한다는 것은 굉장히 어려운 것입니다.

인공지능은 몇 문장 전에 이야기한 것도 기억을 못합니다.

이 문제를 장기 의존성을 잘 학습하지 못하면 그러는 거라고 하는데요.

문맥이 자연스럽게 이어지는 글을 쓰지 못하는 가장 큰 문제입니다.

세 번째 장애물은 시간입니다.

딥러닝은 학습하는데 오랜 시간이 걸리죠.

짧으면 몇 시간에서 길면 며칠까지 걸려야 원하는 결과를 얻을 수 있습니다.

딥러닝으로 여러 가지 시도를 하는데 큰 장애물이 되기도 합니다.

이렇게 한국어 특성을 고려하고 장기 의존성을 학습해야 하고 빠른 학습을 가능하게 하는 것이 소설 쓰기의 큰 장애물이고요.

이런 문제들을 저희가 어떻게 조금이나마 해결하기 위해서 방법을 취했는지에 대해서 얘기를 해보도록 하겠습니다.

크게 토크나이징, 트랜스포머, 메모리 네트워크, 제너레이트 앤 랭크 4가지로 나누어서 소개를 드릴 건데요.

하나하나씩 차례로 나가보겠습니다.

먼저 토크나이징입니다.

한국어의 특성을 잘 이용하기 위해서 텍스트를 토큰 단위로 나눈 토큰라이징이라는 과정이 필요하죠.

사랑해, 사랑했다, 사랑할게라는 단어를 그대로 컴퓨터에 넣으면 이것들은 컴퓨터 입장에서는 전혀 다른 각각의 단어들입니다.

전혀 공통점을 발견하기가 굉장히 어렵겠죠.

그런데 이를 사랑 해, 사랑 했다, 사랑 할게로 나누게 되면 컴퓨터는 이제 이들이 전혀 다른 단어들이 아니고 서로 공통점이 있다는 걸 알 수 있기 때문에 쉽게 텍스트를 이해할 수 있습니다.

이를 토크나이징이라고 하고요.

파이썬에서 쓸 수 있는 라이브러리는 대표적으로 두 가지가 있습니다.


두 가지는 서버 입장에서 라이브러리의 차이점을 잠깐만 비교해보겠습니다.

공교롭게도 두 라이브 모두 발표된 적이 있습니다.

파이콘에서.
어떻게 작동하는지를 잠깐 보면 KoNLPy는 Kkokkoma, Mecab, Twitter 등 학습이 완료된 형태소 분석기들을 파이콘에서 쓸 수 있게 래핑하는 것입니다.

이에 비해서 이것은 자신이 가진 데이터를 비지도학습 모델을 학습시켜서 사용하는 것입니다.

일단 간단한 차이부터 보면 KoNLPy에 보면 자바 라이브러리를 파이썬으로 래핑한 프로그램이기 때문에 이것들을 설치해야 돼서 데이터베이스에서 쓰기가 쉽지 않죠.
이에 비해서 오른쪽은 순수 파이썬 패키지입니다.

좀 더 다른 측면에서 보면 퍼포먼스 측면에서 KoNLPy는 항상 동일한 결과를 제공합니다.

소이엔엘피는 데이터에 따라서 결과가 달라질 수 있습니다.

그래서 제 개인적으로 정리해보자면 신조어가 많이 있을 때는 소이엔엘피를 쓰면 되고요.

그렇지 않을 때는 KoNLPy를 쓰면 될 것 같습니다.

저희는 이 두 개를 어느 정도 썼고요.

추가적으로 저희는 판타지 소설들을 토큰나이징할 때 인명 같은 거 인식하기 어렵잖아요.

그래서 이거를 만들어서 같이 했습니다.

일명 노가다라고 하죠.

이렇게 토큰나이징을 사용해서 언어들의 특징을 많이 분석해봤습니다.

여기까지가 전처리에 대한 내용이었고요.

이제 본격적으로 저희의 메인 모델 설명으로 넘어가 보겠습니다.

메인 모델은 트랜스포머라는 모델인데요.

모델은 작년 이맘때쯤에 처음 발표된, 구글에서 발표했던 겁니다.

본문의 제목이 어텐션 이즈 올 유 니드(Attention is all you need)입니다.

‘당신에게 필요한 건 어텐션이다’죠.
그러면 어텐션에 대해서 알아볼게요.

이렇게 짧게 알아보겠습니다.

어텐션을 전혀 모르시는 분한테 어텐션을 가장 쉽게 이야기하는 방법은 변형에서 얼라인먼트라는 방식으로 이야기할 수 있습니다.

나는 학교에 간다라는 걸 영어로 말하면 아이 고 투 스쿨이죠.

나는 아이에 해당하고 학교에는 투 스쿨에 해당하고 간다는 고.

한국어 어순이 다르죠.

이렇게 변형할 때 어순을 맞춰주기 위한 연구가 많이 있었는데요.

이를 얼라인먼트라고 하고요.

딥 러닝으로 변형을 하기 훨씬 전부터 오랫동안 연구해왔던 겁니다.

딥 러닝을 이용한 변형을 활용하면서 한 게 어텐션입니다.

아이를 만들 때는 나는이라는 거에 집중하고 투 스쿨이라는 거는 학교라는 단어에 더 정보를 많이 이용하고 간다를 할 때는 고를 연결시키는 개념입니다.

어디에 제출해야 될 것이라는 그런 결정은 이런 소스 단어의 정보와 타깃 단어의 정보를 조합해서 판단해서 어텐션이라는 걸 하는데요.

이거는 얼라인먼트 입장에서 어텐션을 한 거고요.

좀 더 포괄하게 어텐션을 이해하기 위해서는 쿼리, 키, 밸류라는 걸 알아두시면 좋습니다.

이는 파이썬에 여러분이 익숙하신 딕셔너리에 대응시켜서 이해해볼 수 있는데요.

흰색이 화이트고 노란색이 옐로고 파란색이 블루라는 파이썬의 딕셔너리가 있을 때 당연히 이거를 키라는 밸류가 맵핑되어 있다고 부르죠.

그리고 노란색이라는 쿼리가 들어왔을 때 그에 매칭되는 키를 찾아서 해당되는 밸류를 해줍니다.

여러분이 너무나도 익숙한 파이썬의 딕셔너리 개념입니다.

이 파이썬의 딕쳐너리의 비유에서 어텐션을 이해해 보면 어텐션 모듈은 쿼리가 들어오면 정확히 일치하는 키를 찾아주는 건 아니고요.

쿼리 정보와 키 정보 간의 유사도, 연관성을 어텐션 모듈이 분석을 해서
밸류를 값을 리턴하는 그런 기능들입니다.

예를 들어 노란색 대신에 금색이라는 좀 다른 단어가 들어왔을 때도 어텐션 모듈에서 금색 쿼리와 노란색 키에 해당되는 그런 정보들이 유사도가 높다고 판단되면 해당되는 밸류의 값에 높은 값을 주어서 값을 가져올 수 있죠.

이게 어텐션의 개념입니다.

그러면 이거를 조금 더 확장시켜서 셀프 어텐션이라는 개념이 있습니다.

자기 자신에게 어텐션을 준다.

좀 이상하게 처음에 느끼실 수 있는데요.

사람도 어떤 단어를 이해하기 위해서 문장의 다른 부분을 봐야 할 때가 있습니다.

예를 들면 언제 떠올랐는지 모를 그믐달이 동녘 하늘에 비스듬히 걸려 있었다라는 게 있을 때 걸려라는 단어를 이해하기 위해서 그믐달이라는 단어를 봐야겠죠.

이처럼 셀프 어텐션도 문장을 이해하기 위해서 그 문장 자체를, 다른 단어의 정보를 이용해서 이해하기.

그래서 쿼리는 걸려라는 단어에 적용하면 되고 키랑 밸류는 문장에 있는 각각의 다른 단어들이 정보화 된 것입니다.

그중에서 어텐션 모듈이 걸려와 그믐달이라는 주 단어에 연관성이 높다고 판단을 하면 그믐달이에 높은 가중치를 주어서 정보를 가져올 수 있습니다.

그래서 한걸음 더 나아가서 사람은 때에 따라서 여러 각도에서 단어를 이해하기 위해서 여러 부분의 문장을 볼 때가 있어요.

예를 들면 걸려라는 단어를 이해하는데 어떻게 걸렸는지 이해하기 위해서 비스듬히라는 단어를 봐야 되고요.

문법적인 부분을 이해하기 위해서는 뒤에 있는 있었다라는 말을 봐야겠죠.

이렇게 한 번에 여러 단어의, 각각 다르게 어텐션을 주는 게 필요할 때가 있고요.

이 방법이 바로 멀티 헤드 어텐션이라는 겁니다.

하나의 단어를 조각으로 쪼개고 각 조각마다 따로 따로 어텐션 모듈을 만들어서 그 어텐션을 계산해서 정보를 가져오는 것이죠.

이렇게 되면 어텐션마다 자연스럽게 다른 종류의 정보를 학습할 수가 있겠고요.

이 멀티 헤드 어텐션은 트랜스포머라는 것의 핵심적인 부분입니다.

그러면 여기까지 이해를 하셨다면 이제 트랜스포서 모델로 돌아와보겠습니다.

이 모델은 논문 제목 그대로, 인코더와 디코더로 나눌 수 있습니다.

인코더는 소스 문장의 정보를 추출하는 것입니다.

여기서 소스 문장이란 번역의 경우 우리가 번역하는 문장이고 소스를 만드는 것에서는 이전문장 혹은 이전문장들이 되겠죠.

여기에 정보를 해서 추출을 하려고 하는데 어떻게 하냐.

일단 소스 문장이 들어오면 통과합니다.

텍스트가 갖고 있는 정보를 잘 표현하는 숫자로 바꾸는 것입니다.

숫자로 바꿔야 계산을 할 수가 있겠죠?

예를 들면 버려진 섬마다 꽃이 피었다라는 문장도 통과하면 각 단어마다 벡터로 바꿔서 할 수 있습니다.

이거는 간단하게 이해해 보면 단어가 문장에서 몇 번째 단어였는지에 따라서 숫자를 조금씩 바꿔서 더해줍니다.

예를 들면 첫 번째 단어에는 숫자를 더해주고 두 번째 숫자에는 다른 숫자를 더해주면 이 값을 보고서 이 단어가 어느 문장의 몇 번째 값일 거라는 거를 추출을 할 수 있겠죠.

이렇게 숫자로 바뀐 문장은 아까 설명을 드렸던 셀프 어텐션 모듈을 통과합니다.

아까 말씀드렸듯이 문장 전체를 보고서 문장을 다시 이해하는 것이고요.

그 뒤에 피드 포워드 네트워크, 레이어 노멀리제이션, 레지듀얼 커넥션이라는 개념이 있는데 이거는 일반적으로 성능을 높이기 위해서 쓰이는 방법이니까 그렇게만 알고 계셔도 좋습니다.

여기까지를 한 레이어라고 부릅니다.

그리고 이거를 N번 반복하는데요.

보통 N은 항상 6번 정도로 트랜스포머에서는 반복을 하고 있습니다.

이렇게 하고 나면 소스 문장의 각 단어가 의미가 충분히 추출된 숫자로 표현된다고 되는 것입니다.

그러면 버려진 섬마다 꽃이 피었다는 문장이 숫자들로 표현이 될 수 있어요.

이제 디코더를 볼 차례인데요.

예를 들면 아까 말씀드렸던 ‘버려진 섬마다 꽃이 피었다’라는 소스 문장 다음에 ‘꽃피는 숲에 저녁 노을이’까지 만들어낸 상황입니다.

그 다음 단어로 비치어라는 단어를 만들어야 된다는 상황이라고 해볼게요.

먼저 지금까지 자신이 쓴 문장이 무엇인지를 일단 이해를 해야겠죠.

그래야 다음까지 나갈 수가 있으니까요.

그래서 이미 생성된 문장에 대해서 셀프 어텐션 포워드 네트워크와 똑같은 방법을 거칩니다.

그 외 인코더와 다른 부분이 딱 하나 있는데 그게 바로 소스 문장에 어텐션을 주는 겁니다.

즉 이전 문장을 한번 보겠다는 거죠.

현재 문장을 보면서 이전 문장도 한번 보겠다.

이 과정 역시 N번 반복합니다.

그러고 나면 이제 다음 단어를 예측할 수 있게 됩니다.

즉 직관적으로 설명드리면 현재 문장에서 지금까지 생성된 단어들을 한번 봤고 그 이전 문장들까지 한번 봤으니 그러면 그다음에 나올 단어는 뭘까? 한번 맞혀 봐, 이 문제를 푸는 거죠.

그런 문장입니다.

여기까지 트랜스포머라는 모델에 대해서 개발적으로 한번 알아봤는데요.

트랜스포머의 장점은 여러 가지가 있습니다.

일단 첫 번째는 앞서 처음에 살펴드렸던 장기 의존성 문제를 어느 정도 해결할 수 있다인데요.

기존에 RNN 계열 모델, 스탠다드 모델이라고 할 수 있는데요.

이 모델은 정보를 순차적으로 계산을 합니다.

예를 들면 열 단어가 있으면 열 단어를 보고서 세 단어를 하려면 10개의 스텝을 거쳐서 그다음으로 이런 식으로 10개의 스텝을 거쳐오면서 정보를 읽기가 쉬웠는데요.

하지만 트랜스포머는 어텐션 모듈만 잘 된다면 현재 단어를 생성할 때 바로 열 단어 전의 단어에 모듈을 주어서 그걸 가져올 수 있습니다.

그래서 장기 의존성 학습이 좀 더 유리하고요.

물론 소스 이전 문장에서 할 수 있는 건 할 수 없기 때문에 완벽하다고 할 수 없어서 세모라고 할 수 있습니다.

두 번째 문장은 학습 속도입니다.

정보를 순차적으로 계산합니다.

그래서 열 개의 단어가 있으면 10번째 단어를 이야기하고 싶으면 9번째 단어를 보고 이야기할 수 있겠죠.

하지만 이에 비해서 트랜스포머는 병렬적으로 하기 때문에 훨씬 기존의 RNN 계열 모델보다 빠르게 학습을 할 수가 있습니다.

수치상으로는 약 30배 이상 빨라진다고 하네요.

그래서 학습 시간이 상당히 빨라졌으므로 시간은 그래도 조금 완화되었다고 할 수 있을 것 같습니다.

여담으로 인코더, 디코더 모델을 설명드렸는데요.

이 프레임워크는 소설 쓰기에만 있는 건 아닌데요.

다른 것에도 쓰일 수 있는 겁니다.

이런 문제에서 인코더, 디코더 모듈이 흔히 있고요.

트랜스포머가 조금 혁신적인 거기 때문에 이거로 다른 문제들을 접근해보는 것들이 많습니다.

트랜스포머까지 어느 정도 설명을 끝냈고요.

저희가 썼던 두 번째 모델인 메모리 네트워크를 넘어가볼 건데요.

앞서 트랜스포머의 좋은 점들을 여러 가지 이야기했지만 물론 당연히 아쉬운 점도 있습니다.

예를 들면 인코더에 들어가는 문장이 길어지면 한 단어, 단어마다 그 문장의 모든 단어들의 어텐션을 계산해야 되기 때문에 계산 문자도가 길이의 제곱에 비례해서 증가합니다.

그래서 인코더로 들어오는 문장의 길이가 어느 정도 정해져 있어요.

그런데 소설을 만들려고 하면 앞에 한 문장, 다섯 문장, 열 문장까지 보고 싶을 때가 있습니다.

그렇기 때문에 소스 문장이 길어지면 길어질수록 전체를 계산해야 되니까 계산이 힘들어지겠죠.

이를 극복하기 위해서 저희는 메모리 네트워크에서 아이디어를 가져왔는데요.

메모리 네트워크는 원래 퀘스쳔 앤서링을 풀기 위한 모델입니다.

퀘스쳔 앤서링 태스크라는 게 어떤 거냐면 지금 왼쪽에서 보시다시피 이런 식으로 문장들이 순서대로 주어지고요.

이게 우리가 문제 풀 때 지문 같은 거라고 보시면 되고요.

퀘스쳔이 있습니다.

‘웨얼 이즈 더 애플?’이라고 물으면 ‘베드룸’이라고 답을 하는 게 이 모델입니다.

지문 전체를 임베딩을 합니다.

센텐스마다 벡터 하나씩 래핑을 해서 메모리로 두는 거죠.

마치 곳간처럼 쌓아놓고 질문이 들어왔을 때 그 질문에 해당하는 지문의 문장을 찾아서 리턴을 해주는 문장입니다.

물론 여기서 퀘스쳔에 해당하는 문장을 찾기 위해서 어텐션 개념이 쓰이고요.

그렇게 쓰는 모델인데요.

여기서 아이디어를 좀 가져와서 메모리 네트워크의 비문학 지문을 넣는 게 아니고 지금까지 생성해낸 그리고 입력을 받았던 문장 전체를 인배딩을 해놓고 메모리를 만들듯이 소설의 직전 문장을 허위로 넣는 것입니다.

지금까지 질문을 하듯이.

그렇게 하면 메모리 네트워크에서 질문이 들어오면 해당하는 값을 찾아 리턴을 하듯이 질문 대신 문장이 들어오면 이전 문장들의 연관이 있는 문장을 추출해서 그다음 문장을 제너레이션하는데 쓸 수 있게 만든 거죠.

트랜스포머에서는 단어 기준으로 앞선 3개에서 5개 문장으로 한 100단어 정도까지 보는 것이 한계였다면 메모리 네트워크를 통하면 효율적으로 이전의 여러 문장을 볼 수 있게 됩니다.

핵심은 이전 문장을 단어별로 보는 게 아니고 문장으로 봐서 문장 임베딩을 통해서 메모리로 저장을 하는 것이죠.

이 덕분에 장기 의존성 문제는 조금 더 해결할 수 있었습니다.

하지만 여전히 완벽한 건 아니라서 세모 정도로 해놓고요.

이게 저희의 모델의 요약인데요.

좀 복잡하시겠지만 나누어서 살펴보시면 이 부분이 메모리 네트워크 센텐스 임베딩이라는 부분이고 여기가 앞서 설명드렸던 트랜스포머고요.

디코더 그리고 제너레이트 앤 랭크 부분입니다.

제너레이트 앤 랭크 부분을 설명을 아직 안 드렸는데요.

여기에 해당되는 게 랭킹 모델인데 여기에 대해서 좀 알아보겠습니다.

빔 서치라는 거에 대해서 설명을 드릴게요.

지금까지 설명을 드리면서 새로운 문장을 생성할 때는 한 단어, 한 단어씩 생성하는 거로 가정을 하고 사용을 손해했어요.

사용을 했어요.

그런데 한 단어씩 만들 경우에는 문제가 있습니다.

우리도 말을 할 때 한 단어씩만 생각하고 말하지 않잖아요.

한 단어씩만 생각해서 말하다 보면 나중에 주어와 서술어의 호응이 맞지 않거나 문장이 길게 되거나 그런 경우가 십상입니다.

그래서 이를 보완하기 위해서 매 스텝마다 단어를 하나를 보내는 게 아니고 후보를 골라내는 그런 기능인데요.

그리고 문장을 끝까지 여러 개로 만들어보고 만들어진 문장을 평가해서 가장 괜찮은 문장을 최종적으로 반환하는 모델입니다.

이렇게 하면 비문이 없고 좀 더 그럴듯한 말을 생성할 것 같습니다.

딥 러닝 모델은 이런 부분에서 뛰어나요.

그래서 말이 되는 자연스러운 말을 할 수 있는 부분이 조금 좋아서 동그라미를 줄 수 있을 것 같습니다.

그리고 랭킹인데요.

문법적으로 자연스러운 것만으로는 약간 부족하게 느껴질 수 있습니다.

우리는 때때로 딥러닝 모델이 했던 말을 또 하고 또 하고 또 하는 거를 쉽게 볼 수가 있어요.

왜냐하면 가장 문법적으로 그럴 듯한 문장이기 때문에 2번, 3번 말해도 이상하지 않은 거죠.

그렇기 때문에 랭킹 모듈이 필요합니다.

여러 문장 후보 중에서 지금까지 생성한 문장들과 얼마나 다른 참신한 문장인지 평가를 해서 거기에 따라 랭킹을 먹여서 최종적으로 반환하는 그런 모듈이 필요한데요.

이렇게 하면 물론 완전히 새로운 이야기까지 나오지 않겠지만 적어도 앞에서 했던 말을 똑같이 반복하지는 않을 수 있겠죠.

그래서 이런 점에서 새로운 이야기 생성에 세모 정도 줄 수 있을 것 같습니다.

결과를 보여드리기 전에 마지막으로 파이썬의 딥러닝 프레임워크에 대해서 잠깐 짧게 얘기하고 넘어가도록 하겠습니다.

최근 들어 딥러닝이 굉장히 핫해지고 딥러닝 구현을 한 사람들이 많은데요.

이 구현에는 파이썬이 절대 지배적이죠.

여기서는 파이썬 딥러닝 프레임워크들이 큰 역할을 했습니다.

물론 다른 프레임워크도 있지만 가장 대표적으로 텐서플로우와 파이토치가 있는데요.

이 두 개만 비교하고 넘어가도록 하겠습니다.

근본적인 차이는 스태틱 그래프와 다이내믹 그래프입니다.

스태틱 그래프란 이것을 실행하기 전에 다 설계를 해놓고 이제 다 만들어놓고 그다음에 실행을 시키는 모델인데요.

그리고 다이내믹 그래프는 고정되어있지 않고 계산 중간중간에 얼마든지 바꿀 수 있는 그러한 모델입니다.

이렇게 왼쪽이 텐서플로우의 개념으로 그래프가 고정되어 있고 흘러가는 것만 보여주고 있고요.

오른쪽 파이토치는 계속 바뀔 수 있는 형식.

프레임워크의 장점과 단점, 뭐가 좋다, 나쁘다가 아니고 장점과 단점이 여기 고무적인 차이에서 나오는데요.

텐서플로우의 경우에 마치 자바나 C가 스태틱 그래프를 갖고 있어서 생긴 문제점처럼 쉽다고 할 수 있습니다.

그리고 파이토치는 중간중간에 디버깅을 하고 이런 게 좀 더 좋죠.

그래서 이 둘 중에 어느 게 낫다고 하기보다는 우리가 웹사이트를 만들 때 자바 같은 언어의 안정성이나 속도가 필요할 때가 있고 파이썬같이 빠른 개발과 빠른 개발을 하면서도 준수한 성능을 원하는 데가 있을 겁니다.

당연한 얘기지만 정답이 있다기 보다는 자신의 상황에 맞는 프레임워크를 선택하면 될 것 같은데요.

파이토치가 파이썬에 가까운 거로 보고 있고 텐서플로우는 스태틱한 걸 좋아하는 사람들이 많이 쓰는 것 같습니다.

저희도 이 프로젝트를 하면서 텐서플로우가 편한 사람은 이걸 쓰고 파이토치가 편한 사람은 이걸 쓰게 했으니까 편하신 걸 쓰면 좋을 것 같고 그래서 오래 기다리셨습니다.

이제 결과를 보여드리기 전에 그전에 저희가 어떻게 만들었는지를 설명을 드릴게요.

저희 모델은 처음 다섯 문장은 사람이 입력을 하고요.

그 뒤에 이거를 소스 문장을 해서 그 뒤부터는 모델이 10개의 후보 문장을 생성을 하고요.

사람이 그중에 하나씩 골라서 글을 이어나가게 됩니다.

사람의 개입이 완전 없지는 않지만 어떤 문장이 후보가 될지는 전적으로 모델이 결정을 하니까요.

이렇게 반복해서 소설 만들기고요.


-----------------------------------------------------------------------------------------
끝이 뭔가 깔끔하지 않은 것 같습니다…?
