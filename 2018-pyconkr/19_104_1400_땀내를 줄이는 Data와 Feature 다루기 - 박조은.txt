안녕하세요?

저는 땀내를 줄이는 데이터와 피쳐 다루기라는 발표를 하게 된 박조은이라고 합니다.

제가 이렇게 용기 내서 발표를 하게 된 이유는 파이콘에서 처음 발표를 해보는데 페이스북 그룹의 파이썬 사용자 그룹에 이런 글을 올려주셨어요.

파이썬을 쓰는 개발자, 어떤 일을 하든 상관없이 사는 이야기라든지 그리고 땀내나는 이야기를 해달라.

그래서 제가 하고 있는 일 중에 가장 땀내가 나고 있는 일이 무엇일까 생각을 해보았어요.

그래서 발표 제목처럼 땀내를 줄이는 데이터와 피쳐 다루기라는 발표를 하게 되었습니다.

간단하게 제 소개를 하자면 저희 아이가 그려준 제 모습인데요.

(웃음)

저는 저희 아이의…

그림일기에서 발췌를 했는데 저는 빨리 숙제하라고 잔소리를 하고 있는 모습이고 저는 옆에서 머신러닝 프로그램 책을 보고 있고 저희 아이는 앞에서 열심히 숙제를, 그다음 둘째 아이는 옆에서 열심히 한글을 쓰고 있는 모습입니다.

평소 제가 집에 있을 때 아이들과 있을 때 주로 이런 모습으로 있고요.

그래서 오늘 이야기는 저에게 있어서 땀내가 많이 났던 이야기를 해보려고 해요.

그리고 저는 수학과 통계에 대한 지식이 거의 없어요.

저의 수학 지식은 고등학교 때 수능을 봤을 때 수학 지식에 멈춰 있는데 수능을 볼 때 수학도 그다지 못 봤던 사람이고 통계도 평균이라든지 이런 것들밖에 잘 몰라요.

그래서 그런 사람이 어떻게 데이터 분석을 하고 기계학습을 하면서 얼마나 땀내를 내게 되는지, 그래서 그런 땀내를 좀 줄여봤으면 좋겠다는 생각으로 그런 땀내를 줄이는 스킬을 좀 얘기해봐야겠다 생각을 했는데 발표를 준비하면서 아무리 생각을 해도...



(웃음) 

도메인이라든지 처리해야 하는 상황에 따라서 뾰족한 방법을 찾기는 참 어렵더라고요.

그래서 좀 망했다는 생각이 들었어요.

그래서 작년 Kaggle Servey 1년 전에 했었는데 사람들이 개인 프로젝트들을 하면서 어떤 부분이 가장 힘든지에 대해서 응답을 해 주었는데, 대부분 데이터를 전처리 하거나 지저분한 데이터를 다듬거나 아니면 그런 것들을 처리하거나 이런 것들이 많이 힘들다고 응답을 해 주셨어요.

그래서 이거를 그냥 텍스트 데이터만 뽑아서 워드표로 그려봤어요.

여기도 보면 저기 A헷으로 되어서 4분의 3 글씨, 저런 글씨들을 전처리를 하고 워드 클라우드를 그렸다면 이 워드 클라우드가 조금 더 예쁘게 그려졌겠죠.

그런데 이 워드 클라우드는 전처리를 하고 그리지 않았기 때문에 오늘 그런 전처리를 할 수 있는 몇 가지 방법을 소개를 해보려고 합니다.

원래 제가 발표를 처음에 신청 했던 거는 땀내를 줄이는 피쳐 엔지니어링이었는데 데이터 전처리를 두고 얘기할 수가 없어서 중간에 제목을 바꿨습니다.



땀내를 줄이는 데이터와 피쳐 다루기라고 발표자료에 있는 코드는 판다스, 넘파이, 사이킷런, 플론나인으로 작성이 되었습니다.

그리고 플론나인이라는 시각화 툴을 설명하면 파이썬으로 시각화를 할 때 맷플랏립을 사용을 하는데 이게 복잡하고 사용방법이 어려우니까 씨본을 통해서 많이 사용을 하는데 이것도 씨본처럼 맷플랏립을 사용하기 쉽게 매핑을 해놓은 라이브러리인데 플론나인 같은 경우에는 아래에서 쓸 수 있는 지지플로문법을 그대로 쓸 수 있게 해놨어요.

그래서 여기 알을 쓰시다가 파이썬을 써야겠다 생각하시는 분들도 있으실 텐데 그분이 플론나인을 쓰면 아래에서 썼던 문법을 그대로 파이썬에서 쓰시면서 그램 오브 그래픽스로 데이터를 시각화해보실 수가 있어요.

그리고 머신러닝의 파이프라인을 간단하게 소개를 하고 넘어가면요.

일단 데이터를 획득을 하고 획득한 데이터를 전처리하고 그래서 전처리한 데이터를 바탕으로 피쳐를 만들 거예요.

그런데 피처를 만들 때 피처 엔지니어링에서 특정 피처 선택한다든지 아니면 그 피처를 변환한다든지 크기 조절, 스케일링 하는 작업을 하고 모델에 넣어줄 거예요.

그래서 학습을 시키고 그다음에 평가까지 하는 과정으로 머신러닝 파이프라인이 구성이 되는데 여기서 저는 전체 과정 중에서 주로 데이터 프리프로세싱과 피처 엔지니어링을 오늘 발표에서 다룰 예정입니다.

우리가 쓰려는 데이터, 저희가 여러 가지 경로에서 데이터를 획득할 수 있을 거예요.

직접 로그를 수집, 아니면 공공 데이터를 다운 받아서 쓴다든지 크롤링을 한다든지 할 텐데요.

제가 공공 데이터 포털에서 좀 샘플 데이터로 쓸 수 있는 게 뭐가 있을까 하고 찾아 봤어요.

그런데 해외 여행자 통관 규정이 있는데 저도 여행을 좋아하기 때문에 이 데이터를 보면 재미있겠다 하고 다운을 받았는데 다HWP로 되어 있었는데 이거를 실제 여행사에서 사용을 해서 놀라웠어요.

이 데이터를 열어보면 다른 포멧에 한글 포멧에 그 나라의 통관 규정이 들어 있어요.
그래서 상가 업소 정보라고 해서 다운을 받았어요.

스킷타워플롯으로 찍어봤는데 이상한 점 2개가 찍혀서 이 점 2개는 뭘까 하고 데이터를 봤어요.

그랬더니 이 두 데이터가 뭘까요.

위경도로 찍었어요.

참고로

-서울하고 부산.

-서울과 부산이었어요.

전국의 데이터가 CSV 파일 여러 개로 나눠져 있는 거예요.

그래서 내가 원하는 데이터를 처리를 하려면 뭔가 전처리가 꼭 필요한 거죠.

그래서 저는 이제 아파트의 분양가격도 궁금해서 아파트의 분양가 최근 추세를 보고 싶었어요.

그래서 공공데이터에서 다운을 받았는데 역시 이 포멧도 잘 모르겠고 엑셀 파일에서 열었을 때 잘 정리된 테이블로 있는 거 같은데 년도, 월별로 나뉘어져 있어요.

그런데 이런 데이터, 나뉘어져 있는 데이터를 의미있게 만드려면 합치거나 전처리를 해 주어야 하잖아요.

그래서 수십 년 간의 데이터를 보고 싶은데, 제가 이거는 이번 파이콘 튜토리얼에서 데이터 전처리를 진행했던 것 중 하나인데 2013년 데이터부터 CSV 형태로 다운 받을 수 있고 그 전의 데이터는 HWP인데 그거까지는 포기를 했어요.

대부분의 데이터는 우리가 원하는 대로 되어 있지 않아요.

그래서 데이터와 피쳐를 한번 얘기하고 넘어가면 데이터는 우리에게 그냥 이렇게 던져진 무언가 날것의 것입니다.

그래서 우리가 데이터를 의미있게 들어준 다음에 피처를 만들어주는데 피처 만들 때 이 피처 중에서도 의미가 있는, 우리에게 의미가 있고 없는 게 있을 거예요.

그래서 신호와 소음을 잘 구분해줘야겠죠.

그래서 데이터를 획득을 할 때 데이터를 머지하거나 컨캣하는 작업이 많이 들어가게 되는데요.

우리가 데이터를 얻었지만 원하는 형태의 데이터가 아닐 거예요.

그래서 전국 신규 아파트 분양 가격, 최근 3년 데이터라고 해서 다운을 받았는데 지역명과 규모 구분, 년도월, 그다음에 제곱미터당 분양가격이 나와 있어요.

그리고 그 전에 2013년부터 15년 9월까지의 데이터를 보면 완전히 컬럼이 달라요.

그리고 생긴 것도 다르고 그 전 데이터에는 규모 구분도 없어요.

이게 몇 제곱미터의 가격인지 그래서 전체 데이터만 있는 거죠.

그래서 땀내나는 전처리가 꼭 필요해요.

앞의 데이터를 보면 이렇게 위의 컬럼이 없는데 컬럼도 만들어줘야 하고 년도 다 떨어져 있고 지역도 비어 있는 값들이 굉장히 많아요.

그래서 이거를 의미 있게 사용하기 위해서 땀내나게 이제 파이썬 구문을 사용해서 전처리를 해줬죠.

그래서 이렇게 비슷하게 만들어줬어요.

비슷하게 만들어줬지만, 우리가 원래 최근에 받았던 데이터와는 다른 모양이에요.

아무리 전처리를 해줬더라도, 그래서 기존의 데이터들은 연과 월이 다 행에 가 있어요.

로우에 가 있기 때문에 행으로 다 이 데이터들을 옮겨줘야 해요.

그래서 R에서도 타이디데이터 만들기 이런 게 있더라고요.

파이썬에서도 멜트라는 거를 사용을 해서 타이디데이터를 만들어줬어요.

우리가 이 컬럼 데이터를 로우 데이터로 옮겨주면 전처리, 시각화하는 데 조금 더 땀내를 줄여줄 거예요.

그래서 데이터, 필요한 데이터를 모아서 데이터를 컨캣으로 병합을 했어요.

그래서 년도와 월, 평당 분양가격, 과거의 데이터는 제곱미터당 가격이 아니라 평당 가로 되어 있어요.

그리고 지역도 여기는 전국 이렇게 되어 있고 그다음에 전체 타입만 되어 있어요.

규모 구분도 없고 그래서 그렇게 양쪽 데이터를 다 똑같은 타입으로 맞춰준 다음에 기존에는 2013년부터 2018년 분양가까지밖에 볼 수 없었는데 2015년, 2018년 분양가를 나눠서 밖에 볼 수 없었는데 데이터의 타입을 맞춰주고 시개월 데이터를 다 모아서 볼 수 있게 됐어요.

참고로 며칠 전에 튜토리얼에서 했던 데이터를 가지고 왔어요.

그래서 일단 이렇게 데이터가 준비가 되어 있으면, 이 데이터가 과연 내가 어떻게 써야 하는지 이 데이터를 미리 봐야 할 텐데요.

이 데이터를 한꺼번에 열어보면 우리가 서버에서 데이터를 막 수집하고 있는데 열어보면 크기가 커서 잘 안 열리는데 판다스나 헤지도 우리가 서버에서, 터미널에서 헤드로 미리보기를 하듯이 이렇게 수집된 데이터를 보면 이거는 청와대 국민 청원 데이터예요.

며칠 전 했던 튜토리얼에서도 자연어 처리하는 튜토리얼을 했었는데요.

이 데이터를 보면 얘가 스타트와 앤드, 시작하는 날짜, 끝나는 날짜와 타이틀, 투표수, 앤서 함수도 있고 아티클 아이디는 저 아이디로 국민청원 데이터에 들어가면 국민청원이 보이겠죠.

그래서 일단 데이터를 1번 봐요.

그리고 이 데이터가 어떤 타입을 가지고 있는지 요약을 해보는 거예요.

그래서 얘가 정수인지 실수인지 그다음에 스트링 형태로 되어 있는지 그런 카테고리 형태인지 그런 요약된 정보를 일단 보고요.

그리고 걔를 EDA를 해봐요.

시각화를 해보는데 같은 데이터를 시각화를 한 거예요.

그런데 왼쪽, 오른쪽하고 완전히 다른 그래프가 보여지는데 왜 그럴까요.

제가 여기 위에도 써놨는데 수치형 데이터하고 카테코리형 데이터의 차이예요.

왼쪽 그래프는 얘가 수치로 되어 있기 때문에 연속형 데이터라고 생각한 거예요.

그래서 0부터 1까지 저 데이터 숫자를 표현해야겠다고 해서 그라데이션이 되어 있어요.

그런데 저는 0과 1을 구분해서 보고 싶어요.



저는 데이터 타입을 봤는데 숫자형으로 되어 있어서 이거를 오브젝트, 스트링 타입으로 변경을 해 주게 되면 이렇게 0과 1을 구분해서 보여져요.

얘는 실제로 숫자지만 수치형 데이터가 아니었어요.

카테코리 데이터인 거죠.

그래서 그리고 이 데이터 타입에 따라서 이렇게 디스크라이브로 요약 해서 보면 요약된 것도 달라요.

수치형 데이터는 민맥스, 카운터값, 표준편차, 평균 같은 통계적인 수치를 보여주는 반면에 왼쪽은 오브젝트 타입이에요.

이거 같은 경우는 텍스트 테이터 같은 것들이 들어가 있죠.

그래서 어떤 데이터의 형태냐에 따라서 우리가 데이터를 다르게 볼 수가 있는데, 시각화에서도 기계가 봤을 때도 그 데이터 타입에 따라서 만약에 머신러닝이나 딥러닝에 어떤 데이터 타입으로 그 데이터를 넣어주느냐에 따라서 얘가 그 데이터를 좀 더 잘 구분을 해줄 수가 있을 거예요.

그리고 데이터를 다루다 보면 미싱 데이터, 결측치를 굉장히 많이 발견하게 돼요.

그리고 이 결측치를 어떻게 처리해 주느냐에 따라서 우리는 좀 더 의미 있는 결과를 얻을 수도 있는데 결측치는 왜 생길까요?

일단 수집이 안 될 수도 있을 거예요.

예를 들어서 설문에 응답을 하지 않았어요.

나는 그 설문 문항에 응답하고 있지 않아서, 서버에 아니면 오류가 생겨서 로그가 누락이 되었어요.

그리고 특정 시기 이후에 로그를 수집하기 시작했어요.

예를 들어서 테이블에 컬럼 하나를 추가했단 말이에요.

그러면 그 전의 데이터는 아마 없을 거예요.

그리고 특정 상황에는 데이터가 필요 없어서 남기지 않았을 수 있어요.

그리고 휴먼 에러일 수 있어요.

그냥 사람이 잘못 남기거나, 그래서 결측치를 이렇게 통계적으로, 몇 개나 있는지 볼 수도 있지만, 이런 시각화 툴을 사용을 해서 전체 데이터 중 어느 지점에서 어느 정도 결측치가 있는지 시각화해서 볼 수 있어요.

미싱노라는 라이브러리가 있는데 이것을 하신 분이 포켓몬을 좋아하시는 분인 거 같아요.

시각화 도구를 통해서 보면 공원보유시설, 이런 곳에 결측치가 많이 있고 주소에도 결측치가 있는 거로 보여요.

그리고 위경도 정보가 있어요.

그러면 여기에 있는 도로명 주소, 지번 주소 같은 경우에는 서로 있거나 없거나 하는 경우가 있기 때문에 반대의 데이터로 서로의 결측치를 보완해줄 수 있을 거고 API를 사용해서 도로명은 지번, 지번은 도로명으로 그리고 그 데이터가 꼭 필요하다면 보정을 해줄 수 있을 거예요.

그래서 결측치를 보면 랜덤하게 있는 데이터가 있고 랜덤하지 않게 결측치가 있기도 해요.
그리고 의도한 결측치, 의도하지 않은 결측치가 있기도 합니다.

그래서 결측치의 종류를 이렇게 보통 세 가지로 나눠요. 

완전 무작위로 결측치가 발생이 있거나 임의적으로 발생했거나 무시할 수 없는 결측치거나.

그래서 어떤 결측치냐에 따라서 접근 방법이 다른데 완전 제거법은 가장 보편적으로 사용이 되고 드롭을 시킵니다. 

데이터가 필요가 없어서 채워졌을 때도 의미가 없기 때문에, 그런데 결측률이 높을 때 그게 필요한데 드롭 시키면 문제가 될 수 있겠고, 그다음에 변수의 분산을 증가하게 하는 비효율적인 방법이 될 수도 있을 거예요.

그리고 단일 대체 방법이 있어요.

예를 들어서 관측된 자료 내에서 나이 정보가 특정 몇 개가 빠졌다, 그런데 그 사람의 다른 정보로 나이를 추측을 해서 나이 정보를 넣어주는 거죠.

아니면 그렇게 하기 어렵다 하면 평균, 중앙, 최빈값을 구해서 넣어줍니다.

그런데 이렇게 되면 편향이 발생할 수 있어요.

예를 들어서 특정 설문 같은 데 내가 환자예요.

환자... 병원에서 치료를 받는데 개인적인 가족력 같은 거는 쓰기가 싫어서 안 썼어요.

그런데 그거를 그냥 그 사람이 그런 환자들은 보통 그런 정보를 안 채운다라고 하는데 그 정보를 평균이나 중앙, 최빈값을 채운다면 다른 건강한 사람들의 데이터로 채웠기 때문에 아마 아픈 사람의 정말 정보들을 제대로 반영해 주지 못해서 분석을 했을 때 원하는 결과를 얻지 못할 수 있을 거예요.

그리고 다중 대체 방법은 단일 대체 방법을 조금 더 보완을 해서 결측이 완전 무작위로 발생한다는 가정을 해서 대체가 가능한 값들의 분포로부터 추출된 값을 데이터 셋을 따로 만들어서 그 데이터셋별로 데이터를 채워주는 거죠.

단일 대체 방법 같은 경우에는 그냥 평균 값이나 이런 거를 그냥 한꺼번에 채워줬다면 이럴 때 평균값, 이럴 때 중앙, 이럴 때 최빈값을 채워주고 특정 데이터에 맞게 채워주는 게 다중 대체 방법이 될 거예요.

제거하거나 대체하기로 결측치를 채워줄 수 있을 텐데요.

판다스에서 코드로 드롭을 할 때 이런 식으로 데이터를 드롭을 시켜주는 거예요.

대체하기 어렵거나 결측 어려운 데이터.

결측치로 채워도 효과가 없으면 그냥 다 드롭을 시켜주는 거예요.

그리고 Imputation 대체해 주는 거예요. 

다른 데이터로 평균, 중앙 채워주거나 아니면 최빈값을 채워주거나 확률분포 활용하든지 다른 데이터 셋을 사용해서 채워줄 수 있어요.

내외부의 데이터를 활용한다면 연령대 정보가 없어서 그 데이터 안에 있는 다른 연령대를 가지고 그 연령을 추출할 수 있겠지만 그 정보를 가지고 이런 정보를 가진 사람들의 연령은 이 정도 된다라고 해서 외부에 있는 데이터를 활용을 해서 대체해줄 수도 있을 거예요.

그리고 무작위로 누락된 수치형 데이터 같은 경우에는 평균값으로 대체를 해줄 수 있겠지만 이런 평균값으로 대체를 해줄 수 있을 때는 아웃라이어 데이터 때문에 편향이 발생할 수 있어요.

내 연봉 데이터는 쓰고 싶지 않아요. 

민감해서, 평균값으로 채웠는데 보통 평균은 굉장히 높은 평균 때문에 내 연봉보다 높아질 수 있어요.

평균값을 채워줄 때 그렇게 의미 있는 게 아니겠죠.

판다스에서 민NA를 써서 민으로 대체하는 값을 넣어봤고요.

중앙값에 아까 연봉의 예를 들었는데 그렇게 양 끝단에 있는 너무 높거나 낮은 값 때문에 중간에 있는 평균을 제대로 못 구하는 거예요.

그럴 때 이상치가 있을 때 중앙값을 채워주는데 만약에 데이터가 짝수개가 있을 때 미디움 값을 채워주면 가운데 2개 평균을 구해서 넣어주면 없는 값이 생길 수 있겠죠.

정수값인데 소수점 들어가는 값이 생길 수 있을 거고요.

그리고 최빈값을 채워준다면 무작위로 누락된 범주형 데이터 같은 경우에는 평균, 중앙값으로 채워줄 수 없을 거예요.

그럴 때 가장 많이 등장하는 값으로 채워주는데 이때도 그냥 이렇게 최빈값을 채워준다기보다는 그 특정 다른 변수들을 고려를 해서 최빈값을 채워준다면 조금 더 도움이 되겠죠.

그래서 이런 값들을 채워줄 때 앞에서 있는 판다스에 있는 핀NA 채워줄 수만겠지만 사이킥 런에는 이거를 채워줄 수 있는 도구들이 많아요.

인퓨터를 사용하면 그 컬럼에 있는 결측치를 채워주게 되는데 기본값은 평균값으로, 그래서 중앙이나 최빈값 채우고 싶으면 옵션 설정하면 되고 특정 구간 나눠서 픽처의 특성을 반영해 주고 싶다고 하면 코드의 다른 분기 코드를 작성을 해서 채워줄 수 있겠죠.

그리고 다른 컬럼의 값을 사용, 아까 제가 예로 들었던 것 중 하나인데 도로명 주소가 없으면 지번 주소로 대체할 수 있을 거예요.

그러면 그 주소가 제대로 되지 않았을 경우에 다른 API를 통해서 지번, 도로명 주소를 받아올 수 있습니다.

그리고 기기값을 사용해서 예측값으로 대체해줄 수도 있을 거예요.

예를 들어서 특정 특성에 따라서 이 데이터일 때, 제가 며칠 전에 여기 파이콘 튜토리얼에서 국민청원에서 기타 카테코리로 진행을 했었는데요.

국민청원에서 기타로 카테코리 입력한 케이스가 많아서그 기타 카테코리를 특정 카테코리로 예측을 해서 넣어준다면 전체 학습을 했을 때 조금 더 나은 결과를 낳지 않을까 하고 예측값으로 대체하는 걸 해봤거든요.

그래서 이 기타인 것과 기타가 아닌 것으로 나눠서 얘를 학습시켜서 다시 다른...

(웃음)
기계학습을 시켜서 결과를 봤는데 결과는 별로 좋지 않았어요.

제가 이 국민청원 데이터뿐만 아니라 다른 캐글을 하면서도 머신러닝 기법으로 결측치를 채워서 예측을 해봤는데 생각보다 그렇게 결과가 좋아지지 않는데 아마도 제가 쓴 데이터들이 제가 예측, 대체를 해줄 때 대체는 해 주는 방법을 조금 더 개선했다면 조금 더 나은 결과를 낳았을 수도 있겠다라는 생각이 들어요.

그래서 이거는 그냥 제가 며칠 전에 했던 튜토리얼에서 이런 순서로 했었어요.

원래 1~10번 순서만 했는데 2번을 추가를 해서 기타가 특정 카테코리로 분류되어 있으면 조금 더 나은 결과를 낳을 수 있겠다고 생각을 해서 기타 카테코리를 기계 학습으로 넣어주었던 튜토리얼을 진행을 했었습니다.

그런데 저는 그래서 이런 기법들을 활용한다고 다른 통계학을 하시는 분들에게 얘기를 했어요.

그랬더니 그분은 R를 쓰시는 분인데 R에는 이미 미스포레스트가 있는데 그거를 사용하면 결측치를 알아서 기계 학습으로 대체해준다고 해요.

아니, 그러면 파이썬에도 있지 않을까 해서 봤더니 파이썬에도 프레딕트 인퓨터라는 게 있더라고요.

얘도 R에 있는 미스포레스트를 기반으로 개발되었다고 해요.

그래서 이제 본격적으로 피처엔지니어링을 얘기를 해볼까 해요.

데이터 전처리까지 얘기를 했고 피처 엔지니어링을 얘기해보도록 하겠습니다.

위키피디아에 있는 피처 엔지니어링의 프로세스를 가져왔어요.

그래서 어떻게 브레인스토밍하고 테스트해보는 거죠. 

피처를 어떻게 할 것인지 피처를 어떤 거를 쓸 것인지 결정을 하고 그래서 모델을 만들고 계속 얘를 보완해나가는 그런 프로세스를 가지고 있는데요.

그 피처의 특성이 어떤 거냐에 따라서 다루는 방법들이 다 달라질 거예요.

그래서 첫 번째 범주형 데이터 다루기를 얘기해보려고 하는데요.

범주형 데이터라고 하면 우리가 흔히 얘기할 때 카테코리라고 얘기하기도 하고 그런 데이터들을 보통 범주형 데이터라고 얘기를 하는데요.

수치형 데이터를 범주형 데이터로 바꿔줄 때 비닝, 버케팅이라는 말을 쓰고요.

그다음에 범주형 데이터를 수치형 데이터로 다시 쓸 때 원핫 인코딩, 원핫 백터가 무엇인지 얘기를 해보면 언어, 파이썬, 루비, 자바, 파이썬이 엑셀 파일에 있다고 해요.

설문조사를 했어요. 

당신은 무슨 언어를 쓰십니까?

했어요.

그런데 얘를 나는 파이썬, 루비, 자바, 이런 식으로서 써주거나 0, 1, 2, 3, 4된 카테코리화된 값을 써줄 수 있겠지만 그렇게 해 주면 머신러닝에서 정확한 값을 낼 수가 없어서 벡터를 만들어주는데 파이썬이 들어가는 피처에 파이썬, 루비, 자바라는 피처를 만들어주고요.

각각의 변수에 만약에 파이썬이 들어간다고 하면 파이썬만 1, 나머지는 0므로 채워주는 거죠.

그래서 파이썬, 루비, 자바 이렇게 해서 봤을 때 각각의 피처에서 해당되는 로우의 해당되는 피처의 1이 표기가 되는 거 그게 바로 원 핫 벡터, 원 핫 인코딩이라고 얘기를 하고요.

그리고 범주형 데이터를 수치형 데이터로 바꿔줄 때 파이썬이나 판다스 같은 데서 일일이 코딩을 해서 원 핫 벡터를 만들어줄 수 있겠지만 사이킷에서 제공하고 있는 것을 사용을 해서 파이썬, 루비, 자바 설정을 해두고 랭이라는 것이었으면 얘가 알아서 랭 파이썬, 랭 루비, 랭 자바 이렇게 해서 원 핫 벡터를 생성을 해 주면 일일이 판다스, 파이썬 코드로 표준 라이브러리를 써서 짜는 것보다 약간 땀내를 줄여줄 수 있겠죠.

그래서 수치형 데이터 다룰 때 연속형 데이터냐 아니면 그 데이터가 비연속형 데이터냐에 따라서 처리하는 방법이 달라져요.

그래서 수치형 데이터를 카테코리 형태의 데이터를 바꿔주는 경우가 발생하게 되는데요.

이때 어떤 기준으로 얘네들을 이렇게 빈 단위로 묶어줄지 그거를 결정을 해야 하는데 그때 빈 단위를 결정할 수 있는 뭔가 논리적 타당성이 필요하겠죠.

그래서 얘를 잘못 묶거나 너무 크고 작은 단위로 묶었을 때 제대로 된 예측 데이터를 얻기 힘들 거예요.

비닝이 뭔지 이해가 안 될 텐데 이렇게 시각화를 해서 보면 0세부터 80세까지 있는데 나이 데이터입니다.

나이 그런데 분포가 굉장히 단위를 1로 줬거든요.

1세 단위로 촘촘하게 되어 있는데 뭉툭하게 만드는 거예요.

이 사람이 0세부터 10세까지 10, 20, 30, 40 이런 식으로 연령대를 구분하기도 하는데 그렇게 연령대를 묶어주는 거를 비닝 버켓팅이라고 하는데 이거는 가족이 몇 명에 따라서 수치형 데이터를 카테코리형 데이터로 묶어주는 예시인데요.

이 사람이 혼자 사는 사람인지 아니면 작은 규모, 큰 규모의 가족인지 이렇게 해가지고 비닝해서 묶어준 다음에 비닝된 데이터 같은 경우에도 다시 원 핫 인코딩을 해서 수치형 데이터로 바꿔주는 거예요.

그래야 머신러닝, 딥러닝 알고리즘에서 사용할 수 있기 때문에


그리고 텍스트 데이터 다루기.

이거는 약간 원 핫 인코딩과도 연관이 되는데요.


그리고 백 오브 워드는 텍스트 데이터를 단어 하나하나 담아주는 거예요.


그래서 그 단어, 특정 단어가 전체 문서에 몇 번이 들어가는지 단어 갯수를 세어주기 때문에 문장의 순서를 무시를 해버려요.

그래서 이 2개, 이스 배드 낫 굿 엣 올과 이스 굿 낫 배드 엣 올 완전히 다른 뜻을 가지고 있게 되는데 BOW로 묶으면 얘의 의미를 잃어버리게 됩니다.

그래서 그런 단점이 있고 백 오브 워드 같은 경우에 파이콘에 버스를 타고 갔다, 발표를 했다, 자원봉사를 했다 이런 문장이 있으면 유니크한 단어만 모아서 토큰화를 하고 토큰화된 순서 대로 데이터화를 해 주는 거예요.

1번에 여러 문장이 있을 거 아니에요.

파이콘에 버스를 타고 갔다가 1번 문장인데 거기에서 얘만 원 핫 벡터, 1로 처리를 해 주는 거예요.

그런데 얘 같은 경우에는 단어의 의미를 잃어버린다는 단점이 있기 때문에 n-그램을 써주는데 단어를 2, 3개 묶어주는 거예요.

파이콘에 버스를 버스를 타고 타고 갔다.

이렇게 2, 3개까지 묶어주는 거예요.

2개에서 7개까지 묶겠다고 숫자를 지정하게 되면 그 숫자만큼 지정을 해 주는 거예요.

그게 바로 엔그램이고요.

백 오버 워드보다는 의미를 조금 더 보완해줄 거고요.

그다음에 픽처 익스트렉서므로 가는데

국민청원데이터인데 특수 문자를 제거하고 텍스트 데이터만 모았어요.

그래서 백 오버 워드로 만들어주게 되면 여기에 내가 몇 개의 단어를 사용할 건지, 벡터, 단어의 가방의 사이즈를 정해줘요.

그러면 그 피처의 크기에 따라서 얘가 가방을 만들어주고 저런 단어를 넣어주는 거예요.

그리고 그 단어가 이 전체 데이터에서 몇 개씩 등장하는지, 예를 들어서 여기에 회사는 회사를이라는 단어 같은 경우에는 255번, 244번이 전체 분석에 쓰였고, 그리고 이렇게 0번 인덱스부터 첫 번째 청원부터 그다음 청원인데 0으로 표시되어 있는데 가끔 1이나 7로 표시 되어 있는 항목은 그 문서에 저 단어가 그 갯수만큼 등장한다는 거예요.

아까 얘기했듯이 BOW 같은 경우는 단어 의미를 보존해주지 않는다는 단점이 있기 때문에 많이 등장하는 단어가 그냥 중요한 단어다 이렇게 하게 되면 불용어 같은 경우에는 사실 굉장히 많이 등장하게 되거든요.

나는 너는 했다 그리고 이런 만어 굉장히 많이 등장하게 되는데 걔네가 많이 등장한다고 해서 무언가 의미를 갖는 게 아니기 때문에 특정 국민청원에 특정 단어가 굉장히 많이 들어갈 거예요.

예를 들어서 초등돌봄교실이라는 단어는 초등돌봄교실 관련 청원에 굉장히 많이 들어갈 텐데 주식과 관련, 비트코인 관련 국민청원은 들어가지 않겠죠.

청원에서 초등돌봄교실이라는 단어는 중요한 의미를 갖기 때문에 얘한테 가중치를 주는 게 바로 TF-IDF입니다.

그래서 BOW에 TF-IDF를 적용해 주면 아까는 그 갯수가 몇 개 등장하는지 표시가 되었다면 얘 같은 경우는 가중치가 어떻게 되는지 알 수 있을 거예요.

그리고 에러 데이터 다루기를 볼 텐데요.

보통 아웃 라이어 데이터와 에러 데이터는 다르게 구분을 하는데 에러 데이터는 잘못 입력된 데이터인데 이거는 전국 공원 표준 데이터에서 시각화를 해본 건데요.

보면 우리나라 지도예요.

그런데 여기 이상한 점들이 찍혀 있어요.

그런데 저 지도의 값들을 실제로 찍어보면 주소가 다른데 이런 태평양 바다가 아니라 우리나라에 있는 데이터인데 위경도 정보가 잘못 되어 있어서 데이터 정보가 잘못 표기 되어 있어요.

이 데이터 표시는 되었지만 위경도가 다른 게 많아요.

그런 데이터 같은 경우에는 주소 정보가 있기 때문에 API를 활용해서 위경도 정보를 보정해줄 수 있을 거예요.

그리고 급여 정보, 보험 정보, 의료보험 정보, 국민연금 정보로 그 사람의 급여를 예측하기도 하죠.

아웃라이어 데이터 다루기를 볼 텐데요.

에러, 오류 데이터와 다르게 아웃라이어 데이터는 이상치를 봅니다.

벗어나 있는 데이터죠.

청와대 국민청원 데이터를 튜토리얼을 해서 그거를 예시로 들 텐데 국민청원은 투표수가 낮다가 갑자기 투표수가 뛸 수가 있어요. 

차이가 커요. 표준 편차가

그런 이상치는 모델에 편향을 줄 수 있어요. 

정상치와 이상치의 기준을 어느 정도로 할 건지 결정해야 하는데 결정할 때 각 데이터의 특성을 고려해야 할 거예요.

도메인에 따라 이상치의 정보가 달라서 이 정도를 어떻게 주느냐에 따라서 어떤 성능이 좋은 모델을 개발하는 데 영향을 줄 수 있겠죠.

그래서 제가 이거를 시각화를 해봤어요.

이상치에 대해서

이상치, 데이터를 그냥 이렇게 모아져 있는 데이터가 있고 저기 보면 이 모아져 있는 데이터에 저 끝에 점 2개를 찍었어요.

그래서 원래 이 모아져 있는 데이터는 이런 회기 얘를 선형회귀를 그려보면 이렇게 밑으로 회귀를 하고 있어요.

그런데 저 위에 있는 이상치 2개 때문에 완전히 회귀선이 달라지게 되죠.

그래서 예를 들어서 보면 기사에 나오는 평균 연봉 굉장히 높은데 내 연봉을 보면 실제로 그렇게 높지 않잖아요. 

저런 아웃 라이어 데이터 때문에 그래서 평균 연봉이 저 아웃라이어 데이터를 반영한 거라고 보기 어렵겠죠.

이상치를 다룰 때 보통 보정을 해 주거나 제거를 주거나 다른 값으로 대체를 하거나 스케일을 제거를 해 주는 방법을 사용해요.

그래서 피처를 셀렉션을 할 텐데요. 모델에 넣기 위해서

적절한 피처를 선택하는 것만으로도 좋은 성능을 낼 수가 있어요.

피처 중에 저희가 결측치를 봤었는데 결측치가 굉장히 심한 것들, 보정해준다고 해서 보정이 안 되는 데이터들이 있어요.

그런 의미가 없는 데이터들은 다 빼주는 거예요.

그런 애들을 다 선택을 해서 골라주는 것만으로도 모델이 굉장히 좋은 성능을 낼 수가 있얶
그래서 판다스를 통해서 EDA를 하고 나는 이런 피처링이 모델링에 도움이 될 거 같고 해서 피처를 선택해서 모델에 넣어줄 수 있겠죠.

그래서 사이킥런에 있는 피처 셀렉션을 사용을 해서 특정 피처들을 선택을 하시도록 할 수 있을 거예요.

그리고 피처 트랜스폼, 피처를 선택을 했어요.

그런데 보통 머신러닝, 딥러닝에서 정규 분포, 각각의 변수가 정규분포를 이루어야지 그 모델이 가장 좋은 성능을 낸다고 해요.

그래서 왼쪽 데이터, 위의 데이터 같은 경우에는 낮은 수치의 데이터 값들이 굉장히 많이 알려져 있어요.

실제로 국민 청원 데이터도 그렇거든요.

국민청원 데이터도 투표수가 낮은 게 대부분이고 높은 게 몇 개 되지 않아요.

그런데 저런 데이터들 때문에 모델이 제대로 된 성능을 내기가 어려운 거예요.

그래서 모델이 제대로 된 성능을 내라고 정규분포처럼 만들어주는 거예요.

그래서 로그를 씌워서 이렇게 정규분포 비슷하게 만들어주고 다시 만약에 여기 레이블 데이터에 이거를 씌웠다 할 때는 예측을 하고 난 다음에 다시 지수함수를 씌워서 복원을 해 주겠죠.

그리고 피처 스케일링, 어느 한 변수가 분산이 너무 클 때 각 변수의 분포를 일정하게 유지하도록 해 주는 거예요.

그래서 회기분석 같은 경우에는 이 스케일링을 해 주든 말든 변수의 유의성은 달라지지 않아요.

하지만 모델이 좋은 성능을 내기 때문에 이런 피처 스케일링을 해 주게 되는데요.

이런 민맥스 스케일링 같은 경우에는 어느 한 변수의 분산이 클 때 분산을 0하고 1 사이로 만들어주는 거예요.

제가 아까 에이지 그래프로 비닝하는 예시를 들었는데 비닝했던 데이터에 민맥스 스케일링을 해줬어요.

그러면 0세부터 80세까지가 0부터 1까지...

분산이 1이 되도록 조정을 해 주었죠.

그리고 차원의 저주라는 게 있어요.

그래서 피처를 만들다 보면 디멘션이 점점점점 넓어지게 되는데 그렇게 디멘션이 차원이 많아지다 보면 특정 디멘션에서는 그런 이 디멘션에서 갖는 백터가 다 달라지거든요.

그래서 그 벡터를 잘 설명할 수 있는 잠재 공간을 찾는 게 굉장히 중요한데 그래서 벡터가 디멘션이 많아질수록 성능이 떨어지는 현상이 발생을 해요.

특정 디멘션까지는 성능이 좋아지다가 너무 넓어지면 오버피팅이 일어나서 성능이 떨어지게 됩니다.

그래서 이럴 때 데이터를 압축해 주거나 시각화를 해 주기 위해서 차원 축소라는 거를 사용을 해요.

PCA를 통해서 피처를 스케일링을 해 주는 거예요.

여기 보면 레이셔로 PCA를 해 주고 싱글벨류로 이렇게 피처 스케일링을 해준 예제예요.

그리고 T-SNE로 워드 특정 단어의 백터입니다.

단어 하나에 대한 벡터거든요.

그런데 얘를 X, Y축에 표시를 하고 싶어요.

그래서 워드 투 백은 비슷한 단어끼리 모여있다고 하잖아요.

XY축으로 표현을 해 주고 싶은데 이 벡터로는 해줄 수 없어요.

이럴 때 T-SNE를 사용을 해서 벡터값을 2차원으로 축소를 해서 국민청원 데이터 일부만 해서 얘를 T-SNE로 시각화를 해준 거예요.

전체 데이터를 뿌려주면 글씨가 너무 많아서보이지 않아서 제가 위에서 몇 개만 뽑아보게 해서 의미 있는 데이터들끼리 비슷하게 배치되어 있다고 보기는 어렵지만 여기 업체는, 업체 이런 단어들이 근처에 모여 있는 걸로 보여져요.

그다음에 피처 파이프라인이라는 게 있어요.

피처를 만들 때 프리프로세싱을 하거나 피처 추출, 스케일링, 셀렉션하는 여러 가지 방법이 있는데 얘를 한 번에 모아서 써줄 수 있어요.

파이프라인에 모아서 써주면 저 과정을 한 번에 처리를 해줘요.

이거 같은 경우는 단어 벡터화, TF-IDF를 함께하라고 해 주는 거예요.

멀티 프로세싱인데 데이터 전처리 하면 장비 속도가 굉장히 오래 걸려요.

내 장비가 자원을 충분히 활용하고 있는지 확인해봐야 하잖아요.

그래서 멀티 프로세싱으로 스레드를 여러 개 만들어서 얘가 일을 충분히 할 수 있도록 시켜야 할 거고요.

그리고 내 장비가 제대로 일하고 있는지 확인해봐야 해요.

스레드 4개 만들어서 파이썬 4개 떠 있는데 80% 이상으로 자원을 잘 쓰고 있어요.

그리고 머신러닝 분류기, 회기하는 거를 사용하다 보면 엔스레드, 엔잡스가 있는데요.

내 CPU를 몇 개를 쓸 건지 지정해줄 수 있어요.

그때 -1로 쓰게 되면 내 CPU가 이 코드를 깃탑에 올려서 다른 사람들이 다운 받아서 실행을 했는데 그 사람의 CPU 코어 갯수랑 내 거랑 달라도 다 쓰게 해 주는 거예요.

저는 코어가 4개라 355기를 쓰고 있어요. 

이 분류기 돌렸을 때 데이터 분석을 할 때 오버피팅, 언더피팅 얘기가 꼭 나오는데 최접점을 찾는 게 중요하고 과소적합, 과대적합을 피해서 최적점을 구할 수 있도록 이런 데이터 전처리라든지 피처 엔지니어링을 해주면 조금 더 도움이 될 거예요.

이것도 시간이...

제가 다 되었는데요.

학습을 할 때 여기 트레이닝 데이터와 밸리데이션 데이터를 확인해서 학습을 잘 하고 있는지 시각화해본 건데요.

학습이 잘 되는 최적점을 찾아서 예측을 해서 의미 있는 결과를 얻는 게 중요하겠죠.

제가 굉장히 좋아하는 논문인데 공짜 점심은 없다는 논문이에요.

머신러닝, 딥러닝을 사용을 할 때 최적화된 기법은 없다라는 건데 그래서 제가 오늘 여러 가지 전처리 기법이나 피처 엔지니어링 기법 설명을 했지만 각각의 데이터, 도메인 데이터 특성에 따라서 처리해야 하는 방법이 달라질 거예요.

그래서 조금이라도 땀내를 줄이는 데 도움이 되었으면 하는 바람이 있습니다.

여기까지 제 발표를 마치겠고요.

제가 발표에 사용한 코드는 이런 코드들이 있고요.

제가 이거는 파이콘 튜토리얼, 금요일에 진행했던 위의 2개는 튜토리얼에서 진행했던 코드입니다.

여기까지 들어주셔서 감사합니다.

-함께: (박수)
-발표자님 감사합니다.
이번 세션 마치도록 하겠습니다.

