https://www.youtube.com/watch?v=IDhaMrR6R8k

우선 많이 와주셔서 되게 감사드리고요.

많이 오신만큼 제가 많은 걸 나눠드릴 수 있도록 에너지를 쏟아보겠습니다.

한국말로 해드리면 딥러닝을 이용한 로그 기반의 게임 AI 개발이고요.

여기에는 키워드가 몇 개 있는데 로그 등.

로그 같은 경우는 다 아시는 거고.

딥러닝 같은 경우도 있고요.

사전에 궁금한 게 있었으면 카페나 인터넷 이용하신 분은 손 한번 들어보세요.

게임업계 종사하시는.

-소리가 잘 안 들립니다.

-가까이 대고 말을 해야 되겠네요. 죄송합니다.

-좀 올려주실 수 있나요?

그리고 게임 다 좋아하실 것 같아요.

반갑습니다.
정식으로 인사드리겠습니다.

저는 김선태라고 합니다.

자연스럽게 게임과 인연이 접할 수 있었던 것 같습니다.

맥주를 정말 좋아해서 공부해서 만들기도 하고요.

그다음에 영국 밴드 오아시스 되게 좋아하고요. 언더월드.

그렇습니다.

제가 소개해드릴 것들은 게임 AI인데 이건 알고 있지 모르겠지만, 그 프로젝트를 하게 된 백 그라운드와 AI 요구사항과 그 과정, 그다음에 마지막으로 테스트 결과를 보여드리도록 하겠습니다.

일단 모바일게임에서, AI는 켜놓고 그냥 가만히 있으면 플레이하잖아요.

노가다하고 피로도를 많이 줄여주기 위해서 사용하는데 저도 초창기부터 게임을 하면서 런칭을 해왔지만 AI는 어떤 정해진 유한한 상태에서 동작을 하다 보니까 모든 경우의 수를 다 컨트롤할 수가 없어요.

그래야 자연스러운데, 그래서 AI를 만들어도 좀 딱딱하고 로봇처럼 기계적일 수 있고요.

오히려 게임 플레이어 분들은 기계적인 어떤 함정을 공략해서 플레이하시는 분도 되게 많이 계시죠.

개발자 입장에서는 기획분들께서 자연스럽게 해야, 딱딱해 보인다, 이렇게 스테이트를 해야 하는데 그러면 자연스럽게 어떻게 스테이트를 하냐.

경우의 수가 많게 되는 거죠.

실제 경험인데 제가 게임을 개발하고 있었는데요.

갑자기 기획자분께서 오시더니 사람 같은 AI를 만들어주세요 하시는 거예요.

게임개발자분이시라면 한 번씩 아마 들어보셨을 것 같아요. 그렇죠?

이런 말 들으면 되게 막막해요.

그런데 대답을 한 게 사람 같은 게 느낌이 달라요.

그리고 정의하기가 힘들고, 그래서 진행한 게 A 로그 베이스 휴먼 라이크 게임이라고, 거창한 부제지만 사실 어떤 거였냐 하면 방어자의, 제가 하려던 포인트는 그 많은 AI 중에서도 방어자가 했던 행동, 패턴들을 학습해서 그 해당 유저가 내가 게임할 수 없을 때 오프라인 때 나를 대신해서 내 마을을 방어해주는 그런 디펜스 AI를 만드는 게 목적이었습니다.

그래서 어떤 게임에 적용했냐 하면 여기 라인 나이트라는 멋진 게임이 있습니다.

리틀나이트라는 게임이고요.

여기서 보시는 게, 포인트를 제가 설명드리면요.

제가 하려 한 건 이 빨간색 지형에 있는 캐릭터들이 지금 나왔죠.

어떤 캐릭터가 나오고 언제 나왔는지 이런 것들을 AI를 했습니다.

공략을 하면 대행하잖아요, 이 캐릭터들이.

이 캐릭터가 어느 캐릭터가 나가야 하고 어느 시점에 나가고 어디에 배치하는 게 적절한지.

그리고 다 딜러인데 갑자기 근딜을 뽑으면 안 되겠죠, 그런 것들.

그래서 방금 말씀드린 것처럼 카드 예측이 필요한 거고요. 

그다음에 캐릭터를 어디에 배치할 것인공간으로 보면 42X35 사각형 영역이 되고요.

그리고 어느 적절할 때 배치하냐, 먼저 배치해서 혼란주면 안 되잖아요.

그래서 기가막힌 타이밍을 예측해서 배치를 해줘야 한다는 게 기획팀의 요구사항이었죠.

이 세 가지를 다 한 거고요. 

이 과정이 되게 우여곡절이 많았지만 여러 가지 방법을 트라이 하면서 겪었던 노하우나 인프런스하는 쪽, 게임은 계속 인퍼런스해야 하잖아요.

그런 것들. 공유해드리도록 하겠습니다.

학습을 시키려면 데이터가 필요하잖아요.

강화학습으로 하는 것들을 많이 봤어요.

그런데 저 같은 경우는 스스로 학습하는 게 아니라 사용자가 했던 플레이 패턴학습해서 나와 비슷한, 나와 비슷한 플레이어와 해야 하기 때문에.

로그를 게임 개발팀에 있는 개발자분하고 협의를 했습니다. 

협의를 두 가지 했고.

그래서 보시면 언제, 누가 언제, 어디서 무엇을 했는가에 대해서 스냅샷 정보가 다 들어가게 돼요.

그래서 스크린샷, 임의처럼 보일 수 있지만 로그로 저장된 걸 보면 캐릭터의 위치, 캐릭터의 어떤 에너지, 캐릭터의 타임, 캐릭터가 어디에 있는지 이런 정보를 계속 다 저장이 되는 겁니다.

아마 대부분 로그들이 이렇게 들어갈 겁니다.

계속 스냅을 찍지 않는 이상 다 볼 수 없어요.

어떤, 예를 들어 데이터가 바뀌었을 때 그때 한 번 말고 대부분 적용할 거예요.

로그는 로그고 이 로그에 대해서 필요한 걸 가지고 와야 하잖아요.

데이터셋을 보시면 1, 4, 4, 6.

X가 1, 4, 46 여기서 시작하니까요.

잘 안 들리나요, 0번이 유닛 코스트가 있어요. 

코스트가 있어야만 카드를 선택할 수 있겠죠. 

코스트 정보가 들어가고 인덱스 1부터 45까지는 유닛의 정보가, 내가 게임을 시작해서 딱 들어가면 카드는 되게 무궁무진하지만 내가 선택한 카드만 들고 갈 수 있잖아요.

그중에 내가 카드 4, 5개 선택해서 들어가잖아요.

덱 정보가 있는 거고요. 

밑에 이런 게 실제로 배틀이 진행되면서 아군, 적군, 빌딩 이런 모든 정보들이 배틀이 진행되는 그런, 나와 상관 없는 게임이 진행되고 있는 데이터가 되고요.

마지막에는 1446번째 있는 거, 1447에.

이건 만약에 어떤 상황에서 유저가 카드가 한 1, 2, 3, 4가 있는데 그중에 1번을 뽑았다, 그 상황에서. 

그러면 그 카드 1번에 대한 내용이 저장되죠.

그 다음에 유닛 덱 정보는 어느 유닛이든 코드가 들어가고요. 

그 다음에 코드를, 그 유닛이 레벨이 1, 2이든 다르잖아요. 

레벨정보가 들어가고 그 카드가 한 번 쓰면 클래시로얄처럼 이렇게 돌잖아요.

나머지는 배틀 상황들을 다 적게 됩니다. 

아군, 적군 포함해서 레벨, 위치, 디펜스 어떻게 되는지, AI인지, 참가자인지.

전형적인 슈퍼바이스로 하는 거죠.

전처리를 이제 해야 하는데 용량이 좀 됐어요.

320기가 정도 되고요. 그다음에 실제로 배틀한 판 수를 찍어보니까 45만 판 정도 됐고요.

전처리 해야 하니까 샘플링통해서 전처리 코드 짜고 데이터 남겨서 했고요.

랭귀지는 전체적으로 스칼라를 썼고요. 

시스템은 이렇게.

이것도 비하인드스토리가 있습니다. 

전처리를 막 시작했어요.

그런데 짜투로 해 본 거예요. 데이터가 잘 들어왔나.

그래서 헬스정보 이런 게 다 들어가 있죠, 여기에는 실제로 데이터를 학습시키기 위해서 필요하지 않은 것들도 있죠.

여기서 한 번 더 필터링을 거쳐서 학습 가능한 형태의 실수형 데이터로 만들어줘야 합니다.

그래서 제플린에서 파싱 코드를 작성을 했고요. 

그다음에 이걸 돌려서 이렇게 학습 가능한 형태.

보면 모르죠.

그런데 계속 하다 보면 이게 뭔지 알아요, 숫자 하나하나가.

이게 코드가.

이게 레벨이고요, 다 보여요.

여기 들어갔구나, 여기 들어갔으니까 데이터가 없구나.

이게 머리에 들어있잖아요. 

여기는 게임 많이 진행 안 됐구나, 게임이 어떻게.

이게 다 눈에 보여요.

그런데 막상 돌려보니까 이 데이터가 320기가 되고 이걸 전처리하면 49메가 바이트 정도 나오는데 7시간이 걸렸어요.

그런데 이걸 하려면 퇴근 전에 항상 돌려놓고 다들 출근하자마자 확인하고 뻑 났는지 안 났는지 확인하고 안 나면 휴 하고 다시 학습을, 이걸 이렇게 개발하면 안 되겠구나, 시간이 많이 오래 걸려서.

그래서 코드쪽으로 더 이상 할 게 없다고 생각해서 장비를 늘렸습니다.

코어가 4개였는데 160개로.

사실 이게 저희가 회사 장비라 빵빵하게 썼거든요.

한 PC에 코어 40개 달려있는 거 그거 4개 쓰고 .

이렇게 160개인데 이게 나머지 156개 코어는 어느 다른 작업장에 쓰고 있던 거였어요.

 그래서 정리하고 나니까 다시 160코어 있었고, 이건 늘렸습니다. 메모리는 256이었는데 512로.

이 정도면 한 번 돌려놓고 되면 그냥 맞아서 하면 될 정도 되구나, 이제부터는 본격적으로 시작한 거죠.

뉴럴 네트워크 설명 드릴 텐데요. 크게 볼 것이 왓, 카드 예측.

위치를 이제 예측해야 하니까 리그레션도 했고 그다음에 바이너리 클래시피케이션, 이건 예를 할 거냐 말 거냐, 더 세부적으로 들어가면 마지막에 카드를 어떤 카드로 할 것인가에 대해서 했었고요.

그다음에 위치를 좁혀주는 거예요. 

내가 예측한 게 이게 어디냐 하면 실제 데이터는 여기였어요.

-
그리고 파일 배치 같은 경우는 소프트맥스를 써서 이거를 기다릴 거냐 아니면 작업할 거냐에 대해서 처리했습니다.

첫 번째로 왓에 대한 거고요. 

왓에 대해서 보시면 이걸 좀 다른 학습하고 데이터 측면에서 다른 게 이미지 데이터가 아니고요.

그렇기 때문에 흔히 중간에 이미지 데이터가 아니라는 거죠.

채널을 32개로 쭉쭉 늘리기 시작해서, 그다음에 필터 사이즈랑 똑같이 갖고요.

그다음에 다시 한번 늘려주고, 362로 그다음에 마지막에 똑같이 해서 118개로 갖는 거로.

그렇게 해서 쫙 펼친 다음에요. 

512로 줄이고 1284로 줄이고 마지막에 10개로 줄이는데 카드가 배치할 수 있는 게 카드가 10개인 거예요.

레벨 10일 때는 카드의 개수가 10개예요. 

그 레벨일 때는 소지할 수 있는 게.

그런데 여기서 왜 백 마스킹을 하냐, 말씀드린 것처럼 카드는 10개지만 게임에서 실제 들고 할 수 있는 카드의 개수는 한정적이잖아요.

그러면 만약에 4개를 들고 가면 그4개에 대한 인덱스가 있을 거예요.

소프트맥스에서 포텐셔를 구하죠.

그런데 예를 들어서 0번째 값이 0.5고 나머지는 이렇게 돼 있어요.

다 합쳐야 해요. 

다 합치게 되면 1이 돼야 하거든요. 

소프트맥스는.

Y값에 대한 인코딩을 내가 썼으니까, 그러니까 만약에 0번째나 얘가 아그맥스가 0이냐, 이때는 위치 예측은 데이터와 똑같아요.

상황과 똑같은 거죠 그 상황에 어떤 카드를 내고 어디에 배치하냐의 액션이 다르지, 그 상황은 똑같은 거거든요.

마지막에 두 개는 뭐냐 하면 X, Y.

그 차이가 로스 값이 0이 되는 걸 찾는 거잖아요.

그래서 저희가 생각했던 것은 예측한 게 어느 위치가 있을 거예요.

실제 Y값이 0이었다면 계속 단계를 좁혀가는 거예요. 

그래서 되게 비슷할 때까지 예측이 갔다, 그러면 얘는 카드 예측이 아니라 위치 예측이기 때문에 어느 정도의 바운더리에 가면 그럴싸하게 위치가 예측된 거예요. 

정확하게 한 게 아니라, 그 로케이션, 그 바운더리의 오차가 몇인가가 중요하죠, 이 문제를 풀기 위해서여도 그래서 이것도 다 돌려놓고 보면 예를 들어 밸류가 10으로 돼 있으면 이걸 형상화시키면 40 35라고 하는 거죠.

이게 좀 잘못돼 있네요.

여기에 실제로 캐릭터가 배치는 더거예요.

타이밍 예측입니다.

타이밍 예측 같은 경우는 시간 있잖아요.

그러니까 지금까지 과거 데이터는 필요가 없었어요.

보면 과거의 데이터가 있어야만 됐어요. 왜 그러냐 하면 어떤 순간을 예측한다는 것은 내가 과거에 어떤 행동들이 다 기록돼서 내가 1초전에 한 게 2초로 영향이 돼서 바뀌고 바뀌어서 내가 뭘 하잖아요.

과거의 정보는 필요없어요. 

게임에 내가 지면 내가 엄청 다굴당하고 있는데, 그런 과정에서.

그런데 아까처럼 스냅샷 같은 경우는 그런 과정이 다 없고 순간만 딱 보고 예측을 하는 거예요.

정확하지 않을 수 있죠, 타이밍으로 보면.

그래서 많이 사용되는 이미지가 이건데요.

예를 들어서 현재가 1이면 1 전에 한 게 1, 3가 있는데.

로그 카운트를 10개 더오 려서 총 300개의 셀을 한 거예요.

그리고 마지막 스테이트의 값을 만들면, 그다음에 거기는 똑같아요. 얘는 소프트맥스에서 웨이트냐, 액션이냐.

그래서 0이면 웨이트가 되고 1이면 액션을 하게 되는 거죠, 예를 들어서 아배맥스가 여기서 1이면, 1.

그러니까 1이면 여기서 액션으로 하게 되는 거죠, 이 타이밍에.

지금 같은 경우는 클래시피캐이션으로 타이밍을 예측하는 거고 이걸 잘 확인되는 건지 확인하기 위한 건데요.

웨이트, 사실 웨이트가 데이터가 더 많아요. 정확히 말씀드리면 9:1 정도, 플레이 하면 매번 액션하지 않습니다. 

기다리다가 자기가 유리한 타이밍이나 전략적으로 하고, 비율을 따지니까 90%가 넘게 웨이팅을 하고 있다가 한 번 어쩌다 하는 거예요.

처음에는 다 0이에요, 그러니까 다 웨이팅을 하다가 이제 진행이 되면서 위쪽으로 올라가죠, 액션이.

진행 과정을 하기 위해서 해본 건데요.

코드에 대해서 관심이 많으실 거 같아서, 좀 바뀌긴 했지만 거의 비슷해요. 이걸 반영해서 쓰셔도 되고 비슷한 결과가 나올 거라고 생각하고요.

비율을 보니까 30% 정도 아까 한 거 같았어요. 그래서 자세히 설명을 드릴게요. 간단한 코드인데 아무것도 모른다고 생각하고 말씀드릴게요.

***


이걸 이제 어떻게 하냐에 대한 것은 다음 장에 있습니다.

복잡한 게 한번 실행을 해볼게요, 시뮬레이션 하면.

실행을 해라, 그러면 이거 하나 물어요. 

뭘 실행하냐, 얘를 실행해라 하는 거예요. 

그러면 올라가니까 뭐뭐 해서 미니마이즈 하라, 뭘 미니마이즈 하라.

아까 설명드린 것처럼 실제 값과 예측 값의 차이를 줄이는 거라고 했잖아요.

그래서 처음에는 여기서부터 지나가겠죠, 그러다가 거의 0에 가까우면 학습을 그만하면 돼요.

그러면 이 과정에서 컴퓨터가 알아서 학습을 해서 만들어내요. 그런데 사람들은 모르죠, 컴퓨터만 알아요.

그래서 미니마이즈했는데 뭘 미니마이즈하냐, 코스트로 쭉 가보면 뭐 뭐 하는데, 아웃풋 레이어랑 Y 데이터 있는데 예측값 실제값 있잖아요.

예측값과 실제값을 넣어서 이게 로스값인데, 이거 말고 여기서는.

그래서 코스트를 계속해서 내리는, 그다음에 러닝레이트를 보면, 마치 공을 하나 굴리듯이 굴러가요.

그러다 퐁당 빠지군도인데, 원래 여기 빠졌어요. 

그러면 50%는 넘은 바운더리 되나요.

그러면 이거를 좀 뭔가 다양하게 시도하려면 여기서 이제 러닝레이트를 크게 잡으면 되죠, 작게 주면 조금조금씩 가니까 여기서 못 올라오죠.

그러니까 기울기값에 대한 러닝레이트가 되는 거예요.

그리고 이거랑 다르게 오버피팅이라는 게 있어요.

학습률은 엄청 좋아요. 

거의 떨어졌어요. 

다른 데이터를 넣어보면 이건 꽝이에요. 

예측을 못 해요.

데이터셋에 국한돼서 그것만 잘 학습하면 되는 거예요.

웨이트가 너무 많으면 좀 줄여주고.

그래서 웨이트값을 조금만, 너무 조금 쓰는 거 아니야, 70%만 써서 돌려보자 하면 웨이트 100이 있다면 얘네가 알아서 랜덤으로 70% 정도값만 쓰게 돼요.

그렇게 쓰게 되면 학습이 좀 더 잘 된다, 여기서 설명 못 드린 게 이 X, Y 데이터, 이 데이터가 어떤 포맷을 얻게 되는데 쉽게 말씀드려서 파일 오픈해서 넣어주면 되는 거예요.

마스크 같은 경우는 아까 말씀드렸다시피 실제 10개면 내가 게임에 들고 갔던 카드 정보 10개에서 1, 2, 3, 4만 들고 오면 그것만 온 시키는 거예요.

그렇게 해서 필터링을 거치는 거죠여도

테스트는 학습 잘 되냐, 안 되냐 확인하잖아요.

그래서 프리딕션한 거에 대해서는 세션 1에서 실제로 잘됐는지 안 됐는지 확인을 해보는 거예요.

그리고 여기는 Y값이 여기는 없어요. 테스트니까.

학습할 때는 Y값이 들어가는데 테스트할 때는 필요 없겠죠.

그리고 드롭아웃도 필요가 없죠.

그래서 여기 집을 하게 되면 이 데이터와 이 데이터가 맞닿아있으면 얘를 한 페어로 하거든요.

실제 내가 가지고 있는 Y값과 구별하게 되고 그래서 클로징해야 하니까 이걸 바꿔서 0, 0, 1, 1 이렇게 몇 개를 맞췄냐, 그렇게 계산해볼 수 있는 거죠.

이거는 학습을 하면서 테스트를 하면서 AI를 딥러닝 하면서 이런 게 있으면 좋을 거 같다고 설명을 팁으로 설명 드린 건데, 로그를 남길 때 계속 똑같은 파일인데 로그를 남기면 오버라이팅되거나 안 되잖아요.

그래서 시, 분, 초 그러면 얘네가 파일 겹칠 일이 없겠죠, 패스는 이렇게 구하고 그다음에 실제로 트레인 된 폴더가 됐고요.

그래서 폴더가 없으면 있는지 없는지 확인하고 폴더가 없으면 만들고, 이거 되게 유용해요.
 
보통 세이브한다고 치면 파일 라이팅을 하는데 플러쉬를 하게 되면 파일 라이트하고 클로즈 안 하면 데이터가 안 보여요.

그런데 학습을 하게 되면 그 진행과정을 매번 볼 수 없어요. 점심 먹고 와서 잘되나 볼 수 있고.

하여튼 그 파일에서 플러쉬하게 되면 내가 볼 수 있어요.

실제로 라이팅되고 있는 파일 뭐가 적히는지 볼 수 있습니다. 이렇게 쓰면.

실제로 폴더가 생겼고, 안에 들어가면 이렇게 생겼어요. 

이게 뭐냐, 체크 포인트 파일인데, 예를 들어서 로그 파일이 이거라면 이걸 딱 오픈해봅니다. 

어떤 파일을 저는 저장했냐 하면, 이렇게 저장하면 확인할 수 있는 게 굉장히 많아요. 

뭘 확인할 수 있는지 확인해보도록 하겠습니다.

히든 사이즈는 얼마나 줬는지, 트레인 사이즈는 몇이었고 이런 걸 나중에 학습하고 나중에 어떻게 학습했지, 보려면 잘 기억도 안 나고 메모장에 조금 써 놓은 게 전부였는데 이렇게 하면 편하게 볼 수가 있고요.

인터파일은 내가 어떤 데이터를 학습시켰지, 데이터 샘플은 한 번 넣어주게 되면 몇 개는 샘플을 보여주죠.

이거 돌리면 스텝별로 나오게 되는 겁니다.

스텝별로 학습을 하다 보면 언제부터 내가 학습이 잘됐지, 이거를 알고 싶을 때가 있어요.

스텝이 몇 번이 됐을 때 다음에 할 때 더 이상 할 필요가 없겠죠.

모델 파일 위치를 확인할 수 있고.

실제로 이런 식으로 돼요, 스텝 혹은 에폭 당 모델을 이렇게 하고, 중간 중간 체크를 하게 되는데 원하는대로 안 되면 스탑을 하게 되죠.

그래서 결론적으로 보면 이건 아까 설명드렸고 타이밍 예측은 70.94%가 나왔고요.

로케이션은 예측한 거랑 실제 데이터 차이는 4.15 정도 나왔는데 이건 캐릭터 1개 사이즈였습니다.

이거는 예측률인데, 시간이 5분밖에 안 남아서 생략하도록 하겠습니다.

이거는 뭐냐 하면 실제 빨간색 같은 경우 카드가 5개면, 빨간색은 50%의 정도로 맞췄는데 제가 예측한 건 뒤에 빨간색이거든요.

타이밍도 이렇게 되고요.

인퍼런스는 이거는 게임을 개발할 때 테서플로 라이버리를 인퍼런스하기 힘들어요.

마이크를 더 내려서 하기 때문에, 그래서 전부 다 내렸어요. 

어차피 그거로 못 썼고.

거기서 인퍼런스할 수 있는 라이브러리를 만든 거죠, 그래서 이 모델이 인퍼런스할 수 있는 라이브러리가 되고 게임 루프 같은 경우는, 지금 시간이 별로 없는데 기본적으로 하이브리드 방식이에요.

카드 예측은 이 셋으로 해요. 

그 다음에 카드 예측을 딥러닝 할 거니, 그래서 예 하면, 카드 유닛 딥러닝 할 거냐 하면 한다 그러면 딥러닝 예측값을 바꾸게 되는 거죠.

카드, 타이밍 운영자가 컨트롤할 수 있도록 돼 있어요.

코드를 보면 이렇게 돼 있어요. 

1차 돼 있고 2차는 딥러닝하는 값으로 바뀌게 되고요.

거의 막바지입니다.

영상을 보며 끝낼 건데요. 

이걸 직접 보시고. 알고리즘 방식과 딥러닝을 맞혀보세요.

어떤 걸 중점으로 보시면 되냐 하면 캐릭터가 행동한 거 있잖아요. 

언제 캐릭터가 나오고 어디에 배치가 되고 어떤 캐릭터가 나왔는지, 그 상황에.

그걸 알고리즘을 사람이 짠 거랑 랜덤이 해서 딥러닝으로 수식해서 한 거랑 어떤 게 딱딱하지 않은지 보시면 될 것 같습니다.

AI입니다.

공격자가 이제 먼저 들어가죠.

상대편 빨간색 캐릭터들이 어떻게 나오는지 보시면 됩니다.

아직까지 안 나왔어요, 캐릭터가.

이제 나오네요.

이쪽이 기울어였죠, 다음으로 넘어가겠습니다.

B가 되겠고요.

플레이 패턴을 똑같이 하는 거예요. 

차이를 주면 안 되니까 공격도 똑같이 하는 거예요. 저렇게 비슷한 위치에.

그리고 저 빨간색쪽 캐릭터가 어떻게 하는지만 보면 되는 거죠.

지금 나왔어요. 나왔죠? 뒤에.

샐리가 나왔네요.

딜러가 나왔어요.

이거는 무조건 이기면 되는 걸 만드는 게 아니에요.

지고 이기고가 상관이 없어요. 

사람같이만 플레이하면 되는 거거든요.

그런 점을 설명드리고 싶습니다. 

끝났고요. 뭐 같으세요?

A 손들어보세요.

재미있는 건데 A 손들어보세요.

A가 딥러닝을 받았다, B인 거 같다?

B가 맞아요.

그러니까 이게 보면.

알고리즘하고 딥러닝하고 설문을 해봤어요.

명확하게 다른 건 있었어요.

알고리즘은 뭔가 랜덤하게 하고 정해진 규칙, 단순하다 이렇게 하는데 딥러닝 하는 건 적절한 타이밍, 전술 전략 구상.

이건 사람만 할 수 있는 거거든요.

전술, 전략을 매번 코딩 못 짜잖아요.

생각해보면 참 개인적으로 좋은 키워드가 있는데.

마지막으로 홍보를 하나 해야 해서 홍보하는데.

관심이 있으신 분들은 밑 사이트가 셔서 클릭하시면 다양한 정보가 있으니까 보시기 바랍니다.

감사합니다. 김선태였습니다.
