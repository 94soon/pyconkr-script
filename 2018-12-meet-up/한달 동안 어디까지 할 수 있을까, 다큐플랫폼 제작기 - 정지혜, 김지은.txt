https://www.youtube.com/watch?v=PsmTH9oDIIg

이제 세 번째 발표는 정지혜 님과 김지은 님이 한 달 동안 어디까지 할 수 있을까 라는 주제로 발표해 주시겠습니다.

큰 박수 부탁드리겠습니다.

-함께: (박수)

-안녕하세요?

한 달 동안 어디까지 할 수 있을까 다큐 플랫폼 제작기라는 제목의 발표를 들려드리게 된 정지혜라고 합니다.

원래는 지은 님이랑 발표를 같이 해야 하는데 사정상 지은 님이 늦으셔가지고 저 혼자 발표를 하겠습니다.

저희는 여기 표지에도 나와 있듯이 MacNGramS란프로젝트에서 했던 것을 들려드리려고 하고 코딩도 한 지 얼마 안 되고 파이썬도 한 지 얼마 안 된 새내기 프로그래머입니다.

이런 자리에서 발표하는 것만으로도 굉장히 영광이고 떨린데 조금 가벼운 마음으로 이런 초짜들의 삽질기를 들어주셨으면 좋겠습니다.

저의 제목에서 제일 중요한 단어는 한 달이라는 단어예요.

이 한 달이라는 시간이 되게 길다면 길고 어떻게 보면 짧은 시간인데 저의에게 이 한 달은 발표를 할 때까지 주어진 시간이었어요.

하나의 서비스를 기획을 해서 그것을 디벨롭을 시키고 그렇게 해서 퍼블리싱을 하는 데까지 1달이라는 시간 동안 이루어내야 했고 하나의 발표 자리에서 발표를 해야 했었어요.

그래서 이 한 달이라는 시간을 저희 조원들은 우리가 할 수 있는 최대한을 해보는 시간으로 정의를 했었고요.

여기서 약간의 TMI를 드리자면 이 한 달이 저희가 파이썬을 통해서 프로그램을 만들어야 하는 시기이기도 했고 파이썬을 배워온 시간이기도 했어요.

파이썬 1달 차들이 이런 것을 했었던 거죠.

정말 파이썬 초짜들인데 저희의 팀을 소개를 자세히 하자면 저희는 데잇걸즈라는 빅데이터 교육 프로그램에서 만난 사이입니다.

여성 인재들을 늘리자는 것으로 진행된 프로젝트였는데 이곳에서 데이터 관련 교육을 받았어요.

그리고 그 교육뿐만 아니라 2번의 프로젝트를 준비했는데 그 중에 최종 프로젝트가 여러분께 들려드릴 다큐멘터리 아카이브 프로젝트입니다.

저희는 6명으로 이루어져 있습니다.

간단하게 전공을 써놨습니다.

정말 코딩과 무관한 과들의 모임이에요.

데잇걸즈라는 프로그램을 통해서 데이터분석을 배우긴 했지만 이전까지 데이터분석을 해본 사람이라고 하면 아무도 없었어요.

그래서 파이썬을 다뤄본 사람도 없었어요.

진짜 병아리인 저희 6명이 어떤 삽질들을 했는지 본격적으로 들려드리도록 하겠습니다.

일단 목표를 정해야겠죠.

다큐멘터리 아카이브를 만들어야겠다는 막연한 주제에 대한 발제는 있었는데 6명이 모였을 때 상세한 거는 정해진 게 없었어요.

간단한 말을 하자면 다큐멘터리를 하시는 분이 좋아하시는 분이 계신가요?

되게 많아요. 저는 이 프로젝트를 진행한 사람임에도 불구하고 다큐멘터리를 찾아서 본 적은 그때까지는 한 번도 없었던 사람이었어요.

무지했는데, 다큐멘터리에 대해서

저희 조, 그러니까 저희 프로젝트에 있었던 한 분이 EBS 다큐 프라임이라는 다큐멘터리에 조연출로 일을 하셨던 분이에요.

그분이 이 주제를 발제하신 분인데 이분의 말로는 다큐멘터리 시청률이 굉장히 낮아요. EBS 다큐프레임 2014년 1년 시청률인데 높게 나온 수치이고 일반적으로 1%가 안 된다고 해요. 3%가 넘으면 굉장히 대박을 친 거고

이렇다시피 다큐멘터리가 굉장히 좋은 것임에도 불구하고 사람들이 찾아 봐야지만 뭔가 소비가 되는 상황이었던 거죠.

그래서 저희는 아카이브를 만들되 그 아카이브가 유저들에게 조금 더 쉽고 편하게 찾아볼 수 있게 되는 그런 다큐멘터리 아카이브였으면 좋겠다는 생각을 하게 됩니다.

그래서 두 가지 목표를 세웠어요.

새로운 플랫폼, 지금까지 다큐멘터리 플랫폼만 모아놓은 것은 국내외 아무곳도 없었습니다.

그리고 그 안에 사람들이 쉽게 찾아볼 수 있게 추천 시스템을 만들어보자 두 가지 목표를 세웁니다.

물론 파이썬으로 진행을 하고요.

일단 데이터를 모아야겠죠. 다큐멘터리 데이터들이 중구난방으로 각 방송사들에 흩어져 있기 때문에 한곳에모으는 작업을 했습니다.

데이터 수집을 하게 되었습니다. 크롤링과 크롤링과 크롤링과 크롤링의 연속이었습니다.

그런데 크롤링을 해봤냐고 물으신다면 아까 말씀드렸듯이 저희는 한 달 차 새내기였기 때문에 크롤링과 유사한 작업들은 해봤지만 해본적이 없었습니다.

튜토리얼은 해봤지만 따라하기가 한 거죠.

다 따라쳤다고 생각을 하는데 에러가 납니다.

저런 상황은 겪어봤지만 저희가 직접 코드를 처음부터 끝까지 짜고 나한테 맞는 크롤러를 만들어본 적은 없었습니다.

여기서 첫 번째 삽질이 등장합니다.

여기 있는 이 화면은 KBS 스페션이라는 다큐 프로그램의 다시 보기 페이지인데요.

여기 보시면 첫 번째 네모에 보면 KBS 스페셜이라고 나와 있죠. 이거는 다큐멘터리 프로그램 이름이에요.

그리고 그 바로 밑에는 방영된 날짜고요.

그 바로 밑에 아파트, 과연 불패인가라는 제목이 있는데 이거는 에피소드 이름이에요.

유명한 남극의 눈물 이런 것들 있죠. 이런 것들은 하나의 프로그램으로 편성을 받은 건데 그런 게 아니라 스페셜 안에 하나의 에피소드로 배정을 받는 경우가 있는데 다큐멘터리 프로그램 이름, 에피소드 이름 두 개 따왔어야 하고 그거를 설명해 주는 시놉시스, 주소가 있는데 아무튼 웹 페이지 정보대로 프로그램 이름, 에피소드 제목, 방영일, 시놉시스 등을 다큐멘터리 정보라고 부를 거예요.

목표는 지상파 네 곳의 다큐 프로그램입니다.

EBS, MBC, SBS, KBS.

JTBC에서도 방영을 하지만 저희 시야는 여기밖에 되지 않더라고요.

그런데 중요한 거는 모든 방송사가 가진 데이터가 달랐습니다.

삽질을 하더라도 최소 4번은 해야 하는...

이 삽질을 다 말씀드리고 싶은데 좀 많아서 두 가지만 얘기를 해드릴게요.

MBC는 크롤링을 원치 않는다라는 그런 생각을 하게 됐어요. 저희가 이거를 하면서

이게 MBC의 다시 보기 화면인데요. 목록형으로 쭈르륵 나와 있어요.

여기 최근에 보헤미안 랩소디... 다 보셨나요. 이거랑 관련된 다큐멘터리도 있는데 이 다큐멘터리의 시놉시스를 따오기 위해서 안으로 들어가 보면 퀸생퀸사... 제일 마지막에 보면 전설의 록밴드 퀸의 이야기를 담은 영화 보헤미안 랩소디가... 이렇게 끝이 납니다.

그러면 더보기를 눌러서 타고 들어가야겠죠.

들어갔는데 이렇게 모든 페이지의 주소값이 알고 보니까 다 같았던 거예요.

저희는 지금까지 페이지에 번호 넘버링이 되어 있어서 자동화를 시켜서 긁어오는 건 해봤지만 주소가 같을 때 어떻게 해야 하지 하는 멘붕에 빠졌습니다.

처음에는 크롤링을 하지 말라는 건가 MBC 하지 말까 했는데 셀레니움을 활용해서 무한 새로고침을 했었습니다.

MBC에서 저희가 긁어온 게 450편 정도 되는데 지금은 데이터 양을 늘렸습니다. 이 당시에 450편이였는데 계속 새로고침을 누르고 눌러서 이런...

이런 상황이 일어났죠.

괜찮아요. 이건 첫 번째 삽질이에요.

이제 그래도 데이터를 아무튼 끌어는 왔지만 굉장히 뿌듯한 마음을 가지고 두 번째 KBS를 시작했어요.

그런데 에피소드 제목을 찾을 수가 없었어요.

여기 보시면 되게 재미있는 다큐멘터리예요. 한번 보시는 걸 추천드리는데 주문을 잊은 음식점이라는 이름을 가진 KBS 스페셜에 속했던 다큐멘터리입니다.

여기서 보시면 알겠지만 주문을 잊은 음식점은 시놉시스 안에만 들어가 있습니다. 꺽쇠로 들어가 있어서 이것만 끌어오면 되나 생각을 했지만 다른 것을 보면 꺽쇠 안에 없고 , 아예 제목이 없는 상황들이 많았습니다.

그래서 뭐 3주 남았는데 주제를 바꿀까 이런 생각을 했었어요.

그래도 해야지 처음에 시작을 했는데, 일주일을 썼고

다양한 방법을 강구했어요. 꺽쇠 안의 거 뽑아오고 다른 데 있나 뒤져보기도 했는데 하나의 좋은 꼼수책을 찾아냈어요. 위키백과에 에피소드 목록이 있었어요.

저희가 찾을 필요가 없었던 거죠.

그래서 여기에서 이 표를 그대로 긁어와서 저희가 얻은 정보와 이 정보를 합치기로 했어요. 합치는 데도 약간의 어려움이 있었던 게 회차를 처음에 생각했는데 왼쪽에는 1회라고 되어 있는데 정작 180회입니다.

무슨 연관성인지는 모르겠지만 어떻게 머지를 했습니다.

이런 삽질을 통해서

그런데 정작 저희가 방송사를 2개밖에 못한 거예요. 4개를 해야 하는데

또 한 일주일 썼겠죠. 이거 하는데? (웃음)

그런데 답이 없고, 끝이없고 이거 끝까지 할 수 있을까? 발표할 때 크롤링 한 거만 얘기하고 내려오는 거 안야?

아니야? 이런 생각을 했어요.

크롤링은 크롤링대로 백업으로 하되 다른 걸 구체화를 시켜놓자.

데이터 크롤링을 적게 하더라도 보여지는 게 있게 발표를 할 수 있어야지

죽도 밥도 안 되기 전에 정제작업을 시작합니다.

저희가 생각하는 서비스 작업을 하기 위해서 시놉시스가 중요했어요.

시놉시스를 처음에 긁어오면 이런 형태입니다.

엄청나게 많은 개행문자, 각 편들마다 다른 구분자들이 들어가 있고 이런 형태를 띄더라고요.

그런데 이 개행문자를 처리해야 하는데 처음에는 완벽한 시놉시스를 그대로 가져와서 쓰고 싶었어요. 웹페이지에 띄워주고 싶었어요. 오타들이 없게 해가지고

그런데 뭔가를 처리하다 보면 .이 사라져 있고 띄어쓰기가 이상하고 월이 사라져 있고 몇월 며칠인데 숫자가 없고 이런 상황이 나타나서 일단 최대한 모든 상황, 특수문자들이 사라진 상태로 시놉시스를 넣었습니다.

아무튼 데이터 프레임을 만들었습니다.

이 중에서 계속 데이터 크롤링은 하고 있었습니다.

크롤링은 하고 있고 추천 시스템을 만들기 위해서 시놉시스는 아무튼 정제는 했고, 그런데 이제 팀원이 6명이니까 사람들이 남는 거예요. 이 작업을 있긴 하지만 작업도 굉장히 남았지만

그러면 이제 2개의 목표가 있긴 하지만 조금 더 목표를 늘려볼까 하는 욕심을 갖게 되었어요. 정제를 시키니까

일단 몇 명은 이거 제대로 하자고 했지만 새로운 목표를 하나 세웠어요.

새로운 태그를 만들어보자 헤시태그처럼 이거 들어가면 관련된 다큐가 뜨게 해보자.

두 번째 데이터 정제를 하게 됩니다.

이런 상태에서 이렇게까지는 만들었어요.

그랬는데 이제는 여기서 이렇게 워드들만 뽑아내고 싶은 거예요. 그것도 중요도가 높은 순서대로 많게는 10개, 적게는 3, 4개

그래서 명사 추출기를 찾아봤습니다. 초보자니까 검색을 해서 구글에 있는 모든 명사 추출기를 다 가지고 왔어요. 토크나이저, 넌익스터렉터, 생각보다 많았는데 뭐가 좋은지 모르잖아요. 저희한테

그러면 어떻게 해요. 다 해봐야죠.

그런데 그건 좋아요.

그런데 성과를 판별하기가 되게 어려운 거예요.

정확한 태그인가? 이거를 정확하게 반영하는가가 어려운 거예요.

그래서 눈으로 읽었습니다. 6명에서

다 읽고 제대로 나오는 거 같네 한 결과가 6명의 머리로는 뉴스 논 익스트렉터였어요.

초보자다 보니까 이런 일들이 많았어요.

그런데 결정하는 데 이게 마냥 잘되지는 않았어요.

하나의 다큐멘터리를 보면 내용이 없습니다라는 시놉시스가 있었어요.

내용이 없어요. (웃음)

어떻게 해야 할지 모르겠는데 시놉시스가 없는 다큐멘터리들이 생각보다 많았던 거예요.

앞의 거는 2005년인데 과거로 갈수록 삭제되어 있는 게 많았어요. 최근 것도 많았는데 길이가 3줄도 있고 20, 30줄도 있고 양이 다 달라요.

그런데 시놉시스가 없는 애들을 어떻게 처리를 할까 고민을 해보니까 버릴까? 지울까 잘 모르겠어요.

2005년도 거니까?

버릴까 생각은 했지만 작업을 해봐야죠. 삽질기니까요.

제목을 그대로 시놉시스로 가져와서 제목으로 뭔가를 해보자는 결론을 냈어요.

그래서 난 값을 찾아서 제목을 옮겨 담고 익스트렉서를 썼는데 절대 한 번에 될 리가 없습니다.

그 이유를 찾아 보니까 제목은 사실 한 줄이잖아요.

그 한 줄로는 추출될 단어의 수가 작았던 거예요. 중요도 계산이 되지 않아서 순서가 나오지 않고 단어도 3개면 3개가 다 뽑히지 않고 그런 상황들이 발생을 했어요.

그래서 결국에는 다른 추출기들을 또 돌리고, 또 돌리고 또 어떤 게 더 낫나 해서 한나눈이낭 OKT를 사용해서 명사를 추출하고 불용어를 제거하자.

태그를 추출하기 위해서 3차 작업을 한 거죠.

그런데 자동화도 못해서 수작업으로 하나씩 돌리는 작업을 했어요.

지금 다시 현재의 업무 시스템을 설명드리면 크롤링은 계속해서 하고 있고 태그는 대충 추출이 끝나가고 태그를 기반으로 추천 시스템을 만들자 짰어요.

그런데 6명이니까 인원이 남아서 욕심을 냈어요.

뭐하지?

목표를 새롭게 짰어요. 대분류를 만들어보자.

EBS 다큐프라임이라는 프로그램에는 이런 식으로 13개의 대분류가 있더라고요. 가정, 경제, 경영, 과학, 교육 이런 게 있어서 학교 교육과 관련된 게 있으면 누르면 볼 수 있는 시스템이었어요.

그런데 다른 방송사는 이런 게 없더라고요.

그래서 이걸 추가해서 하고자 하는 퍼블리싱에 추가하면 좋겠다 해서 machine learing을 해보자고 생각을 했어요.

EBS를 트레이닝을 시키고 나머지를 해보자고 했는데 그렇죠.

한 번에 될 리가 없죠.

그래서 오버피팅이 일어났어요. EBS만 넣다 보니까

문제가 일반인들은 모를 수 있는 부분이지만 저희한테는 조연출이 있었잖아요.

조연출께서 하시는 말씀이 각 방송별로 시놉시스를 쓰는 작가가 따로 있대요.

각자의 문체가 다르잖아요.

중요하게 생각하는 워드들을 강조하는 그런 방법들이 각 방송사별로 특징이 있었던 거예요.

EBS만의 특징만으로 트레이닝을 시키니까 나머지 데이터들은 정확하게 분류를 못해 내는 거예요.

그러면 이제 어떻게 해야 하지 라는 고민을 한 끝에 수작업을 해야죠.

저희는 이렇게 다른 방송사들의 데이터에 시놉시스를 읽고 직접 대분류를 달아줬어요. 이게 정말 얼마나 힘든 작업이었냐 하면 저희한테 논란이 되었던 하나의 시놉시스를 살짝 읽어드릴게요.

첫 문장이 뭐냐하면 지난 4월 27일 남과 북의 두 정상이 만났다예요.

13개의 카테고리 중에서는 정치에 가깝겠죠.

정치, 사회에 가까울 거 같은데 그다음 문장이 뭐냐하면 군사 분계선을 넘어 최초로 배달된 음식이자 남북 정상회담의 음식은 바로 평양냉면이었다예요.

-함께: (웃음)

-어렵사리 평앙에서부터 냉면이 가져왔습니다. 멀다고 하면 안되갔구나 이런 어쩌고 내용이 있어요.

이렇게 되니까 뭐에 넣어야 하지 하는 생각이 드는 거예요.

심지어 중간에는 소문난 냉면 애호가 존 스파이크, 김영철, 냉면 성애자 아이콘 존박, 신동엽 등 유명인들의 냉면 사랑과 폭풍 냉면 먹방이 맛깔나게 펼쳐질 예정이자.

시작과 끝이 극과 극을 달리잖아요.

어디에 넣어야 할지 모르는 거예요.

중요한 건 평양, 남북 이야기가 많이 들어가다 보니까 얘를 저희가 정치 사회에 넣으면... 정치 사회에 아닌 곳에 넣으면 다른 곳이 정치 사회로 들어가는 걸 막는 상황이 벌어질 수 있는 거예요.

그래서 결국에는 아무튼 인문에 넣긴 했어요.

인류의 문화... 아무튼 식문화잖아요. (웃음)

넣긴 했는데 이런 것이 진짜 많았어요. 의학에 넣어야 하는지 과학에 넣어야 하는지 이런 것도 많았고 특히 교육 같은 경우에는 정치에 넣어야 하는지 교육에 넣어야 하는지 이런 것들?

결국에는 아무튼 6명이 머리를 짜내고 오버피팅을 막기 위해서 나머지 방송사의 대분류를 손수 입력해서 트레이닝 데이터를 입력해줬습니다. 이 삽질을 통해서 어느 정도 맞게 나왔어요.

결국에는 EBS, 지상파 데이터를 같이 넣어서 분류 알고리즘을 만들었고 지상파 데이터를 넣는 최종적인 machine learing이 끝났습니다.

이 외에 추천 알고리즘을 만들어야 하는데 뭐가 추천을 만들어주는지 모르고 조 사람들이 TFIDF가 뭔지 모르는 사람들도 있어요.

이런 수많은 삽질 끝에 저희가 웹사이트를 만들긴 했어요. 1달 만에 만들어진 거죠.

저희가 저희 서비스 소개할 때 만든 건데 멋있어서 넣었어요. 초심자 같은 발표라 사진이라도 멋있어 보이려고

처음에는 목표가 2개였어요. 새로운 플랫폼과 추천 시스템을 구축하자.

1달이라는 시간이 짧은 거 같았지만 뭔가 조금의 데이터로 서비스를 만들어만 놓자, 데이터는 넣으면 되잖아.

그러다 보니까 생각보다 1달이 길었고 2개를 더 실행할 수 있는 시간이 되었어요.

물론 수많은 한계가 있었습니다.

하지 말까 생각을 하고 저희 발제자가 욕심이 많습니다.

웹사이트를 만들었는데 웹사이트에 추천하는 다큐멘터리를 10개를 넣고 싶대요.

10개 넣을 수 있잖아요. 그런데 저는 웹사이트를 만드는 사람으로써 5개만 넣어야 예쁜 거예요.

5개만 넣어야 예쁜데 자꾸 10개 넣으라고 해서 안 돼, 5개만 하자 이런 것도 있었고 그런 서로의 논의들이 많이 있었는데 이제는 저희 6명 모두 데이터 분석을 해본 사람 물어봤을 때 저요라고 대답할 수 있는 발자국을 뗀 병아리가, 털이 좀 나게 시작하게 된 거죠.

그래도 저희가 뭘 만들었는지 조금은 자랑하고 싶어서 저희 웹시스템에서 다큐멘터리를 어떻게 찾는지를 간단하게 설명을 드리려고 해요.

저희가 가지고 있는 서비스가 이렇게 엄마라는 키워드 넣으면 검색도 되고요. (웃음)

검색하면 검색하면 방송사별로 눌러서 따로 볼 수도 있고요.

들어가면 상 받은 거 체크도 할 수 있고, 사실 별거 없죠.

바로가기 버튼 누르면 넘어가고 각 헤시태그별로 눌러도 확인할 수 있고 방송사별로 대분류도 확인할 수 있는 서비스예요.

사실 이걸로 뭐 돈 벌 수 있는 광고도 하나도 안 붙였고요. 그냥 들어가서 조회수만... 막 이래.

그런 상태인데요.

아무튼 한 달 동안 배운 사람들이 한 달만에 이런 서비스를 만들었어요.

여기에는 저희보다 훨씬 경력도 많고 많은 학습을 하고 많은 서비스를 개발해보신 분들이 계시겠지만 저희 한 달 차들의 말도 안 되는 삽질기를 통해서 뭔가 재미있는 자극을 얻어가셨다면 그 정도로 저의 발표는 성공적이었다고 생각합니다.

경청해 주셔서 감사하고요.

질문해 주시면 감사하겠습니다.

-함께: (박수)

-어느 분이 먼저...

-너무 재미있게 잘 들었고요.

-감사합니다.

-여기 조금 더하면 사람별로 추천할 수도 있지 않을까요.

-먼저 질문해 주세요.

-그게 하나도 있고 우리가 도서관에 가면 학생들 아르바이트 많이 시키잖아요. 자원봉사라는 이름으로

그런데 자기가 빌려간 책과 도서관에 연관된 것을 이용해서 사실 그 학생들을, 추천 시스템을 개발하게 되는 그렇게 조금 더 진화된 것을 개발할 수 있지 않을까?

-너무 좋은...

일단

-첫 번째

-먼저 말씀을 드리자면 유저 베이스드 추천 필터링은 저희가 준비를 하려고 하는 중이에요.

저희가 웹 사이트도 안 만들어봤고, 웹 사이트를 열흘 만에 만들었거든요.

한 달이다 보니까 그러다 보니까 아까 로그 이런 거 설명하셨는데 그런 것들을 아직은 할 줄 몰랐어요.

그때는 한 달이라는 제약이 있어서 시작을 못했는데 저희가 이 발표 말한 기간이 10월 6일이었는데 그 이후에도 지금 열심히 뭔가 연습을 하고 있거든요. 공부들을

그것을 통해서 유저 베이스는 진행을 해볼 예정이에요.

그리고 그 도서관 이야기는 좋은... 감사합니다.

한번 고려를 해볼게요. (웃음)

다른 질문 혹시...?

-들리시나요.

-네. ㄹ

-검색 기능도 아까 보여주셨는데 검색기능에는 또다른 삽질이나 이런 게 없었나요.

검색이 어떻게 되는지 몰라서 그런데 키워드가 파이썬에서 문자 서칭 단순히 이런 건지 아니면 인덱싱을 따로 어떻게 하신 건지

-이거 같은 경우는 저희가 어떻게 했느냐 하면 제목을 기준으로 일단 키워드 검색이 가능하게 했어요.

그러니까 단어를 입력하면 제목에서 서치를 해가지고 그냥 사실 문자열 검색이죠.

그런 식으로 작업을 하긴 했어요.

그런데 저희가 2차적으로 지금 생각중인 거는 시놉시스 안에서도 이런 단어를 넣어서 검색이 되게 하면 좋겠다고 생각을 했거든요.

그런데 지금 저희가 이 작업을 하면서 가장 중요하게 생각했던 건 저작권이에요.

이게 사실 모든 방송사에 귀속된 하나의 저작물인데 그거를 헤치지 않는 선에서 만들려면 시놉시스를 웹페이지에 띄우면 안 돼요.

그것도 하나의 글을 쓴 거니까 안 되더라고요.

그래서 그 시놉시스를 백그라운드에 넣은 상태에서 검색이 되게끔 작업은 진행하고 있는데 사실 저희의 방법은 그냥 단순한 문자열 서칭입니다.

(웃음)

다른 질문 있으실까요.

-저도 비전공자이고 회사에서 혼자 개발하고 있어서 되게 공감 많이 됐고요. 초보라서...

크롤링 하게 되면 일단 제일 중요한 게 반복적으로 작업을 해줘야 하잖아요. 업데이트도 해줘야 하는데 이거에 대해서 팁 있으시면 좀 부탁드릴게요.

-일단 가장 좋은 방법은 막히지 않게끔 딜레이를 시켜주는 거예요.

그거긴 한데 저희 같은 경우는 일단은 한 달의 기간 동안 그때까지 나온 다큐멘터리를 다 넣어보자라는 거였기 때문에 업데이트를 그 당시에는 생각을 안 하고 일단 만들기는 했어요.

하긴 하겠지만 자동으로 되게끔 작업을 안 했어요.

그런데 저희의 이 시스템에 대해서 약간의 설명을 조금만 덧붙이자면 DB를 사용한 게 아니에요. 이 웹 사이트가

그냥 제이슨팔 하나로 만든 웹사이트거든요.

DB를 사용하려고 했는데 DB를 다뤄본 경험이 저밖에 없어가지고 작업의 유동성이 너무 떨어져서 그냥 제이슨팔로 해서 웹사이트로 들어오게 데이터를 만졌습니다.

지금의 웹사이트 상태에서는 아예 자동화가 안 되는 상태예요.

DB를 구축화한 이후에 자동화를 시킬 예정이고 제가 공부를 많이 했었는데 지금 떠오르는 팁이 별로 없네요.

저희의 시스템은 아무튼 그런 상태입니다.

도움이 혹시 되셨나요. 죄송핵

죄송해요. 혹시 다른 질문 하나 있나요.

-시간상 하나만 더 받을게요. 이분 마지막으로

-혹시 machine learing 관련해서 조금 질문을 드리고 싶어서 손을 들었었는데요.

혹시 machine learing 알고리즘을 구상하실 때 어떻게 구상하셨는지랑 그다음에 두 번째로는 직접 하나하나 대분류 라벨을 다 다셨다고 하셨잖아요.

-네.

-이게 오버피팅이 안 날 정도로 다셨다는 거는 대부분 데이터를 다셨다는 건데 그러면 굳이 데이터를 다시 구상한 이유가 뭔지?

-일단 저희가 생각했던 machine learing 방법에 대해서 말씀을 드리자면 아까 전에 태그를 추출한 부분을 말씀드렸잖아요. 어디갔지?

이렇게 단어들을 뽑아냈잖아요.

저희가 추천... 죄송합니다. 대분류를 얘기하고 있었는데 추천을 얘기하고 있었네요.

추천도... 처음에 말씀하신 게 추천인가요? 죄송해요.

뒷 내용에 집중을 해가지고...

-첫 번째가 이제 어떤 알고리즘을 사용해서 대분류를 분류하였는지 였고요.

그다음에 두 번째가 라벨을 대부분 붙였을 거 같은데 굳이 machine learing을 또 썼던 이유가 뭔지

-일단 대분류 알고리즘은 추천이랑 비슷한데 아까 전에 태그를 추출한 것들에 단어들을 추출해서 어떤 단어들이 있으면 유사한 단어들이 많이 있으면, 그러니까 아까 전에 읽어드렸잖아요.

평양, 남북 이런 것들이 있으면 정치 사회에 들어가는구나 이런 것들을 읽을 수 있게 단어 기반으로 해서 분류 알고리즘을 짰어요.

그리고 아까 수작업을 하는 부분에 대해서 말씀을 하셨는데 많이 달기는 달랐는데 중요한 게 중요한 건 크롤링을 백업작업을 밑에서 하고 있었고 이거를

저희가 데이터를 3천 개 정도의 다큐멘터리를 웹사이트에 결국 넣긴 했는데 이 작업을 할 때 아직 500, 600개밖에 데이터가 없었얶

그 안에서 데이터를 넣어서 괜찮은 아이를 만들었어요.

그런데 그 이후에 들어온 아이들에 대해서 라벨링을 다 하지 않았어요.

그렇다 보니까 machine learing이라고 말은 했는데...

라벨링을 몇 개나 했는지가 되게 중요한 문제인데 조금 많이 부족하긴 한 것 같습니다.

저희가 생각해도, 지금 많이 개선을 하고 있고요.

제가 이 모든 업무의 담당자가 아니라 좋은 답변을 못 드린 게 많은데 나중에 뒤에 찾아오시면 저희 조원들이 다 있으니까 다른 질문 많이 해 주시면 겠습니다. 합니다.

좋겠습니다. 감사합니다.

-다시 한 번 큰 박수 부탁드립니다.

-함께: (박수)

